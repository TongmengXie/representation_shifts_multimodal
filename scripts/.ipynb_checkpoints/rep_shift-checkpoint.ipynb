{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c93407d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:46:58.262177Z",
     "start_time": "2025-08-21T14:46:51.103371Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/py311/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py311/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Comprehensive SAE-based representation shift analysis with layer sweeping,\n",
    "real datasets, and patching logic for LLM->VLM adaptation studies.\n",
    "\"\"\"\n",
    "# Installation requirements:\n",
    "\"\"\"\n",
    "pip install sae-lens transformers torch matplotlib seaborn numpy datasets tqdm\n",
    "\n",
    "# For CUDA support (recommended):\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PaliGemmaForConditionalGeneration\n",
    "from sae_lens import SAE\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, List, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66759196",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:47:05.611207Z",
     "start_time": "2025-08-21T14:46:58.267065Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  4.17it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 27\n"
     ]
    }
   ],
   "source": [
    "# Test to see if both models have the output layer number in the hidden states\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/paligemma2-3b-pt-224', trust_remote_code=True)\n",
    "# Ayda: Gemma models add a bos token but Paligemma models don’t, so it messes up comparison.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"tokenizer.pad_token:{tokenizer.pad_token}\")\n",
    "\n",
    "model1 = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-2-2b\", \n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=None\n",
    "    )\n",
    "\n",
    "language_model1 = model1\n",
    "language_model1.eval()\n",
    "    \n",
    "model2 = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    'google/paligemma2-3b-pt-224', \n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,  \n",
    "    device_map=None  \n",
    ")\n",
    "\n",
    "language_model2 = model2.language_model\n",
    "language_model2.eval()\n",
    "inputs = tokenizer(\n",
    "            \"cat\", \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            add_special_tokens=True  # Ensure special tokens are added properly\n",
    "        )\n",
    "\n",
    "#vocab_size = tokenizer.vocab_size\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "outputs1 = model1(input_ids, output_hidden_states=True)\n",
    "outputs2 = model2(input_ids, output_hidden_states=True)\n",
    "print(len(outputs1.hidden_states), len(outputs2.hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17277ef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:47:05.705433Z",
     "start_time": "2025-08-21T14:47:05.613583Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comprehensive SAE-based representation shift analysis with layer sweeping,\n",
    "real datasets, and patching logic for LLM->VLM adaptation studies.\n",
    "FIXED: Handles PaliGemma loss computation correctly.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sae_lens import SAE\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, List, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Disable gradients globally for memory efficiency\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "@dataclass\n",
    "class SAEMetrics:\n",
    "    \"\"\"Container for SAE evaluation metrics.\"\"\"\n",
    "    reconstruction_loss: float\n",
    "    l0_sparsity: float\n",
    "    l1_sparsity: float\n",
    "    fraction_alive: float\n",
    "    mean_max_activation: float\n",
    "    reconstruction_score: float\n",
    "    model_delta_loss: float \n",
    "    rec_loss_topk: float\n",
    "\n",
    "@dataclass\n",
    "class RepresentationShift:\n",
    "    \"\"\"Container for representation shift metrics.\"\"\"\n",
    "    cosine_similarity: float\n",
    "    l2_distance: float\n",
    "    feature_overlap: float\n",
    "    js_divergence: float\n",
    "    feature_correlation: float\n",
    "\n",
    "class DatasetLoader:\n",
    "    \"\"\"Handles loading and preprocessing of various datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, device: str = \"cuda\"):\n",
    "        self.device = device\n",
    "    \n",
    "    def load_cifar100_captions(self, split: str = \"train\", max_samples: int = 100) -> List[str]:\n",
    "        \"\"\"Load CIFAR-100 with generated captions for multimodal analysis.\"\"\"\n",
    "        try:\n",
    "            # CIFAR-100 doesn't have captions by default, so we create descriptive ones\n",
    "            dataset = load_dataset(\"cifar100\", split=split)\n",
    "            \n",
    "            # CIFAR-100 class names\n",
    "            class_names = [\n",
    "                'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
    "                'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
    "                'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
    "                'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
    "                'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
    "                'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "                'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "                'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "                'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
    "                'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
    "                'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
    "                'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
    "                'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
    "                'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
    "            ]\n",
    "            \n",
    "            texts = []\n",
    "            for i, sample in enumerate(dataset):\n",
    "                if i >= max_samples:\n",
    "                    break\n",
    "                class_name = class_names[sample['fine_label']]\n",
    "                # Generate descriptive captions\n",
    "                captions = [\n",
    "                    f\"This is a photo of a {class_name}.\",\n",
    "                    f\"An image showing a {class_name}.\",\n",
    "                    f\"A picture of a {class_name} in natural setting.\",\n",
    "                    f\"Visual representation of a {class_name}.\"\n",
    "                ]\n",
    "                texts.extend(captions[:2])  # Take 2 captions per image\n",
    "            \n",
    "            print(f\"✅ Loaded {len(texts)} CIFAR-100 captions\")\n",
    "            return texts[:max_samples]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading CIFAR-100: {e}\")\n",
    "            return self._get_fallback_texts()\n",
    "    \n",
    "    def load_coco_captions(self, split: str = \"validation\", max_samples: int = 100) -> List[str]:\n",
    "        \"\"\"Load COCO captions dataset.\"\"\"\n",
    "        try:\n",
    "            # Load COCO captions\n",
    "            dataset = load_dataset(\"HuggingFaceM4/COCO\", split=split)\n",
    "            \n",
    "            texts = []\n",
    "            for i, sample in enumerate(dataset):\n",
    "                if i >= max_samples:\n",
    "                    break\n",
    "                \n",
    "                # COCO has multiple captions per image\n",
    "                if 'sentences' in sample and 'raw' in sample['sentences']:\n",
    "                    for sentence in sample['sentences']['raw'][:2]:  # Take first 2 captions\n",
    "                        texts.append(sentence)\n",
    "                elif 'caption' in sample:\n",
    "                    texts.append(sample['caption'])\n",
    "            \n",
    "            print(f\"✅ Loaded {len(texts)} COCO captions\")\n",
    "            return texts[:max_samples]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading COCO: {e}\")\n",
    "            # Try alternative COCO dataset\n",
    "            try:\n",
    "                dataset = load_dataset(\"nielsr/coco-captions\", split=\"validation\")\n",
    "                texts = [sample['caption'] for sample in dataset.select(range(min(max_samples, len(dataset))))]\n",
    "                print(f\"✅ Loaded {len(texts)} COCO captions (alternative)\")\n",
    "                return texts\n",
    "            except:\n",
    "                return self._get_fallback_texts()\n",
    "    \n",
    "    def load_llava_bench(self, max_samples: int = 100) -> List[str]:\n",
    "        \"\"\"Load LLaVA-Bench questions/descriptions.\"\"\"\n",
    "        try:\n",
    "            # LLaVA bench conversations\n",
    "            dataset = load_dataset(\"lmms-lab/LLaVA-OneVision-Data\", split=\"dev_mini\")\n",
    "            \n",
    "            texts = []\n",
    "            for i, sample in enumerate(dataset):\n",
    "                if i >= max_samples:\n",
    "                    break\n",
    "                \n",
    "                if 'conversations' in sample:\n",
    "                    for conv in sample['conversations'][:2]:  # Take first 2 conversations\n",
    "                        if 'value' in conv:\n",
    "                            texts.append(conv['value'])\n",
    "            \n",
    "            print(f\"✅ Loaded {len(texts)} LLaVA-Bench texts\")\n",
    "            return texts[:max_samples]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading LLaVA-Bench: {e}\")\n",
    "            return self._get_fallback_texts()\n",
    "    \n",
    "    def _get_fallback_texts(self) -> List[str]:\n",
    "        \"\"\"Fallback texts if datasets fail to load.\"\"\"\n",
    "        return [\n",
    "            \"A photo of a red apple on a white background.\",\n",
    "            \"The cat is sitting on a wooden chair.\",\n",
    "            \"Mountains covered with snow in winter landscape.\",\n",
    "            \"A blue car driving on a highway.\",\n",
    "            \"Children playing in a park with green grass.\",\n",
    "            \"A delicious chocolate cake on a plate.\",\n",
    "            \"Ocean waves crashing against rocky shore.\",\n",
    "            \"A person reading a book in a library.\",\n",
    "            \"Colorful flowers blooming in spring garden.\",\n",
    "            \"A dog running happily in the field.\",\n",
    "        ]\n",
    "    \n",
    "    def get_mixed_dataset(self, total_samples: int = 150) -> List[str]:\n",
    "        \"\"\"Get a mixed dataset from multiple sources.\"\"\"\n",
    "        samples_per_source = total_samples // 3\n",
    "        \n",
    "        texts = []\n",
    "        texts.extend(self.load_cifar100_captions(max_samples=samples_per_source))\n",
    "        texts.extend(self.load_coco_captions(max_samples=samples_per_source))\n",
    "        texts.extend(self.load_llava_bench(max_samples=samples_per_source))\n",
    "        \n",
    "        # Shuffle for good measure\n",
    "        import random\n",
    "        random.shuffle(texts)\n",
    "        \n",
    "        return texts[:total_samples]\n",
    "\n",
    "class MemoryEfficientSAEAnalyzer:\n",
    "    \"\"\"Memory-efficient SAE analyzer with layer sweeping and patching logic.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_size: str = \"2b\",\n",
    "                 width: str = \"16k\", \n",
    "                 suffix: str = \"canonical\",\n",
    "                 device: str = \"cuda\",\n",
    "                 output_dir: str = \"../figs_tabs\"):\n",
    "        \"\"\"\n",
    "        Initialize memory-efficient SAE analyzer.\n",
    "        \n",
    "        Args:\n",
    "            model_size: Model size (\"2b\" or \"9b\")\n",
    "            width: SAE width (\"16k\", \"65k\", \"262k\")\n",
    "            suffix: SAE variant (\"canonical\" or specific L0)\n",
    "            device: Device to use\n",
    "            output_dir: Directory for saving outputs\n",
    "        \"\"\"\n",
    "        self.device = device if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model_size = model_size\n",
    "        self.width = width\n",
    "        self.suffix = suffix\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Model cache for memory efficiency\n",
    "        self.model_cache = {}\n",
    "        self.sae_cache = {}\n",
    "        \n",
    "        print(f\"🔧 Initialized SAE Analyzer\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Model Size: {model_size}\")\n",
    "        print(f\"   SAE Width: {width}\")\n",
    "        print(f\"   Output Dir: {output_dir}\")\n",
    "\n",
    "    def get_gemmascope_sae(self, layer: int) -> SAE:\n",
    "        \"\"\"Load Gemma Scope SAE with caching for memory efficiency.\"\"\"\n",
    "        cache_key = f\"layer_{layer}\"\n",
    "        \n",
    "        if cache_key in self.sae_cache:\n",
    "            return self.sae_cache[cache_key]\n",
    "        \n",
    "        release = f\"gemma-scope-{self.model_size}-pt-res\"\n",
    "        if self.suffix == \"canonical\":\n",
    "            release = f\"gemma-scope-{self.model_size}-pt-res-canonical\"\n",
    "            sae_id = f\"layer_{layer}/width_{self.width}/canonical\"\n",
    "        else:\n",
    "            sae_id = f\"layer_{layer}/width_{self.width}/{self.suffix}\"\n",
    "        \n",
    "        print(f\"   📥 Loading SAE Layer {layer}: {sae_id}\")\n",
    "        \n",
    "        try:\n",
    "            sae = SAE.from_pretrained(release, sae_id).to(self.device)\n",
    "            sae.eval()\n",
    "            \n",
    "            # Cache management - keep only last 2 SAEs to save memory\n",
    "            if len(self.sae_cache) >= 2:\n",
    "                oldest_key = list(self.sae_cache.keys())[0]\n",
    "                del self.sae_cache[oldest_key]\n",
    "                gc.collect()\n",
    "            \n",
    "            self.sae_cache[cache_key] = sae\n",
    "            return sae\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading SAE layer {layer}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_model(self, model_name: str):\n",
    "        \"\"\"Load model with caching and proper device placement.\"\"\"\n",
    "        if model_name in self.model_cache:\n",
    "            return self.model_cache[model_name]\n",
    "        \n",
    "        print(f\"📥 Loading model: {model_name}\")\n",
    "        \n",
    "        try:\n",
    "#             tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "            tokenizer = AutoTokenizer.from_pretrained('google/paligemma2-3b-pt-224', trust_remote_code=True)\n",
    "            # Ayda: Gemma models add a bos token but Paligemma models don’t, so it messes up comparison.\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "#             print(f\"model_name: {model_name}, tokenizer.pad_token:{tokenizer.pad_token}\")\n",
    "            # Handle different model types\n",
    "            if \"paligemma\" in model_name.lower():\n",
    "                from transformers import PaliGemmaForConditionalGeneration\n",
    "                model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "                    model_name, \n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=torch.float16,  # Use fp16 for memory efficiency\n",
    "                    device_map=None  # We'll handle device placement manually\n",
    "                )\n",
    "                model = model.to(self.device)\n",
    "                language_model = model.language_model\n",
    "            else:\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name, \n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=None\n",
    "                )\n",
    "                model = model.to(self.device)\n",
    "                language_model = model\n",
    "            \n",
    "            language_model.eval()\n",
    "#             print(model_name, language_model)\n",
    "            # Cache management - keep only one model at a time\n",
    "            if len(self.model_cache) >= 1:\n",
    "                for cached_name in list(self.model_cache.keys()):\n",
    "                    del self.model_cache[cached_name]\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "            self.model_cache[model_name] = (tokenizer, model, language_model)\n",
    "            return tokenizer, model, language_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading model {model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def extract_activations_with_patching(self, \n",
    "                                      model_name: str, \n",
    "                                      text: str, \n",
    "                                      layer: int,\n",
    "                                      sae: Optional[SAE] = None) -> Tuple[torch.Tensor, float]:\n",
    "        \"\"\"\n",
    "        Extract activations ~and compute model delta loss with patching~.\n",
    "\n",
    "        NOTE: Patching is DISABLED to save time. We keep the original code but comment it out.\n",
    "        Always returns model_delta_loss = 0.0.\n",
    "        \"\"\"\n",
    "\n",
    "        tokenizer, model, language_model = self.get_model(model_name)\n",
    "\n",
    "        # FIXED: More robust tokenization with proper padding token handling\n",
    "        # Ensure we have a pad token\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        # Tokenize with safer parameters\n",
    "        inputs = tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            add_special_tokens=True  # Ensure special tokens are added properly\n",
    "        )\n",
    "\n",
    "        # FIXED: Validate token IDs are within vocabulary range\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        input_ids = inputs['input_ids']\n",
    "\n",
    "        # Check for out-of-bounds token IDs\n",
    "        if torch.any(input_ids >= vocab_size) or torch.any(input_ids < 0):\n",
    "            print(f\"⚠️  Invalid token IDs detected. Max ID: {input_ids.max()}, Vocab size: {vocab_size}\")\n",
    "            # Clamp invalid IDs to valid range\n",
    "            input_ids = torch.clamp(input_ids, 0, vocab_size - 1)\n",
    "            inputs['input_ids'] = input_ids\n",
    "\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        # FIXED: More robust label creation\n",
    "        def create_labels(input_ids, pad_token_id):\n",
    "            \"\"\"Create labels with proper masking for loss computation\"\"\"\n",
    "            labels = input_ids.clone()\n",
    "            # Mask padding tokens\n",
    "            labels[labels == pad_token_id] = -100\n",
    "            # FIXED: Also mask the first token (often BOS) to avoid issues\n",
    "            if labels.size(1) > 1:\n",
    "                labels[:, 0] = -100\n",
    "            return labels\n",
    "\n",
    "        # Get unpatched model loss (baseline) — kept for potential logging/consistency\n",
    "        unpatched_loss = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             try:\n",
    "#                 if \"paligemma\" in model_name.lower():\n",
    "#                     # For PaliGemma, we need to handle text-only input differently\n",
    "#                     labels = create_labels(inputs['input_ids'], tokenizer.pad_token_id)\n",
    "\n",
    "#                     # Get outputs from language model\n",
    "#                     unpatched_outputs = language_model(**inputs)\n",
    "\n",
    "#                     # Check if we have logits to compute loss\n",
    "#                     if hasattr(unpatched_outputs, 'logits'):\n",
    "#                         logits = unpatched_outputs.logits\n",
    "\n",
    "#                         # FIXED: More robust loss computation with better shape handling\n",
    "#                         if logits.size(1) > 1 and labels.size(1) > 1:\n",
    "#                             shift_logits = logits[..., :-1, :].contiguous()\n",
    "#                             shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "#                             # Ensure we have valid data for loss computation\n",
    "#                             valid_mask = shift_labels != -100\n",
    "#                             if valid_mask.any():\n",
    "#                                 shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "#                                 shift_labels = shift_labels.view(-1)\n",
    "\n",
    "#                                 # FIXED: Use reduction='mean' and handle empty tensors\n",
    "#                                 loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "#                                 unpatched_loss = loss_fct(shift_logits, shift_labels).item()\n",
    "#                             else:\n",
    "#                                 print(\"⚠️  No valid tokens for loss computation\")\n",
    "#                                 unpatched_loss = 0.0\n",
    "#                         else:\n",
    "#                             print(\"⚠️  Insufficient sequence length for loss computation\")\n",
    "#                             unpatched_loss = 0.0\n",
    "#                     else:\n",
    "#                         # Fallback for models without logits\n",
    "#                         unpatched_loss = 0.0\n",
    "#                         print(f\"⚠️  No logits available for {model_name}, using zero loss\")\n",
    "\n",
    "#                 else:\n",
    "#                     # For regular language models\n",
    "#                     labels = create_labels(inputs['input_ids'], tokenizer.pad_token_id)\n",
    "#                     unpatched_outputs = language_model(**inputs, labels=labels)\n",
    "\n",
    "#                     if hasattr(unpatched_outputs, 'loss') and unpatched_outputs.loss is not None:\n",
    "#                         unpatched_loss = unpatched_outputs.loss.item()\n",
    "#                     else:\n",
    "#                         # FIXED: Same robust loss computation as above\n",
    "#                         if hasattr(unpatched_outputs, 'logits'):\n",
    "#                             logits = unpatched_outputs.logits\n",
    "\n",
    "#                             if logits.size(1) > 1 and labels.size(1) > 1:\n",
    "#                                 shift_logits = logits[..., :-1, :].contiguous()\n",
    "#                                 shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "#                                 valid_mask = shift_labels != -100\n",
    "#                                 if valid_mask.any():\n",
    "#                                     shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "#                                     shift_labels = shift_labels.view(-1)\n",
    "\n",
    "#                                     loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "#                                     unpatched_loss = loss_fct(shift_logits, shift_labels).item()\n",
    "#                                 else:\n",
    "#                                     unpatched_loss = 0.0\n",
    "#                             else:\n",
    "#                                 unpatched_loss = 0.0\n",
    "#                         else:\n",
    "#                             unpatched_loss = 0.0\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"⚠️  Error computing unpatched loss: {e}\")\n",
    "#                 unpatched_loss = 0.0\n",
    "\n",
    "        # Extract activations from target layer\n",
    "        activations = None\n",
    "        # patched_loss = unpatched_loss  # DEFAULT if patching were enabled (kept for reference)\n",
    "\n",
    "        def activation_hook(module, input, output):\n",
    "            nonlocal activations\n",
    "            try:\n",
    "                if isinstance(output, tuple):\n",
    "                    activations = output[0].clone().detach()\n",
    "                else:\n",
    "                    activations = output.clone().detach()\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Error in activation hook: {e}\")\n",
    "\n",
    "        # FIXED: More robust layer identification\n",
    "        target_layer = None\n",
    "        try:\n",
    "            if hasattr(language_model, 'model') and hasattr(language_model.model, 'layers'):\n",
    "                if layer < len(language_model.model.layers):\n",
    "                    target_layer = language_model.model.layers[layer]\n",
    "                else:\n",
    "                    print(f\"❌ Layer {layer} out of range. Model has {len(language_model.model.layers)} layers\")\n",
    "                    return torch.randn(1, 64, 2304).to(self.device), 0.0\n",
    "            elif hasattr(language_model, 'layers'):\n",
    "                if layer < len(language_model.layers):\n",
    "                    target_layer = language_model.layers[layer]\n",
    "                else:\n",
    "                    print(f\"❌ Layer {layer} out of range. Model has {len(language_model.layers)} layers\")\n",
    "                    return torch.randn(1, 64, 2304).to(self.device), 0.0\n",
    "            else:\n",
    "                print(f\"❌ Could not find layers in model structure\")\n",
    "                return torch.randn(1, 64, 2304).to(self.device), 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error accessing layer {layer}: {e}\")\n",
    "            return torch.randn(1, 64, 2304).to(self.device), 0.0\n",
    "\n",
    "        if target_layer is None:\n",
    "            print(f\"❌ Could not find layer {layer}\")\n",
    "            return torch.randn(1, 64, 2304).to(self.device), 0.0\n",
    "\n",
    "        hook = target_layer.register_forward_hook(activation_hook)\n",
    "\n",
    "        # Forward pass to get activations\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                if \"paligemma\" in model_name.lower():\n",
    "                    _ = language_model(**inputs)\n",
    "                else:\n",
    "                    _ = language_model(**inputs)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Error in activation extraction: {e}\")\n",
    "\n",
    "        hook.remove()\n",
    "\n",
    "        # === PATCHING DISABLED ===\n",
    "        # Compute patched loss if SAE is provided\n",
    "        # if sae is not None and activations is not None:\n",
    "        #     patched_loss = self._compute_patched_loss(\n",
    "        #         language_model, inputs, activations, sae, layer, model_name, tokenizer\n",
    "        #     )\n",
    "\n",
    "        # model_delta_loss = patched_loss - unpatched_loss\n",
    "        model_delta_loss = 0.0  # Always return 0 for delta_loss when patching is disabled\n",
    "\n",
    "        if activations is None:\n",
    "            print(f\"⚠️  Failed to extract activations from layer {layer}\")\n",
    "            # FIXED: Return appropriate tensor size based on model\n",
    "            try:\n",
    "                # Try to get the actual hidden size from the model config\n",
    "                if hasattr(language_model, 'config') and hasattr(language_model.config, 'hidden_size'):\n",
    "                    hidden_size = language_model.config.hidden_size\n",
    "                else:\n",
    "                    hidden_size = 2304  # fallback\n",
    "                activations = torch.randn(1, 64, hidden_size).to(self.device)\n",
    "            except:\n",
    "                activations = torch.randn(1, 64, 2304).to(self.device)\n",
    "\n",
    "        return activations, model_delta_loss\n",
    "\n",
    "\n",
    "    def _compute_patched_loss(self, \n",
    "                            language_model, \n",
    "                            inputs: Dict, \n",
    "                            original_activations: torch.Tensor, \n",
    "                            sae: SAE, \n",
    "                            layer: int,\n",
    "                            model_name: str,\n",
    "                            tokenizer) -> float:\n",
    "        \"\"\"Compute loss with SAE-patched activations. FIXED: Robust error handling and loss computation.\"\"\"\n",
    "        try:\n",
    "            # Get SAE reconstruction\n",
    "            flat_activations = original_activations.view(-1, original_activations.size(-1))\n",
    "            print(f\"Activations shape: {flat_activations.shape}\")\n",
    "\n",
    "            sae_output = sae(flat_activations)\n",
    "\n",
    "            # Handle different SAE output formats\n",
    "            if hasattr(sae_output, 'sae_out'):\n",
    "                reconstructed = sae_output.sae_out\n",
    "            elif isinstance(sae_output, tuple):\n",
    "                reconstructed = sae_output[0]\n",
    "            else:\n",
    "                reconstructed = sae_output\n",
    "\n",
    "            # Reshape back to original shape\n",
    "            reconstructed = reconstructed.view(original_activations.shape)\n",
    "\n",
    "            # Patch the reconstructed activations back into the model\n",
    "            patched_activations = reconstructed.detach()  # FIXED: Ensure no gradients\n",
    "\n",
    "            # Create a patching hook\n",
    "            def patching_hook(module, input, output):\n",
    "                try:\n",
    "                    if isinstance(output, tuple):\n",
    "                        return (patched_activations, *output[1:])\n",
    "                    else:\n",
    "                        return patched_activations\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️  Error in patching hook: {e}\")\n",
    "                    return output  # Return original if patching fails\n",
    "\n",
    "            # Hook the target layer for patching\n",
    "            target_layer = None\n",
    "            if hasattr(language_model, 'model') and hasattr(language_model.model, 'layers'):\n",
    "                if layer < len(language_model.model.layers):\n",
    "                    target_layer = language_model.model.layers[layer]\n",
    "            elif hasattr(language_model, 'layers'):\n",
    "                if layer < len(language_model.layers):\n",
    "                    target_layer = language_model.layers[layer]\n",
    "\n",
    "            if target_layer is None:\n",
    "                return 0.0\n",
    "\n",
    "            patch_hook = target_layer.register_forward_hook(patching_hook)\n",
    "\n",
    "            # FIXED: Use the same robust label creation as in main function\n",
    "            def create_labels(input_ids, pad_token_id):\n",
    "                labels = input_ids.clone()\n",
    "                labels[labels == pad_token_id] = -100\n",
    "                if labels.size(1) > 1:\n",
    "                    labels[:, 0] = -100\n",
    "                return labels\n",
    "\n",
    "            # Forward pass with patched activations\n",
    "            patched_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                if \"paligemma\" in model_name.lower():\n",
    "                    labels = create_labels(inputs['input_ids'], tokenizer.pad_token_id)\n",
    "                    patched_outputs = language_model(**inputs)\n",
    "\n",
    "                    if hasattr(patched_outputs, 'logits'):\n",
    "                        logits = patched_outputs.logits\n",
    "\n",
    "                        if logits.size(1) > 1 and labels.size(1) > 1:\n",
    "                            shift_logits = logits[..., :-1, :].contiguous()\n",
    "                            shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "                            valid_mask = shift_labels != -100\n",
    "                            if valid_mask.any():\n",
    "                                shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "                                shift_labels = shift_labels.view(-1)\n",
    "\n",
    "                                loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "                                patched_loss = loss_fct(shift_logits, shift_labels).item()\n",
    "                    else:\n",
    "                        patched_loss = 0.0\n",
    "                else:\n",
    "                    labels = create_labels(inputs['input_ids'], tokenizer.pad_token_id)\n",
    "                    patched_outputs = language_model(**inputs, labels=labels)\n",
    "\n",
    "                    if hasattr(patched_outputs, 'loss') and patched_outputs.loss is not None:\n",
    "                        patched_loss = patched_outputs.loss.item()\n",
    "                    else:\n",
    "                        if hasattr(patched_outputs, 'logits'):\n",
    "                            logits = patched_outputs.logits\n",
    "\n",
    "                            if logits.size(1) > 1 and labels.size(1) > 1:\n",
    "                                shift_logits = logits[..., :-1, :].contiguous()\n",
    "                                shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "                                valid_mask = shift_labels != -100\n",
    "                                if valid_mask.any():\n",
    "                                    shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "                                    shift_labels = shift_labels.view(-1)\n",
    "\n",
    "                                    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "                                    patched_loss = loss_fct(shift_logits, shift_labels).item()\n",
    "\n",
    "            patch_hook.remove()\n",
    "            return patched_loss\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Patching failed: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def compute_sae_metrics(self, activations: torch.Tensor, sae: SAE, model_delta_loss: float) -> SAEMetrics:\n",
    "        \"\"\"Compute comprehensive SAE evaluation metrics including model delta loss and top-20 features.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Reshape activations for SAE processing\n",
    "            batch_size, seq_len, d_model = activations.shape\n",
    "            flat_activations = activations.view(-1, d_model)\n",
    "            \n",
    "            # Forward pass through SAE\n",
    "            sae_output = sae(flat_activations) \n",
    "            \n",
    "            # Handle different SAE output formats\n",
    "            if hasattr(sae_output, 'feature_acts'):\n",
    "                feature_acts = sae_output.feature_acts # shape (batch_size * seq_len,  latent_dim)\n",
    "                reconstructed = sae_output.sae_out\n",
    "            elif isinstance(sae_output, tuple) and len(sae_output) >= 2:\n",
    "                reconstructed, feature_acts = sae_output[0], sae_output[1]\n",
    "            elif hasattr(sae, 'encode') and hasattr(sae, 'decode'):\n",
    "                feature_acts = sae.encode(flat_activations)\n",
    "                reconstructed = sae.decode(feature_acts)\n",
    "            else:\n",
    "                reconstructed = sae_output\n",
    "                if hasattr(sae, 'W_enc') and hasattr(sae, 'b_enc'):\n",
    "                    feature_acts = torch.relu(flat_activations @ sae.W_enc + sae.b_enc)\n",
    "                else:\n",
    "                    print(f\"Failed retrieving SAE reconstructions, random intialisign...\")\n",
    "                    feature_acts = torch.randn(flat_activations.shape[0], 16384, device=flat_activations.device)\n",
    "            \n",
    "            # 1. Reconstruction Loss (MSE)\n",
    "            reconstruction_loss = torch.nn.functional.mse_loss(reconstructed, flat_activations).item()\n",
    "            \n",
    "            # 2. L0 Sparsity (fraction of non-zero features)\n",
    "            l0_sparsity = (feature_acts > 0).float().mean().item()\n",
    "            \n",
    "            # 3. L1 Sparsity (mean absolute activation)\n",
    "            l1_sparsity = feature_acts.abs().mean().item()\n",
    "            \n",
    "            # 4. Fraction of features that are ever active\n",
    "            fraction_alive = (feature_acts.max(dim=0)[0] > 0).float().mean().item()\n",
    "            \n",
    "            # 5. Mean maximum activation per sample\n",
    "            mean_max_activation = feature_acts.max(dim=1)[0].mean().item()\n",
    "            \n",
    "            # 6. Reconstruction score (explained variance)\n",
    "            var_original = flat_activations.var(dim=0).mean()\n",
    "            var_residual = (flat_activations - reconstructed).var(dim=0).mean()\n",
    "            reconstruction_score = max(0.0, 1 - (var_residual / var_original).item())\n",
    "            \n",
    "            # Store top-20 features for analysis\n",
    "            mean_feature_acts = feature_acts.mean(dim=0)  # Average across all tokens/samples\n",
    "            top_20_indices = torch.topk(mean_feature_acts, k=min(20, feature_acts.size(-1)))[1]\n",
    "            self._store_top_features(top_20_indices, mean_feature_acts, \n",
    "                                   reconstruction_loss, l0_sparsity, model_delta_loss)\n",
    "            \n",
    "            # top-20 rec loss\n",
    "            top_acts = feature_acts[..., top_20_indices] # shape (batch_size * seq_len,  latent_dim)\n",
    "            if hasattr(sae, 'decode'):\n",
    "                latent_dim = feature_acts.size(-1)  # e.g., 16384\n",
    "                z_sparse = torch.zeros(feature_acts.size(0), latent_dim,\n",
    "                                       device=feature_acts.device, dtype=feature_acts.dtype)\n",
    "                z_sparse[:, top_20_indices] = top_acts  # place the 20 activations at their true indices\n",
    "                recon_from_topk = sae.decode(z_sparse)  # ✅ correct shape\n",
    "#                 recon_from_topk = sae.decode( top_acts )  # if your SAE supports that\n",
    "            else:\n",
    "                if hasattr(sae, 'W_dec') and hasattr(sae, 'b_dec'):\n",
    "                    # Select the relevant columns from W_dec for the top-20 features\n",
    "                    W_dec_topk = sae.W_dec[:, top_20_indices]  # Select columns corresponding to top-20 activations\n",
    "\n",
    "                    # If necessary, apply a bias term (assuming b_dec is shared across all features)\n",
    "                    b_dec_topk = sae.b_dec  # Bias term stays the same for all activations\n",
    "                    print(top_acts.shape, W_dec_topk.shape, b_dec_topk.shape)\n",
    "                    # Reconstruct the activations from the top-20 features\n",
    "                    recon_from_topk = torch.relu(top_acts @ W_dec_topk + b_dec_topk)\n",
    "            \n",
    "            rec_loss_topk = F.mse_loss(recon_from_topk, flat_activations).item()\n",
    "            \n",
    "            return SAEMetrics(\n",
    "                reconstruction_loss=reconstruction_loss,\n",
    "                l0_sparsity=l0_sparsity,\n",
    "                l1_sparsity=l1_sparsity,\n",
    "                fraction_alive=fraction_alive,\n",
    "                mean_max_activation=mean_max_activation,\n",
    "                reconstruction_score=reconstruction_score,\n",
    "                model_delta_loss=model_delta_loss,\n",
    "                rec_loss_topk=rec_loss_topk\n",
    "            )\n",
    "    \n",
    "    def _store_top_features(self, top_indices: torch.Tensor, feature_acts: torch.Tensor, \n",
    "                           recon_loss: float, sparsity: float, delta_loss: float):\n",
    "        \"\"\"Store top-20 activated features for analysis.\"\"\"\n",
    "        if not hasattr(self, 'top_features_log'):\n",
    "            self.top_features_log = []\n",
    "        \n",
    "        top_features_info = {\n",
    "            'top_20_indices': top_indices.cpu().tolist(),\n",
    "            'top_20_activations': feature_acts[top_indices].cpu().tolist(),\n",
    "            'reconstruction_loss': recon_loss,\n",
    "            'sparsity': sparsity,\n",
    "            'delta_loss': delta_loss,\n",
    "            'timestamp': len(self.top_features_log)  # Simple counter\n",
    "        }\n",
    "        \n",
    "        self.top_features_log.append(top_features_info)\n",
    "\n",
    "    def analyze_layer_sweep(self, \n",
    "                           model1_name: str, \n",
    "                           model2_name: str, \n",
    "                           texts: List[str],\n",
    "                           layers: List[int] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform memory-efficient layer sweep analysis.\n",
    "        \n",
    "        Args:\n",
    "            model1_name: First model (base LLM)\n",
    "            model2_name: Second model (VLM) \n",
    "            texts: List of texts to analyze\n",
    "            layers: List of layers to analyze (default: [8, 12, 16, 20])\n",
    "        \"\"\"\n",
    "        if layers is None:\n",
    "            layers = [8, 12, 16, 20]  # Sample layers across the model\n",
    "        \n",
    "        print(f\"🚀 Starting Layer Sweep Analysis\")\n",
    "        print(f\"   Model 1: {model1_name}\")\n",
    "        print(f\"   Model 2: {model2_name}\")\n",
    "        print(f\"   Layers: {layers}\")\n",
    "        print(f\"   Texts: {len(texts)} samples\")\n",
    "        print(f\"   Memory: {torch.cuda.memory_allocated() / 1e9:.2f}GB\" if torch.cuda.is_available() else \"\")\n",
    "        \n",
    "        results = {\n",
    "            'layers': layers,\n",
    "            'layer_results': {},\n",
    "            'texts': texts[:10]  # Store subset for reference\n",
    "        }\n",
    "        \n",
    "        for layer in tqdm(layers, desc=\"Processing layers\"):\n",
    "            print(f\"\\n📊 Processing Layer {layer}\")\n",
    "            \n",
    "            # Load SAE for this layer\n",
    "            #############\n",
    "            sae = self.get_gemmascope_sae(layer-1)\n",
    "     #############\n",
    "\n",
    "            layer_metrics = {\n",
    "                'model1_metrics': [],\n",
    "                'model2_metrics': [],\n",
    "                'shift_metrics': []\n",
    "            }\n",
    "            \n",
    "            # Process subset of texts for each layer (memory efficiency)\n",
    "            sample_texts = texts[:100]  # Process 100 texts per layer (increased from 20)\n",
    "            \n",
    "            for i, text in enumerate(tqdm(sample_texts, desc=f\"Layer {layer} texts\", leave=False)):\n",
    "                try:\n",
    "                    # Extract activations and compute metrics for model 1\n",
    "                    acts1, delta_loss1 = self.extract_activations_with_patching(\n",
    "                        model1_name, text, layer, sae\n",
    "                    )\n",
    "                    metrics1 = self.compute_sae_metrics(acts1, sae, delta_loss1)\n",
    "                    \n",
    "                    # Extract activations and compute metrics for model 2\n",
    "                    acts2, delta_loss2 = self.extract_activations_with_patching(\n",
    "                        model2_name, text, layer, sae\n",
    "                    )\n",
    "                    metrics2 = self.compute_sae_metrics(acts2, sae, delta_loss2)\n",
    "                    \n",
    "                    # Compute representation shift\n",
    "                    shift = self.compute_representation_shift(acts1, acts2, sae)\n",
    "                    \n",
    "                    layer_metrics['model1_metrics'].append(metrics1)\n",
    "                    layer_metrics['model2_metrics'].append(metrics2)\n",
    "                    layer_metrics['shift_metrics'].append(shift)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️  Error processing text {i} in layer {layer}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Compute layer-level aggregates\n",
    "            layer_metrics['aggregate'] = self._compute_layer_aggregate(layer_metrics)\n",
    "            results['layer_results'][layer] = layer_metrics\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"   Memory after layer {layer}: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n",
    "        \n",
    "        # Compute overall analysis\n",
    "        results['overall_analysis'] = self._compute_overall_analysis(results)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def compute_representation_shift(self, \n",
    "                                   activations1: torch.Tensor, \n",
    "                                   activations2: torch.Tensor,\n",
    "                                   sae: SAE) -> RepresentationShift:\n",
    "        \"\"\"Compute representation shift metrics using SAE features.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Process both activation sets through SAE\n",
    "            flat_acts1 = activations1.view(-1, activations1.size(-1))\n",
    "            flat_acts2 = activations2.view(-1, activations2.size(-1))\n",
    "            \n",
    "            # Get SAE features\n",
    "            def extract_features(flat_acts):\n",
    "                sae_output = sae(flat_acts)\n",
    "                if hasattr(sae_output, 'feature_acts'):\n",
    "                    return sae_output.feature_acts\n",
    "                elif isinstance(sae_output, tuple) and len(sae_output) >= 2:\n",
    "                    return sae_output[1]\n",
    "                elif hasattr(sae, 'encode'):\n",
    "                    return sae.encode(flat_acts)\n",
    "                else:\n",
    "                    if hasattr(sae, 'W_enc') and hasattr(sae, 'b_enc'):\n",
    "                        print(f\"flat_acts: {flat_acts.shape}, sae.W_enc: {sae.W_enc.shape}, sae.b_enc: {sae.b_enc.shape}\")\n",
    "                        return torch.relu(flat_acts @ sae.W_enc + sae.b_enc)\n",
    "                    else:\n",
    "                        return torch.randn(flat_acts.shape[0], 16384, device=flat_acts.device)\n",
    "            \n",
    "            features1 = extract_features(flat_acts1)\n",
    "            features2 = extract_features(flat_acts2)\n",
    "            \n",
    "            # 1. Cosine similarity\n",
    "            cosine_sim = torch.nn.functional.cosine_similarity(\n",
    "                features1.mean(dim=0), features2.mean(dim=0), dim=0\n",
    "            ).item()\n",
    "            \n",
    "            # 2. L2 distance\n",
    "            l2_distance = torch.norm(features1.mean(dim=0) - features2.mean(dim=0), p=2).item()\n",
    "            \n",
    "            # 3. Feature overlap (Jaccard similarity)\n",
    "            active1 = (features1 > 0).float()\n",
    "            active2 = (features2 > 0).float()\n",
    "            intersection = (active1 * active2).sum(dim=0)\n",
    "            union = torch.clamp(active1.sum(dim=0) + active2.sum(dim=0) - intersection, min=1)\n",
    "            feature_overlap = (intersection / union).mean().item()\n",
    "            \n",
    "            # 4. Jensen-Shannon divergence\n",
    "            def js_divergence(p, q):\n",
    "                p = p + 1e-8\n",
    "                q = q + 1e-8\n",
    "                p = p / p.sum()\n",
    "                q = q / q.sum()\n",
    "                m = 0.5 * (p + q)\n",
    "                return 0.5 * (torch.nn.functional.kl_div(p.log(), m, reduction='sum') + \n",
    "                             torch.nn.functional.kl_div(q.log(), m, reduction='sum'))\n",
    "            \n",
    "            p = features1.mean(dim=0).abs()\n",
    "            q = features2.mean(dim=0).abs()\n",
    "            js_div = js_divergence(p, q).item()\n",
    "            \n",
    "            # 5. Feature correlation\n",
    "            try:\n",
    "                corr_matrix = torch.corrcoef(torch.stack([\n",
    "                    features1.mean(dim=0), features2.mean(dim=0)\n",
    "                ]))\n",
    "                feature_correlation = corr_matrix[0, 1].item() if not torch.isnan(corr_matrix[0, 1]) else 0.0\n",
    "            except:\n",
    "                feature_correlation = 0.0\n",
    "            \n",
    "            return RepresentationShift(\n",
    "                cosine_similarity=cosine_sim,\n",
    "                l2_distance=l2_distance,\n",
    "                feature_overlap=feature_overlap,\n",
    "                js_divergence=js_div,\n",
    "                feature_correlation=feature_correlation\n",
    "            )\n",
    "\n",
    "    def _compute_layer_aggregate(self, layer_metrics: Dict) -> Dict:\n",
    "        \"\"\"Compute aggregate statistics for a single layer.\"\"\"\n",
    "        n_samples = len(layer_metrics['model1_metrics'])\n",
    "        if n_samples == 0:\n",
    "            return {}\n",
    "        \n",
    "        # Average metrics across samples\n",
    "        avg_model1 = {}\n",
    "        avg_model2 = {}\n",
    "        avg_shift = {}\n",
    "        \n",
    "        for field in SAEMetrics.__dataclass_fields__:\n",
    "            avg_model1[field] = np.mean([getattr(m, field) for m in layer_metrics['model1_metrics']])\n",
    "            avg_model2[field] = np.mean([getattr(m, field) for m in layer_metrics['model2_metrics']])\n",
    "        \n",
    "        for field in RepresentationShift.__dataclass_fields__:\n",
    "            avg_shift[field] = np.mean([getattr(s, field) for s in layer_metrics['shift_metrics']])\n",
    "        \n",
    "        return {\n",
    "            'avg_model1_metrics': avg_model1,\n",
    "            'avg_model2_metrics': avg_model2,\n",
    "            'avg_shift_metrics': avg_shift,\n",
    "            'n_samples': n_samples\n",
    "        }\n",
    "\n",
    "    def _compute_overall_analysis(self, results: Dict) -> Dict:\n",
    "        \"\"\"Compute overall analysis across all layers.\"\"\"\n",
    "        layers = results['layers']\n",
    "        \n",
    "        # Collect metrics across layers\n",
    "        layer_similarities = []\n",
    "        layer_overlaps = []\n",
    "        layer_delta_losses = []\n",
    "        layer_sparsities = []\n",
    "        layer_rec_loss_topk = []\n",
    "        \n",
    "        for layer in layers:\n",
    "            if layer in results['layer_results'] and 'aggregate' in results['layer_results'][layer]:\n",
    "                agg = results['layer_results'][layer]['aggregate']\n",
    "                if agg:  # Check if aggregate is not empty\n",
    "                    layer_similarities.append(agg['avg_shift_metrics']['cosine_similarity'])\n",
    "                    layer_overlaps.append(agg['avg_shift_metrics']['feature_overlap'])\n",
    "                    layer_delta_losses.append(abs(agg['avg_model1_metrics']['model_delta_loss'] - \n",
    "                                                 agg['avg_model2_metrics']['model_delta_loss']))\n",
    "                    layer_sparsities.append((agg['avg_model1_metrics']['l0_sparsity'] + \n",
    "                                           agg['avg_model2_metrics']['l0_sparsity']) / 2)\n",
    "\n",
    "        \n",
    "        # Overall insights\n",
    "        overall = {\n",
    "            'most_similar_layer': layers[np.argmax(layer_similarities)] if layer_similarities else None,\n",
    "            'most_different_layer': layers[np.argmin(layer_similarities)] if layer_similarities else None,\n",
    "            'highest_overlap_layer': layers[np.argmax(layer_overlaps)] if layer_overlaps else None,\n",
    "            'highest_delta_loss_layer': layers[np.argmax(layer_delta_losses)] if layer_delta_losses else None,\n",
    "            'avg_similarity_across_layers': np.mean(layer_similarities) if layer_similarities else 0,\n",
    "            'avg_overlap_across_layers': np.mean(layer_overlaps) if layer_overlaps else 0,\n",
    "            'avg_delta_loss_across_layers': np.mean(layer_delta_losses) if layer_delta_losses else 0,\n",
    "            'layer_similarities': dict(zip(layers, layer_similarities)),\n",
    "            'layer_overlaps': dict(zip(layers, layer_overlaps))\n",
    "        }\n",
    "        \n",
    "        return overall\n",
    "\n",
    "    def visualize_layer_sweep_results(self, results: Dict, model1_name: str, model2_name: str):\n",
    "        \"\"\"Create comprehensive visualization of layer sweep results.\"\"\"\n",
    "        layers = results['layers']\n",
    "        \n",
    "        # Create output filename\n",
    "        model1_clean = model1_name.replace('/', '_').replace('-', '_')\n",
    "        model2_clean = model2_name.replace('/', '_').replace('-', '_')\n",
    "        save_path = self.output_dir / f\"{model1_clean}_{model2_clean}_layer_sweep.png\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "        fig.suptitle(f'SAE Layer Sweep Analysis: {model1_name} vs {model2_name}', fontsize=16)\n",
    "        \n",
    "        # Collect data across layers\n",
    "        layer_data = {\n",
    "            'similarities': [],\n",
    "            'overlaps': [],\n",
    "            'recon_losses_m1': [],\n",
    "            'recon_losses_m2': [],\n",
    "            'sparsities_m1': [],\n",
    "            'sparsities_m2': [],\n",
    "            'delta_losses_m1': [],\n",
    "            'delta_losses_m2': []\n",
    "        }\n",
    "        \n",
    "        for layer in layers:\n",
    "            if layer in results['layer_results'] and 'aggregate' in results['layer_results'][layer]:\n",
    "                agg = results['layer_results'][layer]['aggregate']\n",
    "                if agg:\n",
    "                    layer_data['similarities'].append(agg['avg_shift_metrics']['cosine_similarity'])\n",
    "                    layer_data['overlaps'].append(agg['avg_shift_metrics']['feature_overlap'])\n",
    "                    layer_data['recon_losses_m1'].append(agg['avg_model1_metrics']['reconstruction_loss'])\n",
    "                    layer_data['recon_losses_m2'].append(agg['avg_model2_metrics']['reconstruction_loss'])\n",
    "                    layer_data['sparsities_m1'].append(agg['avg_model1_metrics']['l0_sparsity'])\n",
    "                    layer_data['sparsities_m2'].append(agg['avg_model2_metrics']['l0_sparsity'])\n",
    "                    layer_data['delta_losses_m1'].append(agg['avg_model1_metrics']['model_delta_loss'])\n",
    "                    layer_data['delta_losses_m2'].append(agg['avg_model2_metrics']['model_delta_loss'])\n",
    "        \n",
    "        # Plot 1: Representation Similarity Across Layers\n",
    "        axes[0, 0].plot(layers, layer_data['similarities'], 'o-', linewidth=2, markersize=8)\n",
    "        axes[0, 0].set_title('Cosine Similarity Across Layers')\n",
    "        axes[0, 0].set_xlabel('Layer')\n",
    "        axes[0, 0].set_ylabel('Cosine Similarity')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "#         axes[0, 0].axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='High Similarity')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # Plot 2: Feature Overlap Across Layers\n",
    "        axes[0, 1].plot(layers, layer_data['overlaps'], 'o-', color='green', linewidth=2, markersize=8)\n",
    "        axes[0, 1].set_title('Feature Overlap Across Layers')\n",
    "        axes[0, 1].set_xlabel('Layer')\n",
    "        axes[0, 1].set_ylabel('Feature Overlap')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "#         axes[0, 1].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Moderate Overlap')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Plot 3: Reconstruction Loss Comparison\n",
    "#         axes[0, 2].plot(layers, layer_data['recon_losses_m1'], 'o-', label='Model 1 (LLM)', linewidth=2)\n",
    "        axes[0, 2].plot(layers, layer_data['recon_losses_m2'], 's-', label='Model 2 (VLM)', linewidth=2)\n",
    "        axes[0, 2].set_title('Reconstruction Loss Across Layers')\n",
    "        axes[0, 2].set_xlabel('Layer')\n",
    "        axes[0, 2].set_ylabel('Reconstruction Loss')\n",
    "        axes[0, 2].legend()\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Sparsity Comparison\n",
    "        axes[1, 0].plot(layers, layer_data['sparsities_m1'], 'o-', label='Model 1 (LLM)', linewidth=2)\n",
    "        axes[1, 0].plot(layers, layer_data['sparsities_m2'], 's-', label='Model 2 (VLM)', linewidth=2)\n",
    "        axes[1, 0].set_title('L0 Sparsity Across Layers')\n",
    "        axes[1, 0].set_xlabel('Layer')\n",
    "        axes[1, 0].set_ylabel('L0 Sparsity')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 5: Model Delta Loss (Patching Performance)\n",
    "        axes[1, 1].plot(layers, layer_data['delta_losses_m1'], 'o-', label='Model 1 (LLM)', linewidth=2)\n",
    "        axes[1, 1].plot(layers, layer_data['delta_losses_m2'], 's-', label='Model 2 (VLM)', linewidth=2)\n",
    "        axes[1, 1].set_title('Model Delta Loss (Patching Quality)')\n",
    "        axes[1, 1].set_xlabel('Layer')\n",
    "        axes[1, 1].set_ylabel('Delta Loss')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        # Plot 6: Summary Heatmap\n",
    "        # Create a summary matrix for visualization\n",
    "        metrics_matrix = np.array([\n",
    "            layer_data['similarities'],\n",
    "            layer_data['overlaps'],\n",
    "            np.array(layer_data['recon_losses_m1']) / max(max(layer_data['recon_losses_m1']), 1e-6),  # Normalize\n",
    "            np.array(layer_data['sparsities_m1']) * 10,  # Scale up for visibility\n",
    "        ])\n",
    "        \n",
    "        im = axes[1, 2].imshow(metrics_matrix, cmap='RdYlBu_r', aspect='auto')\n",
    "        axes[1, 2].set_title('Metrics Heatmap Across Layers')\n",
    "        axes[1, 2].set_xlabel('Layer Index')\n",
    "        axes[1, 2].set_yticks(range(4))\n",
    "        axes[1, 2].set_yticklabels(['Similarity', 'Overlap', 'Recon Loss (norm)', 'Sparsity (x10)'])\n",
    "        axes[1, 2].set_xticks(range(len(layers)))\n",
    "        axes[1, 2].set_xticklabels([f'L{l}' for l in layers])\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=axes[1, 2])\n",
    "        cbar.set_label('Metric Value')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✅ Layer sweep visualization saved to {save_path}\")\n",
    "        \n",
    "        # Save detailed results as JSON including top features\n",
    "        json_path = self.output_dir / f\"{model1_clean}_{model2_clean}_results.json\"\n",
    "        \n",
    "        # Convert results to JSON-serializable format\n",
    "        json_results = {\n",
    "            'layers': layers,\n",
    "            'overall_analysis': results['overall_analysis'],\n",
    "            'layer_summaries': {},\n",
    "            'top_features_analysis': getattr(self, 'top_features_log', [])\n",
    "        }\n",
    "        \n",
    "        for layer in layers:\n",
    "            if layer in results['layer_results'] and 'aggregate' in results['layer_results'][layer]:\n",
    "                agg = results['layer_results'][layer]['aggregate']\n",
    "                if agg:\n",
    "                    json_results['layer_summaries'][str(layer)] = agg\n",
    "        \n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(json_results, f, indent=2)\n",
    "        print(f\"✅ Detailed results saved to {json_path}\")\n",
    "        \n",
    "        # Create top features analysis\n",
    "#         self._analyze_top_features_trends()\n",
    "\n",
    "    def interpret_layer_sweep_results(self, results: Dict) -> Dict[str, str]:\n",
    "        \"\"\"Provide interpretation of layer sweep results.\"\"\"\n",
    "        overall = results['overall_analysis']\n",
    "        interpretations = {}\n",
    "        \n",
    "        # Overall adaptation assessment\n",
    "        avg_similarity = overall['avg_similarity_across_layers']\n",
    "        if avg_similarity > 0.85:\n",
    "            interpretations['adaptation_magnitude'] = \"✅ MINIMAL LLM→VLM adaptation - representations largely preserved\"\n",
    "        elif avg_similarity > 0.7:\n",
    "            interpretations['adaptation_magnitude'] = \"⚠️ MODERATE LLM→VLM adaptation - selective representational changes\"\n",
    "        else:\n",
    "            interpretations['adaptation_magnitude'] = \"🔍 SIGNIFICANT LLM→VLM adaptation - substantial representational reorganization\"\n",
    "        \n",
    "        # Layer-specific insights\n",
    "        if overall['most_different_layer'] is not None:\n",
    "            interpretations['adaptation_location'] = f\"🎯 Layer {overall['most_different_layer']} shows maximum adaptation\"\n",
    "        \n",
    "        if overall['highest_overlap_layer'] is not None:\n",
    "            interpretations['feature_preservation'] = f\"🔗 Layer {overall['highest_overlap_layer']} best preserves LLM features\"\n",
    "        \n",
    "        # Adaptation pattern\n",
    "        layer_sims = list(overall['layer_similarities'].values())\n",
    "        if len(layer_sims) >= 3:\n",
    "            early_sim = np.mean(layer_sims[:len(layer_sims)//3])\n",
    "            late_sim = np.mean(layer_sims[-len(layer_sims)//3:])\n",
    "            \n",
    "            if early_sim > late_sim + 0.1:\n",
    "                interpretations['adaptation_pattern'] = \"📈 Early layers preserve LLM representations better than late layers\"\n",
    "            elif late_sim > early_sim + 0.1:\n",
    "                interpretations['adaptation_pattern'] = \"📉 Late layers preserve LLM representations better than early layers\"\n",
    "            else:\n",
    "                interpretations['adaptation_pattern'] = \"📊 Uniform adaptation pattern across layers\"\n",
    "        \n",
    "        # SAE quality assessment\n",
    "        avg_delta_loss = overall['avg_delta_loss_across_layers']\n",
    "        if avg_delta_loss < 0.1:\n",
    "            interpretations['sae_quality'] = \"✅ SAE reconstructions preserve model functionality well\"\n",
    "        elif avg_delta_loss < 0.5:\n",
    "            interpretations['sae_quality'] = \"⚠️ SAE reconstructions cause moderate functional degradation\"\n",
    "        else:\n",
    "            interpretations['sae_quality'] = \"❌ SAE reconstructions significantly impact model functionality\"\n",
    "        \n",
    "        return interpretations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c66b6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-21T14:46:51.112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Comprehensive SAE Layer Sweep Analysis: LLM→VLM Adaptation\n",
      "======================================================================\n",
      "🔧 Initialized SAE Analyzer\n",
      "   Device: cuda\n",
      "   Model Size: 2b\n",
      "   SAE Width: 16k\n",
      "   Output Dir: ../figs_tabs\n",
      "\n",
      "📚 Loading Datasets...\n",
      "✅ Loaded 666 CIFAR-100 captions\n",
      "❌ Error loading COCO: 'utf-8' codec can't decode byte 0xc4 in position 4: invalid continuation byte\n",
      "❌ Error loading LLaVA-Bench: Config name is missing.\n",
      "Please pick one among the available configs: ['CLEVR-Math(MathV360K)', 'Evol-Instruct-GPT4-Turbo', 'FigureQA(MathV360K)', 'GEOS(MathV360K)', 'GeoQA+(MathV360K)', 'Geometry3K(MathV360K)', 'IconQA(MathV360K)', 'MapQA(MathV360K)', 'MathV360K_TQA', 'MathV360K_VQA-AS', 'MathV360K_VQA-RAD', 'PMC-VQA(MathV360K)', 'Super-CLEVR(MathV360K)', 'TabMWP(MathV360K)', 'UniGeo(MathV360K)', 'VisualWebInstruct(filtered)', 'VizWiz(MathV360K)', 'ai2d(cauldron,llava_format)', 'ai2d(gpt4v)', 'ai2d(internvl)', 'allava_instruct_laion4v', 'allava_instruct_vflan4v', 'aokvqa(cauldron,llava_format)', 'chart2text(cauldron)', 'chartqa(cauldron,llava_format)', 'chrome_writting', 'clevr(cauldron,llava_format)', 'diagram_image_to_text(cauldron)', 'dvqa(cauldron,llava_format)', 'figureqa(cauldron,llava_format)', 'geo170k(align)', 'geo170k(qa)', 'geo3k', 'geomverse(cauldron)', 'hateful_memes(cauldron,llava_format)', 'hitab(cauldron,llava_format)', 'hme100k', 'iam(cauldron)', 'iconqa(cauldron,llava_format)', 'iiit5k', 'image_textualization(filtered)', 'infographic(gpt4v)', 'infographic_vqa', 'infographic_vqa_llava_format', 'intergps(cauldron,llava_format)', 'k12_printing', 'llava_wild_4v_12k_filtered', 'llava_wild_4v_39k_filtered', 'llavar_gpt4_20k', 'lrv_chart', 'lrv_normal(filtered)', 'magpie_pro(l3_80b_mt)', 'magpie_pro(l3_80b_st)', 'magpie_pro(qwen2_72b_st)', 'mapqa(cauldron,llava_format)', 'mathqa', 'mavis_math_metagen', 'mavis_math_rule_geo', 'multihiertt(cauldron)', 'orand_car_a', 'raven(cauldron)', 'rendered_text(cauldron)', 'robut_sqa(cauldron)', 'robut_wikisql(cauldron)', 'robut_wtq(cauldron,llava_format)', 'scienceqa(cauldron,llava_format)', 'scienceqa(nona_context)', 'screen2words(cauldron)', 'sharegpt4o', 'sharegpt4v(coco)', 'sharegpt4v(knowledge)', 'sharegpt4v(llava)', 'sharegpt4v(sam)', 'sroie', 'st_vqa(cauldron,llava_format)', 'tabmwp(cauldron)', 'tallyqa(cauldron,llava_format)', 'textcaps', 'textocr(gpt4v)', 'tqa(cauldron,llava_format)', 'ureader_cap', 'ureader_ie', 'vision_flan(filtered)', 'vistext(cauldron)', 'visual7w(cauldron,llava_format)', 'visualmrc(cauldron)', 'vqarad(cauldron,llava_format)', 'vsr(cauldron,llava_format)', 'websight(cauldron)']\n",
      "Example of usage:\n",
      "\t`load_dataset('lmms-lab/LLaVA-OneVision-Data', 'CLEVR-Math(MathV360K)')`\n",
      "✅ Loaded 353 texts from mixed datasets\n",
      "Sample texts: ['This is a photo of a boy.', 'An image showing a snail.', 'This is a photo of a porcupine.']\n",
      "\n",
      "🔬 Research Configuration:\n",
      "   Model 1 (LLM): google/gemma-2-2b\n",
      "   Model 2 (VLM): google/paligemma2-3b-pt-224\n",
      "   Layers to analyze: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "   SAE Configuration: 2b-16k-canonical\n",
      "   Device: cuda\n",
      "   Total texts: 353\n",
      "\n",
      "🚀 Starting Layer Sweep Analysis...\n",
      "🚀 Starting Layer Sweep Analysis\n",
      "   Model 1: google/gemma-2-2b\n",
      "   Model 2: google/paligemma2-3b-pt-224\n",
      "   Layers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "   Texts: 353 samples\n",
      "   Memory: 0.00GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing layers:   0%|          | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Processing Layer 1\n",
      "   📥 Loading SAE Layer 0: layer_0/width_16k/canonical\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 Comprehensive SAE Layer Sweep Analysis: LLM→VLM Adaptation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration\n",
    "MODEL_SIZE = \"2b\"\n",
    "WIDTH = \"16k\"\n",
    "SUFFIX = \"canonical\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     LAYERS = [4, 8, 12, 16, 20, 24]  # Sample across the model depth\n",
    "LAYERS = list(range(1,27)) # for SAE, should be 0-25\n",
    "try:\n",
    "    # Initialize analyzer\n",
    "    analyzer = MemoryEfficientSAEAnalyzer(\n",
    "        model_size=MODEL_SIZE,\n",
    "        width=WIDTH,\n",
    "        suffix=SUFFIX,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    # Load dataset\n",
    "    print(\"\\n📚 Loading Datasets...\")\n",
    "    dataset_loader = DatasetLoader(device=DEVICE)\n",
    "    texts = dataset_loader.get_mixed_dataset(total_samples=1000)  # Use 1K data as requested\n",
    "\n",
    "    print(f\"✅ Loaded {len(texts)} texts from mixed datasets\")\n",
    "    print(f\"Sample texts: {texts[:3]}\")\n",
    "\n",
    "    # Model configuration for LLM->VLM comparison\n",
    "    model1_name = \"google/gemma-2-2b\"  # Base Gemma-2-2B (LLM)\n",
    "    model2_name = \"google/paligemma2-3b-pt-224\"  # PaliGemma with Gemma-2-2B decoder (VLM)\n",
    "\n",
    "    print(f\"\\n🔬 Research Configuration:\")\n",
    "    print(f\"   Model 1 (LLM): {model1_name}\")\n",
    "    print(f\"   Model 2 (VLM): {model2_name}\")\n",
    "    print(f\"   Layers to analyze: {LAYERS}\")\n",
    "    print(f\"   SAE Configuration: {MODEL_SIZE}-{WIDTH}-{SUFFIX}\")\n",
    "    print(f\"   Device: {DEVICE}\")\n",
    "    print(f\"   Total texts: {len(texts)}\")\n",
    "\n",
    "    # Run layer sweep analysis\n",
    "    print(f\"\\n🚀 Starting Layer Sweep Analysis...\")\n",
    "    results = analyzer.analyze_layer_sweep(\n",
    "        model1_name=model1_name,\n",
    "        model2_name=model2_name,\n",
    "        texts=texts,\n",
    "        layers=LAYERS\n",
    "    )\n",
    "\n",
    "    # Generate interpretations\n",
    "    interpretations = analyzer.interpret_layer_sweep_results(results)\n",
    "\n",
    "    print(f\"\\n📊 LAYER SWEEP RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    overall = results['overall_analysis']\n",
    "    print(f\"Most Similar Layer: {overall['most_similar_layer']}\")\n",
    "    print(f\"Most Different Layer: {overall['most_different_layer']}\")\n",
    "    print(f\"Highest Feature Overlap Layer: {overall['highest_overlap_layer']}\")\n",
    "    print(f\"Average Similarity Across Layers: {overall['avg_similarity_across_layers']:.3f}\")\n",
    "    print(f\"Average Feature Overlap: {overall['avg_overlap_across_layers']:.3f}\")\n",
    "\n",
    "    print(f\"\\n🔍 INTERPRETATIONS:\")\n",
    "    print(\"=\" * 50)\n",
    "    for aspect, interpretation in interpretations.items():\n",
    "        print(f\"{aspect.replace('_', ' ').title()}: {interpretation}\")\n",
    "\n",
    "    # Create visualizations\n",
    "    print(f\"\\n📈 Generating Visualizations...\")\n",
    "    analyzer.visualize_layer_sweep_results(results, model1_name, model2_name)\n",
    "\n",
    "    print(f\"\\n✅ Analysis Complete!\")\n",
    "    print(f\"📁 Results saved to: {analyzer.output_dir}\")\n",
    "    print(f\"🧠 Key Finding: {interpretations.get('adaptation_magnitude', 'Analysis completed')}\")\n",
    "\n",
    "    # Memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"🔧 Final GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "    print(\"\\n💡 Troubleshooting Tips:\")\n",
    "    print(\"   1. Ensure sufficient GPU memory (8GB+ recommended)\")\n",
    "    print(\"   2. Reduce LAYERS list or sample size if out of memory\")\n",
    "    print(\"   3. Check model names are correct and accessible\")\n",
    "    print(\"   4. Install required packages: pip install sae-lens transformers datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b904b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
