{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0b78f21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T01:16:08.248388Z",
     "start_time": "2025-08-08T01:16:02.391633Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/py311/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py311/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PaliGemmaForConditionalGeneration\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import gc\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "# from random import random\n",
    "import random\n",
    "def seed_everywhere(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)    \n",
    "SEED = 42\n",
    "MAX_SEQ_LEN = 64 # location to truncate our inputs\n",
    "DEVICE_1 = 'cuda:1'\n",
    "DEVICE_2 = 'cuda:2' # the second GPU\n",
    "NUM_CLASSES = 0\n",
    "seed_everywhere(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7015005d",
   "metadata": {},
   "source": [
    "# exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bf357cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T01:16:08.277142Z",
     "start_time": "2025-08-08T01:16:08.253694Z"
    }
   },
   "outputs": [],
   "source": [
    "class ActivationDataset(Dataset):\n",
    "    \"\"\"Dataset for model activations with labels\"\"\"\n",
    "    def __init__(self, activations, labels):\n",
    "        self.activations = torch.tensor(activations, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.activations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.activations[idx], self.labels[idx]\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    \"\"\"Simple linear probe for classification\"\"\"\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class LinearProbingExperiment:\n",
    "    def __init__(self, model_name=\"gemma\", concept=\"animals\"):\n",
    "        self.model_name = model_name\n",
    "        self.concept = concept\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "#     def load_activations(self, file_path):\n",
    "#         \"\"\"Load activations from file (assuming numpy format)\"\"\"\n",
    "#         data = np.load(file_path, allow_pickle=True)\n",
    "#         return data['activations'], data['labels']\n",
    "    \n",
    "#     def create_cat_dog_labels(self, texts):\n",
    "#         \"\"\"Simple cat/dog labeler based on keywords\"\"\"\n",
    "#         labels = []\n",
    "#         for text in texts:\n",
    "#             text_lower = text.lower()\n",
    "#             if any(word in text_lower for word in ['cat', 'feline', 'kitten', 'meow']):\n",
    "#                 labels.append(0)  # cat\n",
    "#             elif any(word in text_lower for word in ['dog', 'canine', 'puppy', 'bark', 'woof']):\n",
    "#                 labels.append(1)  # dog\n",
    "#             else:\n",
    "#                 labels.append(-1)  # neither/unknown\n",
    "#         return np.array(labels)\n",
    "    \n",
    "#     def extract_gemma_activations(self, model, tokenizer, texts, layer_idx=-1):\n",
    "#         \"\"\"Extract activations from Gemma model at specified layer\"\"\"\n",
    "#         activations = []\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for text in texts:\n",
    "#                 inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "#                 inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                \n",
    "#                 # Forward pass with output_hidden_states=True\n",
    "#                 outputs = model(**inputs, output_hidden_states=True)\n",
    "#                 hidden_states = outputs.hidden_states[layer_idx]  # Get specified layer\n",
    "                \n",
    "#                 # Use mean pooling across sequence length\n",
    "#                 activation = hidden_states.mean(dim=1).cpu().numpy()\n",
    "#                 activations.append(activation[0])\n",
    "        \n",
    "#         return np.array(activations)\n",
    "    \n",
    "    def train_sklearn_probe(self,\n",
    "                             X_train: np.ndarray,\n",
    "                             y_train: np.ndarray,\n",
    "                             X_test: np.ndarray,\n",
    "                             y_test: np.ndarray,\n",
    "                             texts_test: list = None) -> tuple:\n",
    "        \"\"\"\n",
    "        Train a sklearn logistic regression probe on flattened activations,\n",
    "        and return the trained probe and a results dict including misclassified samples:\n",
    "        - train_acc, test_acc, classification_report\n",
    "        - y_test, y_pred\n",
    "        - misclassified: list of dicts with index, text, true_label, pred_label\n",
    "        \"\"\"\n",
    "        # Set random state for reproducibility\n",
    "        probe = LogisticRegression(max_iter=1000, random_state=SEED)\n",
    "\n",
    "        # Flatten sequence and hidden dimensions\n",
    "        n_train_samples = X_train.shape[0]\n",
    "        n_test_samples = X_test.shape[0]\n",
    "        X_train_flat = X_train.reshape(n_train_samples, -1)\n",
    "        X_test_flat = X_test.reshape(n_test_samples, -1)\n",
    "        print(f\"X_train_flat shape: {X_train_flat.shape}\")\n",
    "        print(f\"y_train shape: {len(y_train)}\")\n",
    "        print(f\"X_test_flat shape: {X_test_flat.shape}\")\n",
    "        print(f\"y_test shape: {len(y_test)}\")\n",
    "\n",
    "        # Fit probe\n",
    "        probe.fit(X_train_flat, y_train)\n",
    "\n",
    "        # Predict\n",
    "        train_pred = probe.predict(X_train_flat)\n",
    "        test_pred = probe.predict(X_test_flat)\n",
    "\n",
    "        # Compute metrics\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        report = classification_report(y_test, test_pred)\n",
    "\n",
    "        # Build results dict\n",
    "        results = {\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'classification_report': report,\n",
    "            'y_test': y_test.tolist() if hasattr(y_test, 'tolist') else list(y_test),\n",
    "            'y_pred': test_pred.tolist(),\n",
    "        }\n",
    "\n",
    "        # Collect misclassified samples\n",
    "        misclassified = []\n",
    "        if texts_test is not None:\n",
    "            for idx, (true_label, pred_label) in enumerate(zip(y_test, test_pred)):\n",
    "                if true_label != pred_label:\n",
    "                    misclassified.append({\n",
    "                        'index': idx,\n",
    "                        'text': texts_test[idx],\n",
    "                        'true_label': int(true_label),\n",
    "                        'pred_label': int(pred_label)\n",
    "                    })\n",
    "        results['misclassified'] = misclassified\n",
    "\n",
    "        return probe, results\n",
    "    \n",
    "    def train_torch_probe(self, X_train, y_train, X_test, y_test, epochs=100):\n",
    "        \"\"\"Train PyTorch linear probe\"\"\"\n",
    "        input_dim = X_train.shape[0] # is this \"how many text snppets\"?\n",
    "#         num_classes = len(np.unique(y_train)) \n",
    "        num_classes = NUM_CLASSES # let's do binary classifier\n",
    "    # TODO: let's imitate sigmoid\n",
    "        train_dataset = ActivationDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        probe = LinearProbe(input_dim, num_classes).to(self.device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(probe.parameters(), lr=0.001)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            probe.train()\n",
    "            total_loss = 0\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = probe(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        probe.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(self.device)\n",
    "            test_outputs = probe(X_test_tensor)\n",
    "            test_pred = torch.argmax(test_outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "            X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(self.device)\n",
    "            train_outputs = probe(X_train_tensor)\n",
    "            train_pred = torch.argmax(train_outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        results = {\n",
    "            'train_acc': accuracy_score(y_train, train_pred),\n",
    "            'test_acc': accuracy_score(y_test, test_pred),\n",
    "            'classification_report': classification_report(y_test, test_pred)\n",
    "        }\n",
    "        \n",
    "        return probe, results\n",
    "    \n",
    "    def run_experiment(self, gemma_activations, gemma_labels, \n",
    "                      polygemma_activations, polygemma_labels):\n",
    "        \"\"\"Run complete probing experiment\"\"\"\n",
    "        print(f\"Running linear probing experiment: {self.concept}\")\n",
    "        print(f\"Gemma training data: {gemma_activations.shape}\")\n",
    "        print(f\"PolyGemma test data: {polygemma_activations.shape}\")\n",
    "        \n",
    "        # Train on Gemma, test on PolyGemma\n",
    "        results = {}\n",
    "    \n",
    "        \n",
    "        # sklearn probe\n",
    "        print(\"\\n--- Training sklearn probe ---\")\n",
    "        # Do the split!\n",
    "        gemma_activations_train, _, gemma_labels_train, _  = train_test_split(gemma_activations, gemma_labels, test_size=0.2, random_state=SEED)\n",
    "        _, polygemma_activations_test, _, polygemma_labels_test = train_test_split(polygemma_activations, polygemma_labels, test_size=0.2, random_state=SEED)\n",
    "        print(f\"size of gemma_activations_train: {gemma_activations_train.shape}, size of gemma_labels_train: {len(gemma_labels_train)}\")\n",
    "        print(f\"size of polygemma_activations_test: {polygemma_activations_test.shape}, size of polygemma_labels_test: {len(polygemma_labels_test)}\")\n",
    "        sklearn_probe, sklearn_results = self.train_sklearn_probe(\n",
    "            gemma_activations_train.cpu(), gemma_labels_train, # train\n",
    "            polygemma_activations_test.cpu(), polygemma_labels_test # test\n",
    "        )\n",
    "        results['sklearn'] = sklearn_results\n",
    "        \n",
    "#         # PyTorch probe\n",
    "#         print(\"\\n--- Training PyTorch probe ---\")\n",
    "#         torch_probe, torch_results = self.train_torch_probe(\n",
    "#             gemma_activations, gemma_labels, # train\n",
    "#             polygemma_activations, polygemma_labels # test\n",
    "#         )\n",
    "#         results['torch'] = torch_results\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_results(self, results, output_path):\n",
    "        \"\"\"Save experiment results\"\"\"\n",
    "        output_path = Path(output_path)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Convert numpy types to Python types for JSON serialization\n",
    "        def convert_numpy(obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return obj\n",
    "        \n",
    "        json_results = json.loads(json.dumps(results, default=convert_numpy))\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(json_results, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f058085",
   "metadata": {},
   "source": [
    "# synth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6814da9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T01:16:08.287669Z",
     "start_time": "2025-08-08T01:16:08.280506Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Synthetic text data for cat/dog classification\n",
    "# text = [\n",
    "#     # Cat examples (label 0)\n",
    "#     \"The fluffy cat purred softly on the windowsill, watching birds outside.\",\n",
    "#     \"My kitten loves to chase the red laser pointer around the living room.\",\n",
    "#     \"The orange tabby cat stretched lazily in the warm afternoon sunlight.\",\n",
    "#     \"She adopted a rescue cat from the local animal shelter last week.\",\n",
    "#     \"The cat's whiskers twitched as it stalked the toy mouse across the floor.\",\n",
    "#     \"Fluffy meowed loudly when her food bowl was empty this morning.\",\n",
    "#     \"The black cat gracefully jumped onto the kitchen counter with ease.\",\n",
    "#     \"My feline friend enjoys napping in cardboard boxes all day long.\",\n",
    "#     \"The cat's green eyes glowed mysteriously in the dim moonlight tonight.\",\n",
    "#     \"Her pet cat brings dead mice to the doorstep every morning.\",\n",
    "#     \"The Siamese cat has the most beautiful blue eyes I've ever seen.\",\n",
    "#     \"Tom cat climbed up the tall oak tree to escape the neighborhood dogs.\",\n",
    "#     \"The veterinarian said the kitten needs its vaccinations next month.\",\n",
    "#     \"My cat purrs so loudly it sounds like a tiny motor running.\",\n",
    "#     \"The calico cat had three adorable kittens in the barn yesterday.\",\n",
    "#     \"She trained her cat to use the toilet instead of a litter box.\",\n",
    "#     \"The Persian cat's long fur requires daily brushing to prevent matting.\",\n",
    "#     \"My indoor cat watches wildlife documentaries on TV with great interest.\",\n",
    "#     \"The stray cat finally trusted me enough to eat from my hand.\",\n",
    "#     \"Her cat knocked over the expensive vase while chasing a butterfly.\",\n",
    "    \n",
    "#     # Dog examples (label 1)\n",
    "#     \"The golden retriever barked excitedly when his owner came home today.\",\n",
    "#     \"My dog loves to fetch tennis balls in the backyard every afternoon.\",\n",
    "#     \"The small puppy wagged its tail when meeting new people yesterday.\",\n",
    "#     \"She takes her German shepherd for long walks in the park.\",\n",
    "#     \"The dog's tail wagged furiously when it saw the treat jar.\",\n",
    "#     \"Max barked at the mailman who comes by every morning.\",\n",
    "#     \"The border collie herded the sheep expertly across the green field.\",\n",
    "#     \"My canine companion loves swimming in the lake during hot summers.\",\n",
    "#     \"The dog trainer taught the puppy basic commands like sit and stay.\",\n",
    "#     \"Her loyal dog waited patiently outside the grocery store for her.\",\n",
    "#     \"The beagle's nose led it straight to the hidden treats upstairs.\",\n",
    "#     \"My dog howls along with the sirens from passing fire trucks.\",\n",
    "#     \"The veterinarian recommended a special diet for the overweight bulldog.\",\n",
    "#     \"The rescue dog was nervous but gradually warmed up to us.\",\n",
    "#     \"My puppy chewed up my favorite pair of running shoes yesterday.\",\n",
    "#     \"The dog park was crowded with excited pups playing together today.\",\n",
    "#     \"Her service dog helps her navigate safely through busy city streets.\",\n",
    "#     \"The hunting dog pointed steadily at the birds hiding in bushes.\",\n",
    "#     \"My dog greets every visitor with enthusiastic tail wagging and jumping.\",\n",
    "#     \"The old dog slept peacefully by the fireplace on cold nights.\",\n",
    "    \n",
    "#     # Neutral/other examples (label 2) - neither cats nor dogs\n",
    "#     \"The morning sun cast beautiful shadows across the empty parking lot.\",\n",
    "#     \"She enjoyed reading mystery novels while drinking her evening tea.\",\n",
    "#     \"The mathematics professor explained complex equations on the whiteboard clearly.\",\n",
    "#     \"Fresh vegetables from the farmers market made an excellent dinner tonight.\",\n",
    "#     \"The old library contained thousands of books on various subjects.\",\n",
    "#     \"He repaired the broken bicycle tire using tools from the garage.\",\n",
    "#     \"The weather forecast predicted rain for the entire weekend ahead.\",\n",
    "#     \"Students gathered in the cafeteria to discuss their upcoming project.\",\n",
    "#     \"The concert featured amazing performances by local musicians and bands.\",\n",
    "#     \"She planted colorful flowers in her garden beds this spring.\",\n",
    "#     \"The computer program crashed unexpectedly during the important presentation today.\",\n",
    "#     \"Ocean waves crashed against the rocky cliffs during the storm.\",\n",
    "#     \"The chef prepared an elaborate feast for the wedding celebration.\",\n",
    "#     \"Mountains covered in snow looked majestic against the clear sky.\",\n",
    "#     \"The museum displayed artifacts from ancient civilizations throughout history.\",\n",
    "#     \"Traffic was heavy on the highway during rush hour yesterday.\",\n",
    "#     \"The smartphone battery died right before the important phone call.\",\n",
    "#     \"Autumn leaves fell gently from the trees in vibrant colors.\",\n",
    "#     \"The construction workers finished building the new bridge ahead of schedule.\",\n",
    "#     \"She studied diligently for her final exams in the quiet library.\"\n",
    "# ]\n",
    "\n",
    "# # Corresponding labels: 0=cat, 1=dog, 2=neutral\n",
    "# labels = [\n",
    "#     # Cat labels (0)\n",
    "#     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#     # Dog labels (1) \n",
    "#     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "#     # Neutral labels (2)\n",
    "#     2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2\n",
    "# ]\n",
    "\n",
    "# # Verify data consistency\n",
    "# print(f\"Total texts: {len(text)}\")\n",
    "# print(f\"Total labels: {len(labels)}\")\n",
    "# print(f\"Cat examples: {labels.count(0)}\")\n",
    "# print(f\"Dog examples: {labels.count(1)}\")\n",
    "# print(f\"Neutral examples: {labels.count(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f494c",
   "metadata": {},
   "source": [
    "# COCO annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6603499d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T01:16:08.347327Z",
     "start_time": "2025-08-08T01:16:08.290526Z"
    }
   },
   "outputs": [],
   "source": [
    "SAVE_DIR = '../data'\n",
    "cat_df = pd.read_csv(f'{SAVE_DIR}/coco_cat_binary_with_captions_balanced.csv').sample(1000, random_state = SEED)\n",
    "dog_df = pd.read_csv(f'{SAVE_DIR}/coco_dog_binary_with_captions_balanced.csv').sample(1000, random_state = SEED)\n",
    "\n",
    "cat_df.captions = cat_df.captions.apply(lambda x:eval(x)[0])\n",
    "dog_df.captions = dog_df.captions.apply(lambda x:eval(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43df1bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T01:16:08.361038Z",
     "start_time": "2025-08-08T01:16:08.350515Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2378    41\n",
       "2420    45\n",
       "1940    48\n",
       "1592    60\n",
       "2283    56\n",
       "        ..\n",
       "259     57\n",
       "1001    67\n",
       "3265    53\n",
       "1912    39\n",
       "3074    46\n",
       "Name: captions, Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_df.captions.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eddd6171",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T01:16:08.369123Z",
     "start_time": "2025-08-08T01:16:08.363558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of cat_texts: 1000, length of cat_labels: 1000\n"
     ]
    }
   ],
   "source": [
    "cat_texts, cat_labels = cat_df.captions.tolist(), cat_df.label.tolist()\n",
    "print(f\"length of cat_texts: {len(cat_texts)}, length of cat_labels: {len(cat_labels)}\")\n",
    "dog_texts, dog_labels = dog_df.captions.tolist(), dog_df.label.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b448f",
   "metadata": {},
   "source": [
    "# load model & get act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a933a40b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T01:16:08.385927Z",
     "start_time": "2025-08-08T01:16:08.370959Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_models_with_eval(model_name, device=\"cuda\"):\n",
    "    if \"paligemma\" in model_name.lower():\n",
    "        from transformers import PaliGemmaForConditionalGeneration\n",
    "        model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "            model_name, \n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float32,  # Use fp16 for memory efficiency\n",
    "            device_map=None  # We'll handle device placement manually\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        language_model = model.language_model\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float32,\n",
    "            device_map=None\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        language_model = model\n",
    "            \n",
    "    language_model.eval()\n",
    "    return language_model\n",
    "\n",
    "def get_inputs_from_text(model_name, text, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # FIXED: More robust tokenization with proper padding token handling\n",
    "    # Ensure we have a pad token\n",
    "\n",
    "    # Tokenize with safer parameters\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        add_special_tokens=True  # Ensure special tokens are added properly\n",
    "    )\n",
    "\n",
    "    # FIXED: Validate token IDs are within vocabulary range\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    input_ids = inputs['input_ids']\n",
    "\n",
    "    # Check for out-of-bounds token IDs\n",
    "    if torch.any(input_ids >= vocab_size) or torch.any(input_ids < 0):\n",
    "        print(f\"⚠️  Invalid token IDs detected. Max ID: {input_ids.max()}, Vocab size: {vocab_size}\")\n",
    "        # Clamp invalid IDs to valid range\n",
    "        input_ids = torch.clamp(input_ids, 0, vocab_size - 1)\n",
    "        inputs['input_ids'] = input_ids\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    return inputs\n",
    "    \n",
    "def get_acts(language_model, text, layer, model_name, device):\n",
    "    inputs = get_inputs_from_text(model_name, text, device)\n",
    "    if hasattr(language_model, 'model') and hasattr(language_model.model, 'layers'):\n",
    "        if layer < len(language_model.model.layers):\n",
    "            target_layer = language_model.model.layers[layer]\n",
    "        else:\n",
    "            print(f\"❌ Layer {layer} out of range. Model has {len(language_model.model.layers)} layers\")\n",
    "            return torch.randn(1, 64, 2304).to(DEVICE), 0.0\n",
    "    activations = None\n",
    "\n",
    "\n",
    "    def activation_hook(module, inputs, output):\n",
    "        nonlocal activations\n",
    "        try:\n",
    "            if isinstance(output, tuple):\n",
    "                activations = output[0].clone().detach()\n",
    "            else:\n",
    "                activations = output.clone().detach()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error in activation hook: {e}\")\n",
    "\n",
    "    # FIXED: More robust layer identification\n",
    "    target_layer = None\n",
    "    try:\n",
    "        if hasattr(language_model, 'model') and hasattr(language_model.model, 'layers'):\n",
    "            if layer < len(language_model.model.layers):\n",
    "                target_layer = language_model.model.layers[layer]\n",
    "            else:\n",
    "                print(f\"❌ Layer {layer} out of range. Model has {len(language_model.model.layers)} layers\")\n",
    "                return torch.randn(1, 64, 2304).to(DEVICE), 0.0\n",
    "        elif hasattr(language_model, 'layers'):\n",
    "            if layer < len(language_model.layers):\n",
    "                target_layer = language_model.layers[layer]\n",
    "            else:\n",
    "                print(f\"❌ Layer {layer} out of range. Model has {len(language_model.layers)} layers\")\n",
    "                return torch.randn(1, 64, 2304).to(DEVICE), 0.0\n",
    "        else:\n",
    "            print(f\"❌ Could not find layers in model structure\")\n",
    "            return torch.randn(1, 64, 2304).to(DEVICE), 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error accessing layer {layer}: {e}\")\n",
    "        return torch.randn(1, 64, 2304).to(DEVICE), 0.0\n",
    "\n",
    "    if target_layer is None:\n",
    "        print(f\"❌ Could not find layer {layer}\")\n",
    "        return torch.randn(1, 64, 2304).to(DEVICE), 0.0\n",
    "\n",
    "    hook = target_layer.register_forward_hook(activation_hook)\n",
    "\n",
    "    # Forward pass to get activations\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            if \"paligemma\" in model_name.lower():\n",
    "                _ = language_model(**inputs)\n",
    "            else:\n",
    "                _ = language_model(**inputs)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error in activation extraction: {e}\")\n",
    "\n",
    "    hook.remove()\n",
    "    if activations is None:\n",
    "        print(f\"⚠️  Failed to extract activations from layer {layer}\")\n",
    "        # FIXED: Return appropriate tensor size based on model\n",
    "        try:\n",
    "            # Try to get the actual hidden size from the model config\n",
    "            if hasattr(language_model, 'config') and hasattr(language_model.config, 'hidden_size'):\n",
    "                hidden_size = language_model.config.hidden_size\n",
    "            else:\n",
    "                hidden_size = 2304  # fallback\n",
    "            activations = torch.randn(1, 64, hidden_size).to(DEVICE)\n",
    "        except:\n",
    "            activations = torch.randn(1, 64, 2304).to(DEVICE)\n",
    "\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c9541b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T01:36:52.901404Z",
     "start_time": "2025-08-08T01:16:08.387634Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 64.93it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 0 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.9050\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91       105\n",
      "           1       0.91      0.88      0.90        95\n",
      "\n",
      "    accuracy                           0.91       200\n",
      "   macro avg       0.91      0.90      0.90       200\n",
      "weighted avg       0.91      0.91      0.90       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 1 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.9150\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92       105\n",
      "           1       0.93      0.88      0.91        95\n",
      "\n",
      "    accuracy                           0.92       200\n",
      "   macro avg       0.92      0.91      0.91       200\n",
      "weighted avg       0.92      0.92      0.91       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 2 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.8900\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.99      0.90       105\n",
      "           1       0.99      0.78      0.87        95\n",
      "\n",
      "    accuracy                           0.89       200\n",
      "   macro avg       0.91      0.88      0.89       200\n",
      "weighted avg       0.91      0.89      0.89       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 3 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.8350\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86       105\n",
      "           1       1.00      0.65      0.79        95\n",
      "\n",
      "    accuracy                           0.83       200\n",
      "   macro avg       0.88      0.83      0.83       200\n",
      "weighted avg       0.87      0.83      0.83       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 4 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.8550\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88       105\n",
      "           1       0.96      0.73      0.83        95\n",
      "\n",
      "    accuracy                           0.85       200\n",
      "   macro avg       0.88      0.85      0.85       200\n",
      "weighted avg       0.87      0.85      0.85       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 5 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.6550\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75       105\n",
      "           1       1.00      0.27      0.43        95\n",
      "\n",
      "    accuracy                           0.66       200\n",
      "   macro avg       0.80      0.64      0.59       200\n",
      "weighted avg       0.79      0.66      0.60       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 6 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.8300\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.99      0.86       105\n",
      "           1       0.98      0.65      0.78        95\n",
      "\n",
      "    accuracy                           0.83       200\n",
      "   macro avg       0.87      0.82      0.82       200\n",
      "weighted avg       0.87      0.83      0.82       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 7 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.5650\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       105\n",
      "           1       1.00      0.08      0.16        95\n",
      "\n",
      "    accuracy                           0.56       200\n",
      "   macro avg       0.77      0.54      0.43       200\n",
      "weighted avg       0.76      0.56      0.44       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 8 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.5350\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69       105\n",
      "           1       1.00      0.02      0.04        95\n",
      "\n",
      "    accuracy                           0.54       200\n",
      "   macro avg       0.77      0.51      0.37       200\n",
      "weighted avg       0.75      0.54      0.38       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 9 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.6450\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75       105\n",
      "           1       1.00      0.25      0.40        95\n",
      "\n",
      "    accuracy                           0.65       200\n",
      "   macro avg       0.80      0.63      0.58       200\n",
      "weighted avg       0.79      0.65      0.58       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 10 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.6650\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.99      0.76       105\n",
      "           1       0.97      0.31      0.46        95\n",
      "\n",
      "    accuracy                           0.67       200\n",
      "   macro avg       0.79      0.65      0.61       200\n",
      "weighted avg       0.78      0.67      0.62       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 14.92 GiB is free. Process 2268115 has 29.46 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 11 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.5700\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.99      0.71       105\n",
      "           1       0.91      0.11      0.19        95\n",
      "\n",
      "    accuracy                           0.57       200\n",
      "   macro avg       0.73      0.55      0.45       200\n",
      "weighted avg       0.72      0.57      0.46       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 14.92 GiB is free. Process 2268115 has 29.46 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 12 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.6150\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.95      0.72       105\n",
      "           1       0.82      0.24      0.37        95\n",
      "\n",
      "    accuracy                           0.61       200\n",
      "   macro avg       0.70      0.60      0.55       200\n",
      "weighted avg       0.70      0.61      0.56       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/conda/envs/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/conda/envs/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LAYER 13 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.5250\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69       105\n",
      "           1       0.00      0.00      0.00        95\n",
      "\n",
      "    accuracy                           0.53       200\n",
      "   macro avg       0.26      0.50      0.34       200\n",
      "weighted avg       0.28      0.53      0.36       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/conda/envs/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/conda/envs/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LAYER 14 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.5250\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69       105\n",
      "           1       0.00      0.00      0.00        95\n",
      "\n",
      "    accuracy                           0.53       200\n",
      "   macro avg       0.26      0.50      0.34       200\n",
      "weighted avg       0.28      0.53      0.36       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 17.61 GiB is free. Process 2268115 has 26.78 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 2.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/conda/envs/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/conda/envs/py311/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LAYER 15 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.5250\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69       105\n",
      "           1       0.00      0.00      0.00        95\n",
      "\n",
      "    accuracy                           0.53       200\n",
      "   macro avg       0.26      0.50      0.34       200\n",
      "weighted avg       0.28      0.53      0.36       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 16 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.5350\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69       105\n",
      "           1       1.00      0.02      0.04        95\n",
      "\n",
      "    accuracy                           0.54       200\n",
      "   macro avg       0.77      0.51      0.37       200\n",
      "weighted avg       0.75      0.54      0.38       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 17.12 GiB is free. Process 2268115 has 27.26 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 2.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 17 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.5350\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69       105\n",
      "           1       1.00      0.02      0.04        95\n",
      "\n",
      "    accuracy                           0.54       200\n",
      "   macro avg       0.77      0.51      0.37       200\n",
      "weighted avg       0.75      0.54      0.38       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 18 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.6600\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      1.00      0.76       105\n",
      "           1       1.00      0.28      0.44        95\n",
      "\n",
      "    accuracy                           0.66       200\n",
      "   macro avg       0.80      0.64      0.60       200\n",
      "weighted avg       0.79      0.66      0.61       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 19 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.7800\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.83       105\n",
      "           1       1.00      0.54      0.70        95\n",
      "\n",
      "    accuracy                           0.78       200\n",
      "   macro avg       0.85      0.77      0.76       200\n",
      "weighted avg       0.84      0.78      0.77       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 14.92 GiB is free. Process 2268115 has 29.46 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 20 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.7500\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      1.00      0.81       105\n",
      "           1       1.00      0.47      0.64        95\n",
      "\n",
      "    accuracy                           0.75       200\n",
      "   macro avg       0.84      0.74      0.73       200\n",
      "weighted avg       0.83      0.75      0.73       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 21 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.8900\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90       105\n",
      "           1       0.96      0.80      0.87        95\n",
      "\n",
      "    accuracy                           0.89       200\n",
      "   macro avg       0.90      0.89      0.89       200\n",
      "weighted avg       0.90      0.89      0.89       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 22 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.8600\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       105\n",
      "           1       0.99      0.72      0.83        95\n",
      "\n",
      "    accuracy                           0.86       200\n",
      "   macro avg       0.89      0.85      0.86       200\n",
      "weighted avg       0.88      0.86      0.86       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 17.61 GiB is free. Process 2268115 has 26.78 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 2.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 23 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.7800\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.83       105\n",
      "           1       1.00      0.54      0.70        95\n",
      "\n",
      "    accuracy                           0.78       200\n",
      "   macro avg       0.85      0.77      0.76       200\n",
      "weighted avg       0.84      0.78      0.77       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 17.61 GiB is free. Process 2268115 has 26.78 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 2.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 24 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.6150\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.73       105\n",
      "           1       1.00      0.19      0.32        95\n",
      "\n",
      "    accuracy                           0.61       200\n",
      "   macro avg       0.79      0.59      0.53       200\n",
      "weighted avg       0.78      0.61      0.54       200\n",
      "\n",
      "⚠️  Error in activation extraction: CUDA out of memory. Tried to allocate 61.04 GiB. GPU 1 has a total capacity of 44.38 GiB of which 15.41 GiB is free. Process 2268115 has 28.97 GiB memory in use. Of the allocated memory 23.62 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Running linear probing experiment: cat_dog_classification\n",
      "Gemma training data: torch.Size([1000, 64, 2304])\n",
      "PolyGemma test data: torch.Size([1000, 64, 2304])\n",
      "\n",
      "--- Training sklearn probe ---\n",
      "size of gemma_activations_train: torch.Size([800, 64, 2304]), size of gemma_labels_train: 800\n",
      "size of polygemma_activations_test: torch.Size([200, 64, 2304]), size of polygemma_labels_test: 200\n",
      "X_train_flat shape: torch.Size([800, 147456])\n",
      "y_train shape: 800\n",
      "X_test_flat shape: torch.Size([200, 147456])\n",
      "y_test shape: 200\n",
      "\n",
      "=== LAYER 25 RESULTS ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.7150\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.60      0.69       105\n",
      "           1       0.66      0.84      0.74        95\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.73      0.72      0.71       200\n",
      "weighted avg       0.74      0.71      0.71       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize experiment\n",
    "experiment = LinearProbingExperiment(concept=\"cat_dog_classification\")\n",
    "\n",
    "# Example: Create dummy data for testing\n",
    "# In practice, replace with actual Gemma/PolyGemma activations\n",
    "np.random.seed(42)\n",
    "\n",
    "# load models\n",
    "model1_name = \"google/gemma-2-2b\"  # Base Gemma-2-2B (LLM)\n",
    "model2_name = \"google/paligemma2-3b-pt-224\"\n",
    "\n",
    "gemma = load_models_with_eval(model1_name, DEVICE_1)\n",
    "paligemma = load_models_with_eval(model2_name, DEVICE_2)\n",
    "\n",
    "# # Dummy activations (replace with real data)\n",
    "# gemma_activations = np.random.randn(1000, 768)  # 1000 samples, 768 dims\n",
    "# gemma_labels = np.random.randint(0, 2, 1000)    # binary cat/dog labels\n",
    "\n",
    "# polygemma_activations = np.random.randn(200, 768)  # 200 test samples\n",
    "# polygemma_labels = np.random.randint(0, 2, 200)\n",
    "\n",
    "layer_to_test = list(range(26))\n",
    "all_results = []\n",
    "# Run experiment per layer\n",
    "for layer in layer_to_test:\n",
    "    # Produce activation-label pairs\n",
    "    with torch.inference_mode():\n",
    "        gemma_activations = get_acts(gemma, cat_texts, layer, model1_name, DEVICE_1)\n",
    "        polygemma_activations = get_acts(paligemma, cat_texts, layer, model2_name, DEVICE_2)\n",
    "\n",
    "    # === Run probe experiment ===\n",
    "    results = experiment.run_experiment(\n",
    "        gemma_activations, cat_labels,\n",
    "        polygemma_activations, cat_labels\n",
    "    )\n",
    "    all_results.append(results)\n",
    "\n",
    "    print(f\"\\n=== LAYER {layer} RESULTS ===\")\n",
    "    print(f\"Train Accuracy: {results['sklearn']['train_acc']:.4f}\")\n",
    "    print(f\"Test Accuracy: {results['sklearn']['test_acc']:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(results['sklearn']['classification_report'])\n",
    "\n",
    "    # Save results\n",
    "    experiment.save_results(results, f\"../output/linear_probing_results_{layer}.json\")\n",
    "\n",
    "    # Save misclassified examples\n",
    "    if 'y_test' in results['sklearn'] and 'y_pred' in results['sklearn']:\n",
    "        mis_indices = [i for i, (yt, yp) in enumerate(zip(\n",
    "            results['sklearn']['y_test'], results['sklearn']['y_pred']\n",
    "        )) if yt != yp]\n",
    "        mis_info = [\n",
    "            {\n",
    "                'text': cat_texts[i],  # assuming you meant cat_texts not text\n",
    "                'true_label': int(results['sklearn']['y_test'][i]),\n",
    "                'pred_label': int(results['sklearn']['y_pred'][i])\n",
    "            }\n",
    "            for i in mis_indices\n",
    "        ]\n",
    "        with open(f\"../output/misclassified_layer_{layer}.json\", 'w') as f:\n",
    "            json.dump(mis_info, f, indent=2)\n",
    "    else:\n",
    "        print(\"Warning: 'y_test' or 'y_pred' not found in results; misclassifications not saved.\")\n",
    "          \n",
    "    del gemma_activations, polygemma_activations, results\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "          \n",
    "# ===== Plot Train and Test Accuracy Across Layers =====\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "layers = layer_to_test\n",
    "train_accs = [res['sklearn']['train_acc'] for res in all_results]\n",
    "test_accs = [res['sklearn']['test_acc'] for res in all_results]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(layers, train_accs)\n",
    "plt.plot(layers, test_accs)\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracy by Layer')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.savefig(\"../figs_tabs/Linear Probe Train and Test Accuracy by Layer.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
