{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b1749e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "SAE-based representation shift analysis using SAE Lens library\n",
    "for comparing Gemma and PaliGemma 2 with Google's Gemma Scope SAEs.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "try:\n",
    "    from sae_lens import SAE\n",
    "except:\n",
    "#     !pip install --upgrade pip setuptools wheel\n",
    "    !pip install --pre sae-lens\n",
    "from sae_lens import SAE\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "from dataclasses import dataclass\n",
    "import seaborn as sns\n",
    "torch.set_grad_enabled(False)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ef0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HUGGING_FACE_API_KEY = os.getenv('HUGGING_FACE_API_KEY')\n",
    "# Paste your token between the quotes:\n",
    "login(token=HUGGING_FACE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7c5ede0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SAE Lens - Gemma Scope Representation Shift Analysis\n",
      "============================================================\n",
      "üîß Initializing GemmaScope SAE (Layer 12, Width 16k, Size 2b, Suffix canonical)\n",
      "üì• Loading Gemma Scope SAE...\n",
      "   Loading from release: gemma-scope-2b-pt-res-canonical\n",
      "   SAE ID: layer_12/width_16k/canonical\n",
      "‚úÖ SAE loaded successfully!\n",
      "   - Dictionary size: 16384\n",
      "   - Model dimension: 2304\n",
      "\n",
      "üî¨ Analysis Configuration:\n",
      "   Layer: 12\n",
      "   SAE Width: 16k\n",
      "   Model Size: 2b\n",
      "   SAE Suffix: canonical\n",
      "   Test Texts: 5\n",
      "\n",
      "üöÄ Starting comparative analysis\n",
      "   Model 1: google/gemma-2-2b\n",
      "   Model 2: google/paligemma2-3b-pt-224\n",
      "   Texts: 5 samples\n",
      "\n",
      "üìù Processing text 1/5: 'The quick brown fox jumps over the lazy dog....'\n",
      "üîç Extracting activations from google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Extracted activations: torch.Size([1, 11, 2304])\n",
      "üîç Extracting activations from google/paligemma2-3b-pt-224\n",
      "‚ùå Error extracting activations: Unrecognized configuration class <class 'transformers.models.paligemma.configuration_paligemma.PaliGemmaConfig'> for this kind of AutoModel: AutoModelForCausalLM.\n",
      "Model type should be one of ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV3Config, DiffLlamaConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, FalconConfig, FalconH1Config, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, JambaConfig, JetMoeConfig, LlamaConfig, Llama4Config, Llama4TextConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MistralConfig, MixtralConfig, MllamaConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, ZambaConfig, Zamba2Config.\n",
      "üîÑ Using dummy activations for demonstration\n",
      "‚ùå Error during analysis: The size of tensor a (11) must match the size of tensor b (10) at non-singleton dimension 0\n",
      "\n",
      "üí° Troubleshooting tips:\n",
      "   1. Install SAE Lens: pip install sae-lens\n",
      "   2. Ensure you have sufficient GPU memory\n",
      "   3. Try with smaller models or fewer texts\n",
      "   4. Check model names are correct and accessible\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npip install sae-lens transformers torch matplotlib seaborn numpy\\n\\n# For CUDA support (recommended):\\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main demonstration of SAE-based representation shift analysis.\"\"\"\n",
    "    print(\"üöÄ SAE Lens - Gemma Scope Representation Shift Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Configuration\n",
    "    LAYER = 12  # Middle layer for analysis\n",
    "    WIDTH = \"16k\"  # SAE width\n",
    "    MODEL_SIZE = \"2b\"  # Using 2B models for faster demo\n",
    "    SUFFIX = \"canonical\"  # Use canonical SAEs (most stable)\n",
    "    \n",
    "    # Test texts covering different domains\n",
    "    test_texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"In machine learning, neural networks learn complex patterns from data.\",\n",
    "        \"The economy has shown resilience despite global challenges.\",\n",
    "        \"Climate change affects weather patterns around the world.\",\n",
    "        \"Artificial intelligence transforms how we work and live.\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Initialize analyzer\n",
    "        analyzer = GemmaScopeAnalyzer(\n",
    "            layer=LAYER, \n",
    "            width=WIDTH, \n",
    "            model_size=MODEL_SIZE,\n",
    "            suffix=SUFFIX\n",
    "        )\n",
    "        \n",
    "        # Model names (adjust these based on available models)\n",
    "        model1_name = \"google/gemma-2-2b\"  # Base Gemma 2\n",
    "#         model2_name = \"google/gemma-2-2b-it\"  # Instruction-tuned version\n",
    "        model2_name = 'google/paligemma2-3b-pt-224'  # What does suffix 224 mean? There's also one ending with \"448\"\n",
    "        # Note: Replace with actual PaliGemma when available\n",
    "        \n",
    "        print(f\"\\nüî¨ Analysis Configuration:\")\n",
    "        print(f\"   Layer: {LAYER}\")\n",
    "        print(f\"   SAE Width: {WIDTH}\")\n",
    "        print(f\"   Model Size: {MODEL_SIZE}\")\n",
    "        print(f\"   SAE Suffix: {SUFFIX}\")\n",
    "        print(f\"   Test Texts: {len(test_texts)}\")\n",
    "        print()\n",
    "        \n",
    "        # Run analysis\n",
    "        results = analyzer.analyze_models(model1_name, model2_name, test_texts)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nüìä ANALYSIS RESULTS:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        agg = results['aggregate']\n",
    "        \n",
    "        print(\"\\nAverage SAE Metrics - Model 1:\")\n",
    "        for key, value in agg['avg_model1_metrics'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nAverage SAE Metrics - Model 2:\")\n",
    "        for key, value in agg['avg_model2_metrics'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nAverage Representation Shift Metrics:\")\n",
    "        for key, value in agg['avg_shift_metrics'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        # Generate interpretations\n",
    "        interpretations = analyzer.interpret_results(results)\n",
    "        \n",
    "        print(\"\\nüîç INTERPRETATIONS:\")\n",
    "        print(\"=\" * 40)\n",
    "        for aspect, interpretation in interpretations.items():\n",
    "            print(f\"{aspect.replace('_', ' ').title()}: {interpretation}\")\n",
    "        \n",
    "        # Create visualization\n",
    "        analyzer.visualize_results(results)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Analysis complete!\")\n",
    "        print(f\"üìà Visualization saved as '../figs_tabs/sae_analysis_{model1_name}_{model2_name}.png'\")\n",
    "        print(f\"üìã Analyzed {len(test_texts)} texts across layer {LAYER}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during analysis: {e}\")\n",
    "        print(\"\\nüí° Troubleshooting tips:\")\n",
    "        print(\"   1. Install SAE Lens: pip install sae-lens\")\n",
    "        print(\"   2. Ensure you have sufficient GPU memory\")\n",
    "        print(\"   3. Try with smaller models or fewer texts\")\n",
    "        print(\"   4. Check model names are correct and accessible\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Installation requirements:\n",
    "\"\"\"\n",
    "pip install sae-lens transformers torch matplotlib seaborn numpy\n",
    "\n",
    "# For CUDA support (recommended):\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
