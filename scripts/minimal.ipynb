{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f66999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "SAE-based representation shift analysis using SAE Lens library\n",
    "for comparing Gemma and PaliGemma 2 with Google's Gemma Scope SAEs.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "try:\n",
    "    from sae_lens import SAE\n",
    "except:\n",
    "#     !pip install --upgrade pip setuptools wheel\n",
    "    !pip install --pre sae-lens\n",
    "from sae_lens import SAE\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "from dataclasses import dataclass\n",
    "import seaborn as sns\n",
    "torch.set_grad_enabled(False)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0854b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HUGGING_FACE_API_KEY = os.getenv('HUGGING_FACE_API_KEY')\n",
    "# Paste your token between the quotes:\n",
    "login(token=HUGGING_FACE_API_KEY)\n",
    "device='cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b7c35b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 SAE Lens - Gemma Scope Representation Shift Analysis\n",
      "============================================================\n",
      "🔧 Initializing GemmaScope SAE (Layer 12, Width 16k, Size 2b, Suffix canonical)\n",
      "📥 Loading Gemma Scope SAE...\n",
      "   Loading from release: gemma-scope-2b-pt-res-canonical\n",
      "   SAE ID: layer_12/width_16k/canonical\n",
      "✅ SAE loaded successfully!\n",
      "   - Dictionary size: 16384\n",
      "   - Model dimension: 2304\n",
      "\n",
      "🔬 Research Question: Representational shift during LLM->VLM adaptation\n",
      "   Comparing the SAME Gemma-2-2B architecture before and after multimodal training\n",
      "\n",
      "🔬 Analysis Configuration:\n",
      "   Layer: 12 (Gemma decoder layer)\n",
      "   SAE Width: 16k\n",
      "   Model Size: 2b\n",
      "   SAE Suffix: canonical\n",
      "   Test Texts: 5\n",
      "   Research Focus: LLM->VLM representational adaptation\n",
      "\n",
      "🚀 Starting comparative analysis\n",
      "   Model 1: google/gemma-2-2b\n",
      "   Model 2: google/paligemma2-3b-pt-224\n",
      "   Texts: 5 samples\n",
      "\n",
      "📝 Processing text 1/5: 'The quick brown fox jumps over the lazy dog....'\n",
      "🔍 Extracting activations from google/gemma-2-2b\n",
      "   📝 Loading standard Gemma model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📝 Tokenized input shape: torch.Size([1, 64])\n",
      "   🔍 Processing text through standard Gemma\n",
      "   ✅ Extracted activations: torch.Size([1, 64, 2304])\n",
      "🔍 Extracting activations from google/paligemma2-3b-pt-224\n",
      "   📷 Loading PaliGemma (extracting Gemma decoder)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Extracted Gemma decoder: <class 'transformers.models.gemma2.modeling_gemma2.Gemma2Model'>\n",
      "   📝 Tokenized input shape: torch.Size([1, 64])\n",
      "   🔍 Processing text through PaliGemma's Gemma decoder\n",
      "   ✅ Extracted activations: torch.Size([1, 64, 2304])\n",
      "   ✅ Completed analysis for text 1\n",
      "📝 Processing text 2/5: 'In machine learning, neural networks learn complex...'\n",
      "🔍 Extracting activations from google/gemma-2-2b\n",
      "   📝 Loading standard Gemma model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📝 Tokenized input shape: torch.Size([1, 64])\n",
      "   🔍 Processing text through standard Gemma\n",
      "   ✅ Extracted activations: torch.Size([1, 64, 2304])\n",
      "🔍 Extracting activations from google/paligemma2-3b-pt-224\n",
      "   📷 Loading PaliGemma (extracting Gemma decoder)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Extracted Gemma decoder: <class 'transformers.models.gemma2.modeling_gemma2.Gemma2Model'>\n",
      "   📝 Tokenized input shape: torch.Size([1, 64])\n",
      "   🔍 Processing text through PaliGemma's Gemma decoder\n",
      "   ✅ Extracted activations: torch.Size([1, 64, 2304])\n",
      "   ✅ Completed analysis for text 2\n",
      "📝 Processing text 3/5: 'The economy has shown resilience despite global ch...'\n",
      "🔍 Extracting activations from google/gemma-2-2b\n",
      "   📝 Loading standard Gemma model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📝 Tokenized input shape: torch.Size([1, 64])\n",
      "   🔍 Processing text through standard Gemma\n",
      "   ✅ Extracted activations: torch.Size([1, 64, 2304])\n",
      "🔍 Extracting activations from google/paligemma2-3b-pt-224\n",
      "   📷 Loading PaliGemma (extracting Gemma decoder)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Extracted Gemma decoder: <class 'transformers.models.gemma2.modeling_gemma2.Gemma2Model'>\n",
      "   📝 Tokenized input shape: torch.Size([1, 64])\n",
      "   🔍 Processing text through PaliGemma's Gemma decoder\n",
      "   ✅ Extracted activations: torch.Size([1, 64, 2304])\n",
      "   ✅ Completed analysis for text 3\n",
      "📝 Processing text 4/5: 'Climate change affects weather patterns around the...'\n",
      "🔍 Extracting activations from google/gemma-2-2b\n",
      "   📝 Loading standard Gemma model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📝 Tokenized input shape: torch.Size([1, 64])\n",
      "   🔍 Processing text through standard Gemma\n",
      "   ✅ Extracted activations: torch.Size([1, 64, 2304])\n",
      "🔍 Extracting activations from google/paligemma2-3b-pt-224\n",
      "   📷 Loading PaliGemma (extracting Gemma decoder)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Extracted Gemma decoder: <class 'transformers.models.gemma2.modeling_gemma2.Gemma2Model'>\n",
      "   📝 Tokenized input shape: torch.Size([1, 64])\n",
      "   🔍 Processing text through PaliGemma's Gemma decoder\n",
      "   ✅ Extracted activations: torch.Size([1, 64, 2304])\n",
      "   ✅ Completed analysis for text 4\n",
      "📝 Processing text 5/5: 'Artificial intelligence transforms how we work and...'\n",
      "🔍 Extracting activations from google/gemma-2-2b\n",
      "   📝 Loading standard Gemma model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📝 Tokenized input shape: torch.Size([1, 64])\n",
      "   🔍 Processing text through standard Gemma\n",
      "   ✅ Extracted activations: torch.Size([1, 64, 2304])\n",
      "🔍 Extracting activations from google/paligemma2-3b-pt-224\n",
      "   📷 Loading PaliGemma (extracting Gemma decoder)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Extracted Gemma decoder: <class 'transformers.models.gemma2.modeling_gemma2.Gemma2Model'>\n",
      "   📝 Tokenized input shape: torch.Size([1, 64])\n",
      "   🔍 Processing text through PaliGemma's Gemma decoder\n",
      "   ✅ Extracted activations: torch.Size([1, 64, 2304])\n",
      "   ✅ Completed analysis for text 5\n",
      "\n",
      "📊 ANALYSIS RESULTS:\n",
      "========================================\n",
      "\n",
      "Average SAE Metrics - Model 1:\n",
      "  reconstruction_loss: 208.8037\n",
      "  l0_sparsity: 0.0460\n",
      "  l1_sparsity: 0.3081\n",
      "  fraction_alive: 0.3566\n",
      "  mean_max_activation: 70.5644\n",
      "  reconstruction_score: -6.6486\n",
      "\n",
      "Average SAE Metrics - Model 2:\n",
      "  reconstruction_loss: 17.4955\n",
      "  l0_sparsity: 0.0566\n",
      "  l1_sparsity: 0.2921\n",
      "  fraction_alive: 0.0909\n",
      "  mean_max_activation: 45.3976\n",
      "  reconstruction_score: -0.0509\n",
      "\n",
      "Average Representation Shift Metrics:\n",
      "  cosine_similarity: 0.7945\n",
      "  l2_distance: 102.1991\n",
      "  feature_overlap: 0.0402\n",
      "  js_divergence: -7.5626\n",
      "  feature_correlation: 0.7821\n",
      "\n",
      "🔍 INTERPRETATIONS - LLM->VLM Adaptation:\n",
      "==================================================\n",
      "Sae Quality: ⚠️ SAE is sparse but high reconstruction loss - may be losing information\n",
      "Shift Magnitude: ⚠️ Moderate representation shift - some shared features\n",
      "Model Similarity: 🔍 Models show different SAE characteristics - architectural differences detected\n",
      "\n",
      "🧠 LLM->VLM Adaptation Insights:\n",
      "----------------------------------------\n",
      "⚠️  MODERATE adaptation - some representational drift during VLM training\n",
      "🔄 FEATURE SPECIALIZATION - VLM uses different feature combinations than LLM\n",
      "⚡ EFFICIENCY CHANGE - Multimodal training affected information encoding efficiency\n",
      "✅ Visualization saved to sae_analysis.png\n",
      "\n",
      "✅ Analysis complete!\n",
      "📈 Visualization saved as 'sae_analysis.png'\n",
      "📋 Analyzed 5 texts across layer 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npip install sae-lens transformers torch matplotlib seaborn numpy\\n\\n# For CUDA support (recommended):\\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main demonstration of SAE-based representation shift analysis.\"\"\"\n",
    "    print(\"🚀 SAE Lens - Gemma Scope Representation Shift Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Configuration\n",
    "    LAYER = 12  # Middle layer for analysis\n",
    "    WIDTH = \"16k\"  # SAE width\n",
    "    MODEL_SIZE = \"2b\"  # Using 2B models for faster demo\n",
    "    SUFFIX = \"canonical\"  # Use canonical SAEs (most stable)\n",
    "    \n",
    "    # Test texts covering different domains\n",
    "    test_texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"In machine learning, neural networks learn complex patterns from data.\",\n",
    "        \"The economy has shown resilience despite global challenges.\",\n",
    "        \"Climate change affects weather patterns around the world.\",\n",
    "        \"Artificial intelligence transforms how we work and live.\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Initialize analyzer\n",
    "        analyzer = GemmaScopeAnalyzer(\n",
    "            layer=LAYER, \n",
    "            width=WIDTH, \n",
    "            model_size=MODEL_SIZE,\n",
    "            suffix=SUFFIX,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Model names for LLM->VLM adaptation study  \n",
    "        model1_name = \"google/gemma-2-2b\"  # Base Gemma-2-2B (LLM)\n",
    "        model2_name = \"google/paligemma2-3b-pt-224\"  # PaliGemma using Gemma-2-2B decoder (VLM)\n",
    "        \n",
    "        print(f\"\\n🔬 Research Question: Representational shift during LLM->VLM adaptation\")\n",
    "        print(f\"   Comparing the SAME Gemma-2-2B architecture before and after multimodal training\")\n",
    "        \n",
    "        print(f\"\\n🔬 Analysis Configuration:\")\n",
    "        print(f\"   Layer: {LAYER} (Gemma decoder layer)\")\n",
    "        print(f\"   SAE Width: {WIDTH}\")\n",
    "        print(f\"   Model Size: {MODEL_SIZE}\")\n",
    "        print(f\"   SAE Suffix: {SUFFIX}\")\n",
    "        print(f\"   Test Texts: {len(test_texts)}\")\n",
    "        print(f\"   Research Focus: LLM->VLM representational adaptation\")\n",
    "        print()\n",
    "        \n",
    "        # Run analysis\n",
    "        results = analyzer.analyze_models(model1_name, model2_name, test_texts)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n📊 ANALYSIS RESULTS:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        agg = results['aggregate']\n",
    "        \n",
    "        print(\"\\nAverage SAE Metrics - Model 1:\")\n",
    "        for key, value in agg['avg_model1_metrics'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nAverage SAE Metrics - Model 2:\")\n",
    "        for key, value in agg['avg_model2_metrics'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nAverage Representation Shift Metrics:\")\n",
    "        for key, value in agg['avg_shift_metrics'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        # Generate interpretations\n",
    "        interpretations = analyzer.interpret_results(results)\n",
    "        \n",
    "        print(\"\\n🔍 INTERPRETATIONS - LLM->VLM Adaptation:\")\n",
    "        print(\"=\" * 50)\n",
    "        for aspect, interpretation in interpretations.items():\n",
    "            print(f\"{aspect.replace('_', ' ').title()}: {interpretation}\")\n",
    "        \n",
    "        # Add specific LLM->VLM insights\n",
    "        agg = results['aggregate']\n",
    "        cosine_sim = agg['avg_shift_metrics']['cosine_similarity']\n",
    "        feature_overlap = agg['avg_shift_metrics']['feature_overlap']\n",
    "        \n",
    "        print(f\"\\n🧠 LLM->VLM Adaptation Insights:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if cosine_sim < 0.7:\n",
    "            print(\"🔍 SIGNIFICANT adaptation detected - multimodal training substantially changed representations\")\n",
    "        elif cosine_sim < 0.85:\n",
    "            print(\"⚠️  MODERATE adaptation - some representational drift during VLM training\")  \n",
    "        else:\n",
    "            print(\"✅ MINIMAL adaptation - Gemma decoder largely preserved during VLM training\")\n",
    "            \n",
    "        if feature_overlap < 0.4:\n",
    "            print(\"🔄 FEATURE SPECIALIZATION - VLM uses different feature combinations than LLM\")\n",
    "        elif feature_overlap < 0.6:\n",
    "            print(\"📊 PARTIAL REUSE - VLM partially reuses LLM features with modifications\")\n",
    "        else:\n",
    "            print(\"🔗 FEATURE PRESERVATION - VLM largely reuses LLM feature representations\")\n",
    "            \n",
    "        recon_diff = abs(agg['avg_model1_metrics']['reconstruction_loss'] - \n",
    "                        agg['avg_model2_metrics']['reconstruction_loss'])\n",
    "        if recon_diff > 0.1:\n",
    "            print(\"⚡ EFFICIENCY CHANGE - Multimodal training affected information encoding efficiency\")\n",
    "        \n",
    "        # Create visualization\n",
    "        analyzer.visualize_results(results)\n",
    "        \n",
    "        print(f\"\\n✅ Analysis complete!\")\n",
    "        print(f\"📈 Visualization saved as 'sae_analysis.png'\")\n",
    "        print(f\"📋 Analyzed {len(test_texts)} texts across layer {LAYER}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during analysis: {e}\")\n",
    "        print(\"\\n💡 Troubleshooting tips:\")\n",
    "        print(\"   1. Install SAE Lens: pip install sae-lens\")\n",
    "        print(\"   2. Ensure you have sufficient GPU memory\")\n",
    "        print(\"   3. Try with smaller models or fewer texts\")\n",
    "        print(\"   4. Check model names are correct and accessible\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Installation requirements:\n",
    "\"\"\"\n",
    "pip install sae-lens transformers torch matplotlib seaborn numpy\n",
    "\n",
    "# For CUDA support (recommended):\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faecf625",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results\n",
      "\u001b[31mNameError\u001b[39m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "905c0686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:58<00:00, 29.01s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.56s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import PaliGemmaForConditionalGeneration\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "                    pretrained_model_name_or_path = 'google/paligemma2-3b-pt-224', \n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=torch.float32,\n",
    "                    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "082139b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Extracted Gemma decoder: <class 'transformers.models.gemma2.modeling_gemma2.Gemma2Model'>\n"
     ]
    }
   ],
   "source": [
    "language_model = model.language_model\n",
    "print(f\"   ✅ Extracted Gemma decoder: {type(language_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dcaa489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2Model(\n",
       "  (embed_tokens): Embedding(257216, 2304, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-25): 26 x Gemma2DecoderLayer(\n",
       "      (self_attn): Gemma2Attention(\n",
       "        (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "      )\n",
       "      (mlp): Gemma2MLP(\n",
       "        (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "        (act_fn): PytorchGELUTanh()\n",
       "      )\n",
       "      (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  (rotary_emb): Gemma2RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model # 26 layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
