{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9b5bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Comprehensive SAE-based representation shift analysis with layer sweeping,\n",
    "real datasets, and patching logic for LLM->VLM adaptation studies.\n",
    "\"\"\"\n",
    "# Installation requirements:\n",
    "\"\"\"\n",
    "pip install sae-lens transformers torch matplotlib seaborn numpy datasets tqdm\n",
    "\n",
    "# For CUDA support (recommended):\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sae_lens import SAE\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, List, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e47b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Comprehensive SAE Layer Sweep Analysis: LLM→VLM Adaptation\n",
      "======================================================================\n",
      "🔧 Initialized SAE Analyzer\n",
      "   Device: cuda\n",
      "   Model Size: 2b\n",
      "   SAE Width: 16k\n",
      "   Output Dir: ../figs_tabs\n",
      "\n",
      "📚 Loading Datasets...\n",
      "✅ Loaded 40 texts from mixed datasets\n",
      "Sample texts: ['An image showing a cup.', 'A photo of a red apple on a white background.', 'An image showing a boy.']\n",
      "\n",
      "🔬 Research Configuration:\n",
      "   Model 1 (LLM): google/gemma-2-2b\n",
      "   Model 2 (VLM): google/paligemma2-3b-pt-224\n",
      "   Layers to analyze: [4, 8, 12, 16, 20, 24]\n",
      "   SAE Configuration: 2b-16k-canonical\n",
      "   Device: cuda\n",
      "   Total texts: 40\n",
      "\n",
      "🚀 Starting Layer Sweep Analysis...\n",
      "🚀 Starting Layer Sweep Analysis\n",
      "   Model 1: google/gemma-2-2b\n",
      "   Model 2: google/paligemma2-3b-pt-224\n",
      "   Layers: [4, 8, 12, 16, 20, 24]\n",
      "   Texts: 40 samples\n",
      "   Memory: 0.00GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Processing Layer 4\n",
      "   📥 Loading SAE Layer 4: layer_4/width_16k/canonical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer 4 texts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 66.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:   5%|▌         | 1/20 [00:13<04:16, 13.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  10%|█         | 2/20 [00:26<03:53, 12.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  15%|█▌        | 3/20 [00:38<03:37, 12.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  20%|██        | 4/20 [00:51<03:24, 12.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 59.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  25%|██▌       | 5/20 [01:04<03:11, 12.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 53.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  30%|███       | 6/20 [01:16<02:57, 12.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  35%|███▌      | 7/20 [01:29<02:43, 12.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  40%|████      | 8/20 [01:41<02:28, 12.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 59.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  45%|████▌     | 9/20 [01:53<02:15, 12.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 59.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  50%|█████     | 10/20 [02:04<02:01, 12.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  55%|█████▌    | 11/20 [02:16<01:48, 12.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  60%|██████    | 12/20 [02:29<01:37, 12.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  65%|██████▌   | 13/20 [02:41<01:25, 12.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  70%|███████   | 14/20 [02:53<01:12, 12.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  75%|███████▌  | 15/20 [03:05<01:00, 12.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  80%|████████  | 16/20 [03:17<00:48, 12.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 57.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  85%|████████▌ | 17/20 [03:29<00:35, 11.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  90%|█████████ | 18/20 [03:41<00:24, 12.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts:  95%|█████████▌| 19/20 [03:53<00:12, 12.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 4 texts: 100%|██████████| 20/20 [04:05<00:00, 12.15s/it]\u001b[A\n",
      "Processing layers:  17%|█▋        | 1/6 [04:09<20:48, 249.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "   Memory after layer 4: 12.53GB\n",
      "\n",
      "📊 Processing Layer 8\n",
      "   📥 Loading SAE Layer 8: layer_8/width_16k/canonical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer 8 texts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:   5%|▌         | 1/20 [00:12<03:48, 12.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 57.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  10%|█         | 2/20 [00:23<03:31, 11.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  15%|█▌        | 3/20 [00:35<03:22, 11.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  20%|██        | 4/20 [00:47<03:11, 11.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  25%|██▌       | 5/20 [00:59<02:59, 11.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  30%|███       | 6/20 [01:11<02:48, 12.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 56.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  35%|███▌      | 7/20 [01:23<02:35, 11.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 56.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  40%|████      | 8/20 [01:35<02:23, 11.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  45%|████▌     | 9/20 [01:47<02:12, 12.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  50%|█████     | 10/20 [01:59<01:59, 11.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 56.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  55%|█████▌    | 11/20 [02:11<01:46, 11.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  60%|██████    | 12/20 [02:23<01:34, 11.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  65%|██████▌   | 13/20 [02:34<01:22, 11.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  70%|███████   | 14/20 [02:47<01:11, 11.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  75%|███████▌  | 15/20 [02:58<00:59, 11.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  80%|████████  | 16/20 [03:10<00:47, 11.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 55.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  85%|████████▌ | 17/20 [03:22<00:35, 11.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  90%|█████████ | 18/20 [03:34<00:23, 11.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts:  95%|█████████▌| 19/20 [03:45<00:11, 11.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 8 texts: 100%|██████████| 20/20 [03:57<00:00, 11.73s/it]\u001b[A\n",
      "Processing layers:  33%|███▎      | 2/6 [08:07<16:11, 242.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "   Memory after layer 8: 12.83GB\n",
      "\n",
      "📊 Processing Layer 12\n",
      "   📥 Loading SAE Layer 12: layer_12/width_16k/canonical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer 12 texts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:   5%|▌         | 1/20 [00:11<03:38, 11.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  10%|█         | 2/20 [00:23<03:33, 11.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  15%|█▌        | 3/20 [00:35<03:22, 11.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 56.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  20%|██        | 4/20 [00:47<03:12, 12.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 63.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  25%|██▌       | 5/20 [00:59<02:57, 11.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 57.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  30%|███       | 6/20 [01:10<02:44, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  35%|███▌      | 7/20 [01:22<02:32, 11.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  40%|████      | 8/20 [01:34<02:21, 11.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 59.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  45%|████▌     | 9/20 [01:46<02:09, 11.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 57.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.99it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer 12 texts:  50%|█████     | 10/20 [01:58<01:58, 11.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 57.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  55%|█████▌    | 11/20 [02:09<01:46, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  60%|██████    | 12/20 [02:21<01:34, 11.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  65%|██████▌   | 13/20 [02:33<01:23, 11.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 54.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  70%|███████   | 14/20 [02:45<01:11, 11.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 58.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  75%|███████▌  | 15/20 [02:57<00:59, 11.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 57.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  80%|████████  | 16/20 [03:09<00:47, 11.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 57.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  85%|████████▌ | 17/20 [03:21<00:35, 11.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  90%|█████████ | 18/20 [03:33<00:23, 11.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts:  95%|█████████▌| 19/20 [03:44<00:11, 11.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 12 texts: 100%|██████████| 20/20 [03:56<00:00, 11.78s/it]\u001b[A\n",
      "Processing layers:  50%|█████     | 3/6 [12:05<12:01, 240.48s/it]A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "   Memory after layer 12: 12.83GB\n",
      "\n",
      "📊 Processing Layer 16\n",
      "   📥 Loading SAE Layer 16: layer_16/width_16k/canonical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer 16 texts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 57.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:   5%|▌         | 1/20 [00:11<03:44, 11.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 55.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  10%|█         | 2/20 [00:23<03:34, 11.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  15%|█▌        | 3/20 [00:35<03:22, 11.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 63.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  20%|██        | 4/20 [00:47<03:08, 11.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 55.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  25%|██▌       | 5/20 [00:59<02:56, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  30%|███       | 6/20 [01:11<02:46, 11.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  35%|███▌      | 7/20 [01:23<02:35, 11.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 57.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  40%|████      | 8/20 [01:35<02:23, 11.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 65.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  45%|████▌     | 9/20 [01:46<02:10, 11.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 58.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  50%|█████     | 10/20 [01:58<01:57, 11.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 10.51it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  55%|█████▌    | 11/20 [02:10<01:47, 11.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  60%|██████    | 12/20 [02:22<01:35, 11.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 63.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  65%|██████▌   | 13/20 [02:34<01:23, 11.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 63.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  70%|███████   | 14/20 [02:46<01:12, 12.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  75%|███████▌  | 15/20 [02:59<01:00, 12.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  80%|████████  | 16/20 [03:11<00:48, 12.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  85%|████████▌ | 17/20 [03:23<00:36, 12.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  90%|█████████ | 18/20 [03:34<00:23, 11.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 56.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts:  95%|█████████▌| 19/20 [03:46<00:11, 11.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 16 texts: 100%|██████████| 20/20 [03:58<00:00, 11.95s/it]\u001b[A\n",
      "Processing layers:  67%|██████▋   | 4/6 [16:09<08:03, 241.95s/it]A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "   Memory after layer 16: 12.83GB\n",
      "\n",
      "📊 Processing Layer 20\n",
      "   📥 Loading SAE Layer 20: layer_20/width_16k/canonical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer 20 texts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:   5%|▌         | 1/20 [00:12<03:49, 12.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 56.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  10%|█         | 2/20 [00:23<03:35, 11.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  15%|█▌        | 3/20 [00:36<03:24, 12.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  20%|██        | 4/20 [00:47<03:11, 11.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  25%|██▌       | 5/20 [01:00<03:00, 12.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 58.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  30%|███       | 6/20 [01:11<02:46, 11.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  35%|███▌      | 7/20 [01:23<02:33, 11.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 57.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  40%|████      | 8/20 [01:35<02:22, 11.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  45%|████▌     | 9/20 [01:47<02:11, 11.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  50%|█████     | 10/20 [01:59<01:58, 11.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  55%|█████▌    | 11/20 [02:10<01:46, 11.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  60%|██████    | 12/20 [02:22<01:34, 11.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 59.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  65%|██████▌   | 13/20 [02:34<01:22, 11.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  70%|███████   | 14/20 [02:46<01:11, 11.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  75%|███████▌  | 15/20 [02:58<00:59, 11.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  80%|████████  | 16/20 [03:10<00:47, 11.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  85%|████████▌ | 17/20 [03:21<00:35, 11.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  90%|█████████ | 18/20 [03:33<00:23, 11.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts:  95%|█████████▌| 19/20 [03:45<00:11, 11.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 63.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 20 texts: 100%|██████████| 20/20 [03:56<00:00, 11.67s/it]\u001b[A\n",
      "Processing layers:  83%|████████▎ | 5/6 [20:11<04:02, 242.07s/it]A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "   Memory after layer 20: 12.83GB\n",
      "\n",
      "📊 Processing Layer 24\n",
      "   📥 Loading SAE Layer 24: layer_24/width_16k/canonical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer 24 texts:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:   5%|▌         | 1/20 [00:11<03:44, 11.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  10%|█         | 2/20 [00:23<03:29, 11.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 63.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  15%|█▌        | 3/20 [00:34<03:16, 11.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 63.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  20%|██        | 4/20 [00:46<03:05, 11.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  25%|██▌       | 5/20 [00:58<02:55, 11.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  30%|███       | 6/20 [01:10<02:44, 11.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  35%|███▌      | 7/20 [01:21<02:31, 11.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  40%|████      | 8/20 [01:33<02:19, 11.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 64.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  45%|████▌     | 9/20 [01:45<02:08, 11.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  50%|█████     | 10/20 [01:57<01:58, 11.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  55%|█████▌    | 11/20 [02:08<01:45, 11.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  60%|██████    | 12/20 [02:20<01:33, 11.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 64.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  65%|██████▌   | 13/20 [02:32<01:21, 11.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  70%|███████   | 14/20 [02:43<01:10, 11.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 59.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  75%|███████▌  | 15/20 [02:56<01:00, 12.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  80%|████████  | 16/20 [03:08<00:47, 12.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  85%|████████▌ | 17/20 [03:20<00:36, 12.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  90%|█████████ | 18/20 [03:32<00:24, 12.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 61.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts:  95%|█████████▌| 19/20 [03:44<00:11, 11.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 60.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "📥 Loading model: google/paligemma2-3b-pt-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Layer 24 texts: 100%|██████████| 20/20 [03:56<00:00, 11.84s/it]\u001b[A\n",
      "Processing layers: 100%|██████████| 6/6 [24:14<00:00, 242.36s/it]A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Error computing unpatched loss: 'BaseModelOutputWithPast' object has no attribute 'logits'\n",
      "⚠️  Patching failed: name 'tokenizer' is not defined\n",
      "   Memory after layer 24: 12.83GB\n",
      "\n",
      "📊 LAYER SWEEP RESULTS:\n",
      "==================================================\n",
      "Most Similar Layer: 4\n",
      "Most Different Layer: 24\n",
      "Highest Feature Overlap Layer: 8\n",
      "Average Similarity Across Layers: 0.638\n",
      "Average Feature Overlap: 0.028\n",
      "\n",
      "🔍 INTERPRETATIONS:\n",
      "==================================================\n",
      "Adaptation Magnitude: 🔍 SIGNIFICANT LLM→VLM adaptation - substantial representational reorganization\n",
      "Adaptation Location: 🎯 Layer 24 shows maximum adaptation\n",
      "Feature Preservation: 🔗 Layer 8 best preserves LLM features\n",
      "Adaptation Pattern: 📈 Early layers preserve LLM representations better than late layers\n",
      "Sae Quality: ❌ SAE reconstructions significantly impact model functionality\n",
      "\n",
      "📈 Generating Visualizations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Layer sweep visualization saved to ../figs_tabs/google_gemma_2_2b_google_paligemma2_3b_pt_224_layer_sweep.png\n",
      "✅ Detailed results saved to ../figs_tabs/google_gemma_2_2b_google_paligemma2_3b_pt_224_results.json\n",
      "\n",
      "✅ Analysis Complete!\n",
      "📁 Results saved to: ../figs_tabs\n",
      "🧠 Key Finding: 🔍 SIGNIFICANT LLM→VLM adaptation - substantial representational reorganization\n",
      "🔧 Final GPU Memory: 12.83GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npip install sae-lens transformers torch matplotlib seaborn numpy datasets tqdm\\n\\n# For CUDA support (recommended):\\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Comprehensive SAE-based representation shift analysis with layer sweeping,\n",
    "real datasets, and patching logic for LLM->VLM adaptation studies.\n",
    "FIXED: Handles PaliGemma loss computation correctly.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sae_lens import SAE\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, List, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Disable gradients globally for memory efficiency\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "@dataclass\n",
    "class SAEMetrics:\n",
    "    \"\"\"Container for SAE evaluation metrics.\"\"\"\n",
    "    reconstruction_loss: float\n",
    "    l0_sparsity: float\n",
    "    l1_sparsity: float\n",
    "    fraction_alive: float\n",
    "    mean_max_activation: float\n",
    "    reconstruction_score: float\n",
    "    model_delta_loss: float  # New patching metric\n",
    "\n",
    "@dataclass\n",
    "class RepresentationShift:\n",
    "    \"\"\"Container for representation shift metrics.\"\"\"\n",
    "    cosine_similarity: float\n",
    "    l2_distance: float\n",
    "    feature_overlap: float\n",
    "    js_divergence: float\n",
    "    feature_correlation: float\n",
    "\n",
    "class DatasetLoader:\n",
    "    \"\"\"Handles loading and preprocessing of various datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, device: str = \"cuda\", data_save_dir: str = \"../data\"):\n",
    "        self.device = device\n",
    "        self.data_save_dir = data_save_dir\n",
    "    \n",
    "    def load_cifar100_captions(self, split: str = \"train\", max_samples: int = 100) -> List[str]:\n",
    "        \"\"\"Load CIFAR-100 with generated captions for multimodal analysis.\"\"\"\n",
    "        try:\n",
    "            # CIFAR-100 doesn't have captions by default, so we create descriptive ones\n",
    "            dataset = load_dataset(\"cifar100\", split=split)\n",
    "            \n",
    "            # CIFAR-100 class names\n",
    "            class_names = [\n",
    "                'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
    "                'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
    "                'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
    "                'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
    "                'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
    "                'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "                'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "                'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "                'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
    "                'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
    "                'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
    "                'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
    "                'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
    "                'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
    "            ]\n",
    "            \n",
    "            texts = []\n",
    "            for i, sample in enumerate(dataset):\n",
    "                if i >= max_samples:\n",
    "                    break\n",
    "                class_name = class_names[sample['fine_label']]\n",
    "                # Generate descriptive captions\n",
    "                captions = [\n",
    "                    f\"This is a photo of a {class_name}.\",\n",
    "                    f\"An image showing a {class_name}.\",\n",
    "                    f\"A picture of a {class_name} in natural setting.\",\n",
    "                    f\"Visual representation of a {class_name}.\"\n",
    "                ]\n",
    "                texts.extend(captions[:2])  # Take 2 captions per image\n",
    "            \n",
    "            print(f\"✅ Loaded {len(texts)} CIFAR-100 captions\")\n",
    "            return texts[:max_samples]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading CIFAR-100: {e}\")\n",
    "            return self._get_fallback_texts()\n",
    "    \n",
    "    def load_coco_captions(self, split: str = \"validation\", max_samples: int = 100) -> List[str]:\n",
    "        \"\"\"Load COCO captions dataset.\"\"\"\n",
    "        try:\n",
    "            # Load COCO captions\n",
    "            dataset = load_dataset(\"HuggingFaceM4/COCO\", split=split)\n",
    "            \n",
    "            texts = []\n",
    "            for i, sample in enumerate(dataset):\n",
    "                if i >= max_samples:\n",
    "                    break\n",
    "                \n",
    "                # COCO has multiple captions per image\n",
    "                if 'sentences' in sample and 'raw' in sample['sentences']:\n",
    "                    for sentence in sample['sentences']['raw'][:2]:  # Take first 2 captions\n",
    "                        texts.append(sentence)\n",
    "                elif 'caption' in sample:\n",
    "                    texts.append(sample['caption'])\n",
    "            \n",
    "            print(f\"✅ Loaded {len(texts)} COCO captions\")\n",
    "            return texts[:max_samples]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading COCO: {e}\")\n",
    "            # Try alternative COCO dataset\n",
    "            try:\n",
    "                dataset = load_dataset(\"nielsr/coco-captions\", split=\"validation\")\n",
    "                texts = [sample['caption'] for sample in dataset.select(range(min(max_samples, len(dataset))))]\n",
    "                print(f\"✅ Loaded {len(texts)} COCO captions (alternative)\")\n",
    "                return texts\n",
    "            except:\n",
    "                return self._get_fallback_texts()\n",
    "    \n",
    "    def load_llava_bench(self, max_samples: int = 100) -> List[str]:\n",
    "        \"\"\"Load LLaVA-Bench questions/descriptions.\"\"\"\n",
    "        try:\n",
    "            # LLaVA bench conversations\n",
    "            dataset = load_dataset(\"lmms-lab/LLaVA-OneVision-Data\", split=\"dev_mini\")\n",
    "            \n",
    "            texts = []\n",
    "            for i, sample in enumerate(dataset):\n",
    "                if i >= max_samples:\n",
    "                    break\n",
    "                \n",
    "                if 'conversations' in sample:\n",
    "                    for conv in sample['conversations'][:2]:  # Take first 2 conversations\n",
    "                        if 'value' in conv:\n",
    "                            texts.append(conv['value'])\n",
    "            \n",
    "            print(f\"✅ Loaded {len(texts)} LLaVA-Bench texts\")\n",
    "            return texts[:max_samples]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading LLaVA-Bench: {e}\")\n",
    "            return self._get_fallback_texts()\n",
    "    \n",
    "    def _get_fallback_texts(self) -> List[str]:\n",
    "        \"\"\"Fallback texts if datasets fail to load.\"\"\"\n",
    "        return [\n",
    "            \"A photo of a red apple on a white background.\",\n",
    "            \"The cat is sitting on a wooden chair.\",\n",
    "            \"Mountains covered with snow in winter landscape.\",\n",
    "            \"A blue car driving on a highway.\",\n",
    "            \"Children playing in a park with green grass.\",\n",
    "            \"A delicious chocolate cake on a plate.\",\n",
    "            \"Ocean waves crashing against rocky shore.\",\n",
    "            \"A person reading a book in a library.\",\n",
    "            \"Colorful flowers blooming in spring garden.\",\n",
    "            \"A dog running happily in the field.\",\n",
    "        ]\n",
    "    \n",
    "    def get_mixed_dataset(self, total_samples: int = 150, read_local: bool = False, save: bool = True) -> List[str]:\n",
    "        \"\"\"Get a mixed dataset from multiple sources.\"\"\"\n",
    "        samples_per_source = total_samples // 3\n",
    "        \n",
    "        if read_local:\n",
    "            with open(f\"{self.data_save_dir}/texts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                texts = json.load(f)\n",
    "        else:\n",
    "            texts = []\n",
    "            texts.extend(self.load_cifar100_captions(max_samples=samples_per_source))\n",
    "            texts.extend(self.load_coco_captions(max_samples=samples_per_source))\n",
    "            texts.extend(self.load_llava_bench(max_samples=samples_per_source))\n",
    "\n",
    "\n",
    "            # Shuffle for good measure\n",
    "            \n",
    "            random.shuffle(texts)\n",
    "            if save:\n",
    "                with open(f\"{self.data_save_dir}/texts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(texts, f, ensure_ascii=False, indent=2)\n",
    "                    \n",
    "        return texts[:total_samples]\n",
    "\n",
    "class MemoryEfficientSAEAnalyzer:\n",
    "    \"\"\"Memory-efficient SAE analyzer with layer sweeping and patching logic.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_size: str = \"2b\",\n",
    "                 width: str = \"16k\", \n",
    "                 suffix: str = \"canonical\",\n",
    "                 device: str = \"cuda\",\n",
    "                 output_dir: str = \"../figs_tabs\"):\n",
    "        \"\"\"\n",
    "        Initialize memory-efficient SAE analyzer.\n",
    "        \n",
    "        Args:\n",
    "            model_size: Model size (\"2b\" or \"9b\")\n",
    "            width: SAE width (\"16k\", \"65k\", \"262k\")\n",
    "            suffix: SAE variant (\"canonical\" or specific L0)\n",
    "            device: Device to use\n",
    "            output_dir: Directory for saving outputs\n",
    "        \"\"\"\n",
    "        self.device = device if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model_size = model_size\n",
    "        self.width = width\n",
    "        self.suffix = suffix\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Model cache for memory efficiency\n",
    "        self.model_cache = {}\n",
    "        self.sae_cache = {}\n",
    "        \n",
    "        print(f\"🔧 Initialized SAE Analyzer\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Model Size: {model_size}\")\n",
    "        print(f\"   SAE Width: {width}\")\n",
    "        print(f\"   Output Dir: {output_dir}\")\n",
    "\n",
    "    def get_gemmascope_sae(self, layer: int) -> SAE:\n",
    "        \"\"\"Load Gemma Scope SAE with caching for memory efficiency.\"\"\"\n",
    "        cache_key = f\"layer_{layer}\"\n",
    "        \n",
    "        if cache_key in self.sae_cache:\n",
    "            return self.sae_cache[cache_key]\n",
    "        \n",
    "        release = f\"gemma-scope-{self.model_size}-pt-res\"\n",
    "        if self.suffix == \"canonical\":\n",
    "            release = f\"gemma-scope-{self.model_size}-pt-res-canonical\"\n",
    "            sae_id = f\"layer_{layer}/width_{self.width}/canonical\"\n",
    "        else:\n",
    "            sae_id = f\"layer_{layer}/width_{self.width}/{self.suffix}\"\n",
    "        \n",
    "        print(f\"   📥 Loading SAE Layer {layer}: {sae_id}\")\n",
    "        \n",
    "        try:\n",
    "            sae = SAE.from_pretrained(release, sae_id).to(self.device)\n",
    "            sae.eval()\n",
    "            \n",
    "            # Cache management - keep only last 2 SAEs to save memory\n",
    "            if len(self.sae_cache) >= 2:\n",
    "                oldest_key = list(self.sae_cache.keys())[0]\n",
    "                del self.sae_cache[oldest_key]\n",
    "                gc.collect()\n",
    "            \n",
    "            self.sae_cache[cache_key] = sae\n",
    "            return sae\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading SAE layer {layer}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_model(self, model_name: str):\n",
    "        \"\"\"Load model with caching and proper device placement.\"\"\"\n",
    "        if model_name in self.model_cache:\n",
    "            return self.model_cache[model_name]\n",
    "        \n",
    "        print(f\"📥 Loading model: {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Handle different model types\n",
    "            if \"paligemma\" in model_name.lower():\n",
    "                from transformers import PaliGemmaForConditionalGeneration\n",
    "                model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "                    model_name, \n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=torch.float32,  # Use fp16 for memory efficiency\n",
    "                    device_map=None  # We'll handle device placement manually\n",
    "                )\n",
    "                model = model.to(self.device)\n",
    "                language_model = model.language_model\n",
    "            else:\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name, \n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=torch.float32,\n",
    "                    device_map=None\n",
    "                )\n",
    "                model = model.to(self.device)\n",
    "                language_model = model\n",
    "            \n",
    "            language_model.eval()\n",
    "            \n",
    "            # Cache management - keep only one model at a time\n",
    "            if len(self.model_cache) >= 1:\n",
    "                for cached_name in list(self.model_cache.keys()):\n",
    "                    del self.model_cache[cached_name]\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "            self.model_cache[model_name] = (tokenizer, model, language_model)\n",
    "            return tokenizer, model, language_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading model {model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def extract_activations_with_patching(self, \n",
    "                                        model_name: str, \n",
    "                                        text: str, \n",
    "                                        layer: int,\n",
    "                                        sae: Optional[SAE] = None) -> Tuple[torch.Tensor, float]:\n",
    "        \"\"\"\n",
    "        Extract activations and compute model delta loss with patching.\n",
    "        FIXED: Properly handles loss computation for both model types.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (activations, model_delta_loss)\n",
    "        \"\"\"\n",
    "        tokenizer, model, language_model = self.get_model(model_name)\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=64\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get unpatched model loss (baseline)\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                if \"paligemma\" in model_name.lower():\n",
    "                    # For PaliGemma, we need to handle the text-only input properly\n",
    "                    # Create target labels for loss computation\n",
    "                    labels = inputs['input_ids'].clone()\n",
    "                    # Mask padding tokens in labels\n",
    "                    labels[labels == tokenizer.pad_token_id] = -100\n",
    "                    unpatched_outputs = language_model(**inputs, labels=labels)\n",
    "                else:\n",
    "                    # For regular language models\n",
    "                    labels = inputs['input_ids'].clone()\n",
    "                    labels[labels == tokenizer.pad_token_id] = -100\n",
    "                    unpatched_outputs = language_model(**inputs, labels=labels)\n",
    "                \n",
    "                # Check if loss is available\n",
    "                if hasattr(unpatched_outputs, 'loss') and unpatched_outputs.loss is not None:\n",
    "                    unpatched_loss = unpatched_outputs.loss.item()\n",
    "                else:\n",
    "                    # Compute loss manually using logits\n",
    "                    logits = unpatched_outputs.logits\n",
    "                    shift_logits = logits[..., :-1, :].contiguous()\n",
    "                    shift_labels = labels[..., 1:].contiguous()\n",
    "                    \n",
    "                    # Flatten for cross entropy computation\n",
    "                    shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "                    shift_labels = shift_labels.view(-1)\n",
    "                    \n",
    "                    # Compute cross entropy loss (ignore -100 labels)\n",
    "                    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "                    unpatched_loss = loss_fct(shift_logits, shift_labels).item()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Error computing unpatched loss: {e}\")\n",
    "                unpatched_loss = 0.0\n",
    "        \n",
    "        # Extract activations from target layer\n",
    "        activations = None\n",
    "        patched_loss = unpatched_loss  # Default if no patching\n",
    "        \n",
    "        def activation_hook(module, input, output):\n",
    "            nonlocal activations\n",
    "            if isinstance(output, tuple):\n",
    "                activations = output[0].clone()\n",
    "            else:\n",
    "                activations = output.clone()\n",
    "        \n",
    "        # Hook the target layer\n",
    "        if hasattr(language_model, 'model') and hasattr(language_model.model, 'layers'):\n",
    "            target_layer = language_model.model.layers[layer]\n",
    "        elif hasattr(language_model, 'layers'):\n",
    "            target_layer = language_model.layers[layer]\n",
    "        else:\n",
    "            print(f\"❌ Could not find layer {layer}\")\n",
    "            return torch.randn(1, 64, 2304).to(self.device), 0.0\n",
    "        \n",
    "        hook = target_layer.register_forward_hook(activation_hook)\n",
    "        \n",
    "        # Forward pass to get activations\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                if \"paligemma\" in model_name.lower():\n",
    "                    _ = language_model(**inputs)\n",
    "                else:\n",
    "                    _ = language_model(**inputs)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Error in activation extraction: {e}\")\n",
    "        \n",
    "        hook.remove()\n",
    "        \n",
    "        # Compute patched loss if SAE is provided\n",
    "        if sae is not None and activations is not None:\n",
    "            patched_loss = self._compute_patched_loss(\n",
    "                language_model, inputs, activations, sae, layer, model_name\n",
    "            )\n",
    "        \n",
    "        model_delta_loss = patched_loss - unpatched_loss\n",
    "        \n",
    "        if activations is None:\n",
    "            print(f\"⚠️  Failed to extract activations from layer {layer}\")\n",
    "            activations = torch.randn(1, 64, 2304).to(self.device)\n",
    "        \n",
    "        return activations, model_delta_loss\n",
    "\n",
    "    def _compute_patched_loss(self, \n",
    "                            language_model, \n",
    "                            inputs: Dict, \n",
    "                            original_activations: torch.Tensor, \n",
    "                            sae: SAE, \n",
    "                            layer: int,\n",
    "                            model_name: str) -> float:\n",
    "        \"\"\"Compute loss with SAE-patched activations. FIXED: Proper loss computation.\"\"\"\n",
    "        try:\n",
    "            # Get SAE reconstruction\n",
    "            flat_activations = original_activations.view(-1, original_activations.size(-1))\n",
    "            sae_output = sae(flat_activations)\n",
    "            \n",
    "            # Handle different SAE output formats\n",
    "            if hasattr(sae_output, 'sae_out'):\n",
    "                reconstructed = sae_output.sae_out\n",
    "            elif isinstance(sae_output, tuple):\n",
    "                reconstructed = sae_output[0]\n",
    "            else:\n",
    "                reconstructed = sae_output\n",
    "            \n",
    "            # Reshape back to original shape\n",
    "            reconstructed = reconstructed.view(original_activations.shape)\n",
    "            \n",
    "            # Patch the reconstructed activations back into the model\n",
    "            patched_activations = reconstructed\n",
    "            \n",
    "            # Create a patching hook\n",
    "            def patching_hook(module, input, output):\n",
    "                if isinstance(output, tuple):\n",
    "                    return (patched_activations, *output[1:])\n",
    "                else:\n",
    "                    return patched_activations\n",
    "            \n",
    "            # Hook the target layer for patching\n",
    "            if hasattr(language_model, 'model') and hasattr(language_model.model, 'layers'):\n",
    "                target_layer = language_model.model.layers[layer]\n",
    "            elif hasattr(language_model, 'layers'):\n",
    "                target_layer = language_model.layers[layer]\n",
    "            else:\n",
    "                return 0.0\n",
    "            \n",
    "            patch_hook = target_layer.register_forward_hook(patching_hook)\n",
    "            \n",
    "            # Forward pass with patched activations\n",
    "            with torch.no_grad():\n",
    "                if \"paligemma\" in model_name.lower():\n",
    "                    # For PaliGemma, create proper labels\n",
    "                    labels = inputs['input_ids'].clone()\n",
    "                    labels[labels == tokenizer.pad_token_id] = -100\n",
    "                    patched_outputs = language_model(**inputs, labels=labels)\n",
    "                else:\n",
    "                    # For regular language models\n",
    "                    labels = inputs['input_ids'].clone()\n",
    "                    labels[labels == tokenizer.pad_token_id] = -100\n",
    "                    patched_outputs = language_model(**inputs, labels=labels)\n",
    "                \n",
    "                # Get patched loss\n",
    "                if hasattr(patched_outputs, 'loss') and patched_outputs.loss is not None:\n",
    "                    patched_loss = patched_outputs.loss.item()\n",
    "                else:\n",
    "                    # Compute loss manually\n",
    "                    logits = patched_outputs.logits\n",
    "                    shift_logits = logits[..., :-1, :].contiguous()\n",
    "                    shift_labels = labels[..., 1:].contiguous()\n",
    "                    \n",
    "                    shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "                    shift_labels = shift_labels.view(-1)\n",
    "                    \n",
    "                    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "                    patched_loss = loss_fct(shift_logits, shift_labels).item()\n",
    "            \n",
    "            patch_hook.remove()\n",
    "            return patched_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Patching failed: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def compute_sae_metrics(self, activations: torch.Tensor, sae: SAE, model_delta_loss: float) -> SAEMetrics:\n",
    "        \"\"\"Compute comprehensive SAE evaluation metrics including model delta loss.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Reshape activations for SAE processing\n",
    "            batch_size, seq_len, d_model = activations.shape\n",
    "            flat_activations = activations.view(-1, d_model)\n",
    "            \n",
    "            # Forward pass through SAE\n",
    "            sae_output = sae(flat_activations)\n",
    "            \n",
    "            # Handle different SAE output formats\n",
    "            if hasattr(sae_output, 'feature_acts'):\n",
    "                feature_acts = sae_output.feature_acts\n",
    "                reconstructed = sae_output.sae_out\n",
    "            elif isinstance(sae_output, tuple) and len(sae_output) >= 2:\n",
    "                reconstructed, feature_acts = sae_output[0], sae_output[1]\n",
    "            elif hasattr(sae, 'encode') and hasattr(sae, 'decode'):\n",
    "                feature_acts = sae.encode(flat_activations)\n",
    "                reconstructed = sae.decode(feature_acts)\n",
    "            else:\n",
    "                reconstructed = sae_output\n",
    "                if hasattr(sae, 'W_enc') and hasattr(sae, 'b_enc'):\n",
    "                    feature_acts = torch.relu(flat_activations @ sae.W_enc + sae.b_enc)\n",
    "                else:\n",
    "                    feature_acts = torch.randn(flat_activations.shape[0], 16384, device=flat_activations.device)\n",
    "            \n",
    "            # 1. Reconstruction Loss (MSE)\n",
    "            reconstruction_loss = torch.nn.functional.mse_loss(reconstructed, flat_activations).item()\n",
    "            \n",
    "            # 2. L0 Sparsity (fraction of non-zero features)\n",
    "            l0_sparsity = (feature_acts > 0).float().mean().item()\n",
    "            \n",
    "            # 3. L1 Sparsity (mean absolute activation)\n",
    "            l1_sparsity = feature_acts.abs().mean().item()\n",
    "            \n",
    "            # 4. Fraction of features that are ever active\n",
    "            fraction_alive = (feature_acts.max(dim=0)[0] > 0).float().mean().item()\n",
    "            \n",
    "            # 5. Mean maximum activation per sample\n",
    "            mean_max_activation = feature_acts.max(dim=1)[0].mean().item()\n",
    "            \n",
    "            # 6. Reconstruction score (explained variance)\n",
    "            var_original = flat_activations.var(dim=0).mean()\n",
    "            var_residual = (flat_activations - reconstructed).var(dim=0).mean()\n",
    "            reconstruction_score = max(0.0, 1 - (var_residual / var_original).item())\n",
    "            \n",
    "            return SAEMetrics(\n",
    "                reconstruction_loss=reconstruction_loss,\n",
    "                l0_sparsity=l0_sparsity,\n",
    "                l1_sparsity=l1_sparsity,\n",
    "                fraction_alive=fraction_alive,\n",
    "                mean_max_activation=mean_max_activation,\n",
    "                reconstruction_score=reconstruction_score,\n",
    "                model_delta_loss=model_delta_loss\n",
    "            )\n",
    "\n",
    "    def analyze_layer_sweep(self, \n",
    "                           model1_name: str, \n",
    "                           model2_name: str, \n",
    "                           texts: List[str],\n",
    "                           layers: List[int] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform memory-efficient layer sweep analysis.\n",
    "        \n",
    "        Args:\n",
    "            model1_name: First model (base LLM)\n",
    "            model2_name: Second model (VLM) \n",
    "            texts: List of texts to analyze\n",
    "            layers: List of layers to analyze (default: [8, 12, 16, 20])\n",
    "        \"\"\"\n",
    "        if layers is None:\n",
    "            layers = [8, 12, 16, 20]  # Sample layers across the model\n",
    "        \n",
    "        print(f\"🚀 Starting Layer Sweep Analysis\")\n",
    "        print(f\"   Model 1: {model1_name}\")\n",
    "        print(f\"   Model 2: {model2_name}\")\n",
    "        print(f\"   Layers: {layers}\")\n",
    "        print(f\"   Texts: {len(texts)} samples\")\n",
    "        print(f\"   Memory: {torch.cuda.memory_allocated() / 1e9:.2f}GB\" if torch.cuda.is_available() else \"\")\n",
    "        \n",
    "        results = {\n",
    "            'layers': layers,\n",
    "            'layer_results': {},\n",
    "            'texts': texts[:10]  # Store subset for reference\n",
    "        }\n",
    "        \n",
    "        for layer in tqdm(layers, desc=\"Processing layers\"):\n",
    "            print(f\"\\n📊 Processing Layer {layer}\")\n",
    "            \n",
    "            # Load SAE for this layer\n",
    "            sae = self.get_gemmascope_sae(layer)\n",
    "            \n",
    "            layer_metrics = {\n",
    "                'model1_metrics': [],\n",
    "                'model2_metrics': [],\n",
    "                'shift_metrics': []\n",
    "            }\n",
    "            \n",
    "            # Process subset of texts for each layer (memory efficiency)\n",
    "            sample_texts = texts[:20]  # Process 20 texts per layer\n",
    "            \n",
    "            for i, text in enumerate(tqdm(sample_texts, desc=f\"Layer {layer} texts\", leave=False)):\n",
    "                try:\n",
    "                    # Extract activations and compute metrics for model 1\n",
    "                    acts1, delta_loss1 = self.extract_activations_with_patching(\n",
    "                        model1_name, text, layer, sae\n",
    "                    )\n",
    "                    metrics1 = self.compute_sae_metrics(acts1, sae, delta_loss1)\n",
    "                    \n",
    "                    # Extract activations and compute metrics for model 2\n",
    "                    acts2, delta_loss2 = self.extract_activations_with_patching(\n",
    "                        model2_name, text, layer, sae\n",
    "                    )\n",
    "                    metrics2 = self.compute_sae_metrics(acts2, sae, delta_loss2)\n",
    "                    \n",
    "                    # Compute representation shift\n",
    "                    shift = self.compute_representation_shift(acts1, acts2, sae)\n",
    "                    \n",
    "                    layer_metrics['model1_metrics'].append(metrics1)\n",
    "                    layer_metrics['model2_metrics'].append(metrics2)\n",
    "                    layer_metrics['shift_metrics'].append(shift)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️  Error processing text {i} in layer {layer}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Compute layer-level aggregates\n",
    "            layer_metrics['aggregate'] = self._compute_layer_aggregate(layer_metrics)\n",
    "            results['layer_results'][layer] = layer_metrics\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"   Memory after layer {layer}: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n",
    "        \n",
    "        # Compute overall analysis\n",
    "        results['overall_analysis'] = self._compute_overall_analysis(results)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def compute_representation_shift(self, \n",
    "                                   activations1: torch.Tensor, \n",
    "                                   activations2: torch.Tensor,\n",
    "                                   sae: SAE) -> RepresentationShift:\n",
    "        \"\"\"Compute representation shift metrics using SAE features.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Process both activation sets through SAE\n",
    "            flat_acts1 = activations1.view(-1, activations1.size(-1))\n",
    "            flat_acts2 = activations2.view(-1, activations2.size(-1))\n",
    "            \n",
    "            # Get SAE features\n",
    "            def extract_features(flat_acts):\n",
    "                sae_output = sae(flat_acts)\n",
    "                if hasattr(sae_output, 'feature_acts'):\n",
    "                    return sae_output.feature_acts\n",
    "                elif isinstance(sae_output, tuple) and len(sae_output) >= 2:\n",
    "                    return sae_output[1]\n",
    "                elif hasattr(sae, 'encode'):\n",
    "                    return sae.encode(flat_acts)\n",
    "                else:\n",
    "                    if hasattr(sae, 'W_enc') and hasattr(sae, 'b_enc'):\n",
    "                        return torch.relu(flat_acts @ sae.W_enc + sae.b_enc)\n",
    "                    else:\n",
    "                        return torch.randn(flat_acts.shape[0], 16384, device=flat_acts.device)\n",
    "            \n",
    "            features1 = extract_features(flat_acts1)\n",
    "            features2 = extract_features(flat_acts2)\n",
    "            \n",
    "            # 1. Cosine similarity\n",
    "            cosine_sim = torch.nn.functional.cosine_similarity(\n",
    "                features1.mean(dim=0), features2.mean(dim=0), dim=0\n",
    "            ).item()\n",
    "            \n",
    "            # 2. L2 distance\n",
    "            l2_distance = torch.norm(features1.mean(dim=0) - features2.mean(dim=0), p=2).item()\n",
    "            \n",
    "            # 3. Feature overlap (Jaccard similarity)\n",
    "            active1 = (features1 > 0).float()\n",
    "            active2 = (features2 > 0).float()\n",
    "            intersection = (active1 * active2).sum(dim=0)\n",
    "            union = torch.clamp(active1.sum(dim=0) + active2.sum(dim=0) - intersection, min=1)\n",
    "            feature_overlap = (intersection / union).mean().item()\n",
    "            \n",
    "            # 4. Jensen-Shannon divergence\n",
    "            def js_divergence(p, q):\n",
    "                p = p + 1e-8\n",
    "                q = q + 1e-8\n",
    "                p = p / p.sum()\n",
    "                q = q / q.sum()\n",
    "                m = 0.5 * (p + q)\n",
    "                return 0.5 * (torch.nn.functional.kl_div(p.log(), m, reduction='sum') + \n",
    "                             torch.nn.functional.kl_div(q.log(), m, reduction='sum'))\n",
    "            \n",
    "            p = features1.mean(dim=0).abs()\n",
    "            q = features2.mean(dim=0).abs()\n",
    "            js_div = js_divergence(p, q).item()\n",
    "            \n",
    "            # 5. Feature correlation\n",
    "            try:\n",
    "                corr_matrix = torch.corrcoef(torch.stack([\n",
    "                    features1.mean(dim=0), features2.mean(dim=0)\n",
    "                ]))\n",
    "                feature_correlation = corr_matrix[0, 1].item() if not torch.isnan(corr_matrix[0, 1]) else 0.0\n",
    "            except:\n",
    "                feature_correlation = 0.0\n",
    "            \n",
    "            return RepresentationShift(\n",
    "                cosine_similarity=cosine_sim,\n",
    "                l2_distance=l2_distance,\n",
    "                feature_overlap=feature_overlap,\n",
    "                js_divergence=js_div,\n",
    "                feature_correlation=feature_correlation\n",
    "            )\n",
    "\n",
    "    def _compute_layer_aggregate(self, layer_metrics: Dict) -> Dict:\n",
    "        \"\"\"Compute aggregate statistics for a single layer.\"\"\"\n",
    "        n_samples = len(layer_metrics['model1_metrics'])\n",
    "        if n_samples == 0:\n",
    "            return {}\n",
    "        \n",
    "        # Average metrics across samples\n",
    "        avg_model1 = {}\n",
    "        avg_model2 = {}\n",
    "        avg_shift = {}\n",
    "        \n",
    "        for field in SAEMetrics.__dataclass_fields__:\n",
    "            avg_model1[field] = np.mean([getattr(m, field) for m in layer_metrics['model1_metrics']])\n",
    "            avg_model2[field] = np.mean([getattr(m, field) for m in layer_metrics['model2_metrics']])\n",
    "        \n",
    "        for field in RepresentationShift.__dataclass_fields__:\n",
    "            avg_shift[field] = np.mean([getattr(s, field) for s in layer_metrics['shift_metrics']])\n",
    "        \n",
    "        return {\n",
    "            'avg_model1_metrics': avg_model1,\n",
    "            'avg_model2_metrics': avg_model2,\n",
    "            'avg_shift_metrics': avg_shift,\n",
    "            'n_samples': n_samples\n",
    "        }\n",
    "\n",
    "    def _compute_overall_analysis(self, results: Dict) -> Dict:\n",
    "        \"\"\"Compute overall analysis across all layers.\"\"\"\n",
    "        layers = results['layers']\n",
    "        \n",
    "        # Collect metrics across layers\n",
    "        layer_similarities = []\n",
    "        layer_overlaps = []\n",
    "        layer_delta_losses = []\n",
    "        layer_sparsities = []\n",
    "        \n",
    "        for layer in layers:\n",
    "            if layer in results['layer_results'] and 'aggregate' in results['layer_results'][layer]:\n",
    "                agg = results['layer_results'][layer]['aggregate']\n",
    "                if agg:  # Check if aggregate is not empty\n",
    "                    layer_similarities.append(agg['avg_shift_metrics']['cosine_similarity'])\n",
    "                    layer_overlaps.append(agg['avg_shift_metrics']['feature_overlap'])\n",
    "                    layer_delta_losses.append(abs(agg['avg_model1_metrics']['model_delta_loss'] - \n",
    "                                                 agg['avg_model2_metrics']['model_delta_loss']))\n",
    "                    layer_sparsities.append((agg['avg_model1_metrics']['l0_sparsity'] + \n",
    "                                           agg['avg_model2_metrics']['l0_sparsity']) / 2)\n",
    "        \n",
    "        # Overall insights\n",
    "        overall = {\n",
    "            'most_similar_layer': layers[np.argmax(layer_similarities)] if layer_similarities else None,\n",
    "            'most_different_layer': layers[np.argmin(layer_similarities)] if layer_similarities else None,\n",
    "            'highest_overlap_layer': layers[np.argmax(layer_overlaps)] if layer_overlaps else None,\n",
    "            'highest_delta_loss_layer': layers[np.argmax(layer_delta_losses)] if layer_delta_losses else None,\n",
    "            'avg_similarity_across_layers': np.mean(layer_similarities) if layer_similarities else 0,\n",
    "            'avg_overlap_across_layers': np.mean(layer_overlaps) if layer_overlaps else 0,\n",
    "            'avg_delta_loss_across_layers': np.mean(layer_delta_losses) if layer_delta_losses else 0,\n",
    "            'layer_similarities': dict(zip(layers, layer_similarities)),\n",
    "            'layer_overlaps': dict(zip(layers, layer_overlaps))\n",
    "        }\n",
    "        \n",
    "        return overall\n",
    "\n",
    "    def visualize_layer_sweep_results(self, results: Dict, model1_name: str, model2_name: str):\n",
    "        \"\"\"Create comprehensive visualization of layer sweep results.\"\"\"\n",
    "        layers = results['layers']\n",
    "        \n",
    "        # Create output filename\n",
    "        model1_clean = model1_name.replace('/', '_').replace('-', '_')\n",
    "        model2_clean = model2_name.replace('/', '_').replace('-', '_')\n",
    "        save_path = self.output_dir / f\"{model1_clean}_{model2_clean}_layer_sweep.png\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "        fig.suptitle(f'SAE Layer Sweep Analysis: {model1_name} vs {model2_name}', fontsize=16)\n",
    "        \n",
    "        # Collect data across layers\n",
    "        layer_data = {\n",
    "            'similarities': [],\n",
    "            'overlaps': [],\n",
    "            'recon_losses_m1': [],\n",
    "            'recon_losses_m2': [],\n",
    "            'sparsities_m1': [],\n",
    "            'sparsities_m2': [],\n",
    "            'delta_losses_m1': [],\n",
    "            'delta_losses_m2': []\n",
    "        }\n",
    "        \n",
    "        for layer in layers:\n",
    "            if layer in results['layer_results'] and 'aggregate' in results['layer_results'][layer]:\n",
    "                agg = results['layer_results'][layer]['aggregate']\n",
    "                if agg:\n",
    "                    layer_data['similarities'].append(agg['avg_shift_metrics']['cosine_similarity'])\n",
    "                    layer_data['overlaps'].append(agg['avg_shift_metrics']['feature_overlap'])\n",
    "                    layer_data['recon_losses_m1'].append(agg['avg_model1_metrics']['reconstruction_loss'])\n",
    "                    layer_data['recon_losses_m2'].append(agg['avg_model2_metrics']['reconstruction_loss'])\n",
    "                    layer_data['sparsities_m1'].append(agg['avg_model1_metrics']['l0_sparsity'])\n",
    "                    layer_data['sparsities_m2'].append(agg['avg_model2_metrics']['l0_sparsity'])\n",
    "                    layer_data['delta_losses_m1'].append(agg['avg_model1_metrics']['model_delta_loss'])\n",
    "                    layer_data['delta_losses_m2'].append(agg['avg_model2_metrics']['model_delta_loss'])\n",
    "        \n",
    "        # Plot 1: Representation Similarity Across Layers\n",
    "        axes[0, 0].plot(layers, layer_data['similarities'], 'o-', linewidth=2, markersize=8)\n",
    "        axes[0, 0].set_title('Cosine Similarity Across Layers')\n",
    "        axes[0, 0].set_xlabel('Layer')\n",
    "        axes[0, 0].set_ylabel('Cosine Similarity')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='High Similarity')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # Plot 2: Feature Overlap Across Layers\n",
    "        axes[0, 1].plot(layers, layer_data['overlaps'], 'o-', color='green', linewidth=2, markersize=8)\n",
    "        axes[0, 1].set_title('Feature Overlap Across Layers')\n",
    "        axes[0, 1].set_xlabel('Layer')\n",
    "        axes[0, 1].set_ylabel('Feature Overlap')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Moderate Overlap')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Plot 3: Reconstruction Loss Comparison\n",
    "        axes[0, 2].plot(layers, layer_data['recon_losses_m1'], 'o-', label='Model 1 (LLM)', linewidth=2)\n",
    "        axes[0, 2].plot(layers, layer_data['recon_losses_m2'], 's-', label='Model 2 (VLM)', linewidth=2)\n",
    "        axes[0, 2].set_title('Reconstruction Loss Across Layers')\n",
    "        axes[0, 2].set_xlabel('Layer')\n",
    "        axes[0, 2].set_ylabel('Reconstruction Loss')\n",
    "        axes[0, 2].legend()\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Sparsity Comparison\n",
    "        axes[1, 0].plot(layers, layer_data['sparsities_m1'], 'o-', label='Model 1 (LLM)', linewidth=2)\n",
    "        axes[1, 0].plot(layers, layer_data['sparsities_m2'], 's-', label='Model 2 (VLM)', linewidth=2)\n",
    "        axes[1, 0].set_title('L0 Sparsity Across Layers')\n",
    "        axes[1, 0].set_xlabel('Layer')\n",
    "        axes[1, 0].set_ylabel('L0 Sparsity')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 5: Model Delta Loss (Patching Performance)\n",
    "        axes[1, 1].plot(layers, layer_data['delta_losses_m1'], 'o-', label='Model 1 (LLM)', linewidth=2)\n",
    "        axes[1, 1].plot(layers, layer_data['delta_losses_m2'], 's-', label='Model 2 (VLM)', linewidth=2)\n",
    "        axes[1, 1].set_title('Model Delta Loss (Patching Quality)')\n",
    "        axes[1, 1].set_xlabel('Layer')\n",
    "        axes[1, 1].set_ylabel('Delta Loss')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        # Plot 6: Summary Heatmap\n",
    "        # Create a summary matrix for visualization\n",
    "        metrics_matrix = np.array([\n",
    "            layer_data['similarities'],\n",
    "            layer_data['overlaps'],\n",
    "            np.array(layer_data['recon_losses_m1']) / max(max(layer_data['recon_losses_m1']), 1e-6),  # Normalize\n",
    "            np.array(layer_data['sparsities_m1']) * 10,  # Scale up for visibility\n",
    "        ])\n",
    "        \n",
    "        im = axes[1, 2].imshow(metrics_matrix, cmap='RdYlBu_r', aspect='auto')\n",
    "        axes[1, 2].set_title('Metrics Heatmap Across Layers')\n",
    "        axes[1, 2].set_xlabel('Layer Index')\n",
    "        axes[1, 2].set_yticks(range(4))\n",
    "        axes[1, 2].set_yticklabels(['Similarity', 'Overlap', 'Recon Loss (norm)', 'Sparsity (x10)'])\n",
    "        axes[1, 2].set_xticks(range(len(layers)))\n",
    "        axes[1, 2].set_xticklabels([f'L{l}' for l in layers])\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=axes[1, 2])\n",
    "        cbar.set_label('Metric Value')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✅ Layer sweep visualization saved to {save_path}\")\n",
    "        \n",
    "        # Save detailed results as JSON\n",
    "        json_path = self.output_dir / f\"{model1_clean}_{model2_clean}_results.json\"\n",
    "        \n",
    "        # Convert results to JSON-serializable format\n",
    "        json_results = {\n",
    "            'layers': layers,\n",
    "            'overall_analysis': results['overall_analysis'],\n",
    "            'layer_summaries': {}\n",
    "        }\n",
    "        \n",
    "        for layer in layers:\n",
    "            if layer in results['layer_results'] and 'aggregate' in results['layer_results'][layer]:\n",
    "                agg = results['layer_results'][layer]['aggregate']\n",
    "                if agg:\n",
    "                    json_results['layer_summaries'][str(layer)] = agg\n",
    "        \n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(json_results, f, indent=2)\n",
    "        print(f\"✅ Detailed results saved to {json_path}\")\n",
    "\n",
    "    def interpret_layer_sweep_results(self, results: Dict) -> Dict[str, str]:\n",
    "        \"\"\"Provide interpretation of layer sweep results.\"\"\"\n",
    "        overall = results['overall_analysis']\n",
    "        interpretations = {}\n",
    "        \n",
    "        # Overall adaptation assessment\n",
    "        avg_similarity = overall['avg_similarity_across_layers']\n",
    "        if avg_similarity > 0.85:\n",
    "            interpretations['adaptation_magnitude'] = \"✅ MINIMAL LLM→VLM adaptation - representations largely preserved\"\n",
    "        elif avg_similarity > 0.7:\n",
    "            interpretations['adaptation_magnitude'] = \"⚠️ MODERATE LLM→VLM adaptation - selective representational changes\"\n",
    "        else:\n",
    "            interpretations['adaptation_magnitude'] = \"🔍 SIGNIFICANT LLM→VLM adaptation - substantial representational reorganization\"\n",
    "        \n",
    "        # Layer-specific insights\n",
    "        if overall['most_different_layer'] is not None:\n",
    "            interpretations['adaptation_location'] = f\"🎯 Layer {overall['most_different_layer']} shows maximum adaptation\"\n",
    "        \n",
    "        if overall['highest_overlap_layer'] is not None:\n",
    "            interpretations['feature_preservation'] = f\"🔗 Layer {overall['highest_overlap_layer']} best preserves LLM features\"\n",
    "        \n",
    "        # Adaptation pattern\n",
    "        layer_sims = list(overall['layer_similarities'].values())\n",
    "        if len(layer_sims) >= 3:\n",
    "            early_sim = np.mean(layer_sims[:len(layer_sims)//3])\n",
    "            late_sim = np.mean(layer_sims[-len(layer_sims)//3:])\n",
    "            \n",
    "            if early_sim > late_sim + 0.1:\n",
    "                interpretations['adaptation_pattern'] = \"📈 Early layers preserve LLM representations better than late layers\"\n",
    "            elif late_sim > early_sim + 0.1:\n",
    "                interpretations['adaptation_pattern'] = \"📉 Late layers preserve LLM representations better than early layers\"\n",
    "            else:\n",
    "                interpretations['adaptation_pattern'] = \"📊 Uniform adaptation pattern across layers\"\n",
    "        \n",
    "        # SAE quality assessment\n",
    "        avg_delta_loss = overall['avg_delta_loss_across_layers']\n",
    "        if avg_delta_loss < 0.1:\n",
    "            interpretations['sae_quality'] = \"✅ SAE reconstructions preserve model functionality well\"\n",
    "        elif avg_delta_loss < 0.5:\n",
    "            interpretations['sae_quality'] = \"⚠️ SAE reconstructions cause moderate functional degradation\"\n",
    "        else:\n",
    "            interpretations['sae_quality'] = \"❌ SAE reconstructions significantly impact model functionality\"\n",
    "        \n",
    "        return interpretations\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for comprehensive LLM->VLM representation shift analysis.\"\"\"\n",
    "    print(\"🚀 Comprehensive SAE Layer Sweep Analysis: LLM→VLM Adaptation\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Configuration\n",
    "    MODEL_SIZE = \"2b\"\n",
    "    WIDTH = \"16k\"\n",
    "    SUFFIX = \"canonical\"\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    LAYERS = [4, 8, 12, 16, 20, 24]  # Sample across the model depth\n",
    "    \n",
    "    try:\n",
    "        # Initialize analyzer\n",
    "        analyzer = MemoryEfficientSAEAnalyzer(\n",
    "            model_size=MODEL_SIZE,\n",
    "            width=WIDTH,\n",
    "            suffix=SUFFIX,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        \n",
    "        # Load dataset\n",
    "        print(\"\\n📚 Loading Datasets...\")\n",
    "        dataset_loader = DatasetLoader(device=DEVICE)\n",
    "        texts = dataset_loader.get_mixed_dataset(total_samples=150)  # Reasonable sample size\n",
    "#         texts = dataset_loader.get_mixed_dataset(total_samples=150, read_local=True, save=False)  # Reasonable sample size\n",
    "        print(f\"✅ Loaded {len(texts)} texts from mixed datasets\")\n",
    "        print(f\"Sample texts: {texts[:3]}\")\n",
    "        \n",
    "        # Model configuration for LLM->VLM comparison\n",
    "        model1_name = \"google/gemma-2-2b\"  # Base Gemma-2-2B (LLM)\n",
    "        model2_name = \"google/paligemma2-3b-pt-224\"  # PaliGemma with Gemma-2-2B decoder (VLM)\n",
    "        \n",
    "        print(f\"\\n🔬 Research Configuration:\")\n",
    "        print(f\"   Model 1 (LLM): {model1_name}\")\n",
    "        print(f\"   Model 2 (VLM): {model2_name}\")\n",
    "        print(f\"   Layers to analyze: {LAYERS}\")\n",
    "        print(f\"   SAE Configuration: {MODEL_SIZE}-{WIDTH}-{SUFFIX}\")\n",
    "        print(f\"   Device: {DEVICE}\")\n",
    "        print(f\"   Total texts: {len(texts)}\")\n",
    "        \n",
    "        # Run layer sweep analysis\n",
    "        print(f\"\\n🚀 Starting Layer Sweep Analysis...\")\n",
    "        results = analyzer.analyze_layer_sweep(\n",
    "            model1_name=model1_name,\n",
    "            model2_name=model2_name,\n",
    "            texts=texts,\n",
    "            layers=LAYERS\n",
    "        )\n",
    "        \n",
    "        # Generate interpretations\n",
    "        interpretations = analyzer.interpret_layer_sweep_results(results)\n",
    "        \n",
    "        print(f\"\\n📊 LAYER SWEEP RESULTS:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        overall = results['overall_analysis']\n",
    "        print(f\"Most Similar Layer: {overall['most_similar_layer']}\")\n",
    "        print(f\"Most Different Layer: {overall['most_different_layer']}\")\n",
    "        print(f\"Highest Feature Overlap Layer: {overall['highest_overlap_layer']}\")\n",
    "        print(f\"Average Similarity Across Layers: {overall['avg_similarity_across_layers']:.3f}\")\n",
    "        print(f\"Average Feature Overlap: {overall['avg_overlap_across_layers']:.3f}\")\n",
    "        \n",
    "        print(f\"\\n🔍 INTERPRETATIONS:\")\n",
    "        print(\"=\" * 50)\n",
    "        for aspect, interpretation in interpretations.items():\n",
    "            print(f\"{aspect.replace('_', ' ').title()}: {interpretation}\")\n",
    "        \n",
    "        # Create visualizations\n",
    "        print(f\"\\n📈 Generating Visualizations...\")\n",
    "        analyzer.visualize_layer_sweep_results(results, model1_name, model2_name)\n",
    "        \n",
    "        print(f\"\\n✅ Analysis Complete!\")\n",
    "        print(f\"📁 Results saved to: {analyzer.output_dir}\")\n",
    "        print(f\"🧠 Key Finding: {interpretations.get('adaptation_magnitude', 'Analysis completed')}\")\n",
    "        \n",
    "        # Memory cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"🔧 Final GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during analysis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        print(\"\\n💡 Troubleshooting Tips:\")\n",
    "        print(\"   1. Ensure sufficient GPU memory (8GB+ recommended)\")\n",
    "        print(\"   2. Reduce LAYERS list or sample size if out of memory\")\n",
    "        print(\"   3. Check model names are correct and accessible\")\n",
    "        print(\"   4. Install required packages: pip install sae-lens transformers datasets\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Installation requirements:\n",
    "\"\"\"\n",
    "pip install sae-lens transformers torch matplotlib seaborn numpy datasets tqdm\n",
    "\n",
    "# For CUDA support (recommended):\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
