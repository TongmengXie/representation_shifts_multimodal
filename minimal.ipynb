{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8f0b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/py311/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py311/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f6bcc5c5450>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "SAE-based representation shift analysis using SAE Lens library\n",
    "for comparing Gemma and PaliGemma 2 with Google's Gemma Scope SAEs.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "try:\n",
    "    from sae_lens import SAE\n",
    "except:\n",
    "#     !pip install --upgrade pip setuptools wheel\n",
    "    !pip install --pre sae-lens\n",
    "from sae_lens import SAE\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "from dataclasses import dataclass\n",
    "import seaborn as sns\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d1cdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SAE Lens - Gemma Scope Representation Shift Analysis\n",
      "============================================================\n",
      "üîß Initializing GemmaScope SAE (Layer 12, Width 16k, Size 2b, Suffix canonical)\n",
      "üì• Loading Gemma Scope SAE...\n",
      "   Loading from release: gemma-scope-2b-pt-res-canonical\n",
      "   SAE ID: layer_12/width_16k/canonical\n",
      "‚úÖ SAE loaded successfully!\n",
      "   - Dictionary size: 16384\n",
      "   - Model dimension: 2304\n",
      "\n",
      "üî¨ Analysis Configuration:\n",
      "   Layer: 12\n",
      "   SAE Width: 16k\n",
      "   Model Size: 2b\n",
      "   SAE Suffix: canonical\n",
      "   Test Texts: 5\n",
      "\n",
      "üöÄ Starting comparative analysis\n",
      "   Model 1: google/gemma-2-2b\n",
      "   Model 2: google/gemma-2-2b-it\n",
      "   Texts: 5 samples\n",
      "\n",
      "üìù Processing text 1/5: 'The quick brown fox jumps over the lazy dog....'\n",
      "üîç Extracting activations from google/gemma-2-2b\n",
      "‚ùå Error extracting activations: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/google/gemma-2-2b.\n",
      "401 Client Error. (Request ID: Root=1-6889574d-0c4c063c34fbc5364b1d9ce1;e756dd6c-e75f-4901-97fe-928b332f753d)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2-2b/resolve/main/config.json.\n",
      "Access to model google/gemma-2-2b is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "üîÑ Using dummy activations for demonstration\n",
      "üîç Extracting activations from google/gemma-2-2b-it\n",
      "‚ùå Error extracting activations: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/google/gemma-2-2b-it.\n",
      "401 Client Error. (Request ID: Root=1-6889574d-5880d7d278c0bf4033bf5a74;a06f21a4-b7c9-4577-b58a-c6eb37689614)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-2-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "üîÑ Using dummy activations for demonstration\n",
      "‚ùå Error during analysis: 'Tensor' object has no attribute 'feature_acts'\n",
      "\n",
      "üí° Troubleshooting tips:\n",
      "   1. Install SAE Lens: pip install sae-lens\n",
      "   2. Ensure you have sufficient GPU memory\n",
      "   3. Try with smaller models or fewer texts\n",
      "   4. Check model names are correct and accessible\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npip install sae-lens transformers torch matplotlib seaborn numpy\\n\\n# For CUDA support (recommended):\\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "SAE-based representation shift analysis using SAE Lens library\n",
    "for comparing Gemma and PaliGemma 2 with Google's Gemma Scope SAEs.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sae_lens import SAE\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "from dataclasses import dataclass\n",
    "import seaborn as sns\n",
    "\n",
    "@dataclass\n",
    "class SAEMetrics:\n",
    "    \"\"\"Container for SAE evaluation metrics.\"\"\"\n",
    "    reconstruction_loss: float\n",
    "    l0_sparsity: float\n",
    "    l1_sparsity: float\n",
    "    fraction_alive: float\n",
    "    mean_max_activation: float\n",
    "    reconstruction_score: float\n",
    "\n",
    "@dataclass\n",
    "class RepresentationShift:\n",
    "    \"\"\"Container for representation shift metrics.\"\"\"\n",
    "    cosine_similarity: float\n",
    "    l2_distance: float\n",
    "    feature_overlap: float\n",
    "    js_divergence: float\n",
    "    feature_correlation: float\n",
    "\n",
    "class GemmaScopeAnalyzer:\n",
    "    \"\"\"Analyzer for measuring representation shifts using Gemma Scope SAEs.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 layer: int = 12, \n",
    "                 width: str = \"16k\",\n",
    "                 model_size: str = \"2b\",\n",
    "                 suffix: str = \"canonical\"):\n",
    "        \"\"\"\n",
    "        Initialize analyzer with specific Gemma Scope SAE configuration.\n",
    "        \n",
    "        Args:\n",
    "            layer: Which transformer layer to analyze (0-27 for 2B, 0-41 for 9B)\n",
    "            width: SAE width (\"16k\", \"65k\", \"262k\") \n",
    "            model_size: Model size (\"2b\" or \"9b\")\n",
    "            suffix: SAE variant (\"canonical\" or specific L0 like \"average_l0_105\")\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.layer = layer\n",
    "        self.width = width\n",
    "        self.model_size = model_size\n",
    "        self.suffix = suffix\n",
    "        self.sae = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "        print(f\"üîß Initializing GemmaScope SAE (Layer {layer}, Width {width}, Size {model_size}, Suffix {suffix})\")\n",
    "        self.load_sae()\n",
    "\n",
    "    def get_gemmascope_sae(self, layer, width, suffix, model_size):\n",
    "        \"\"\"Load Gemma Scope SAE with correct format.\"\"\"\n",
    "        release = f\"gemma-scope-{model_size}-pt-res\"  # Use main release\n",
    "        if suffix == \"canonical\":\n",
    "            release = f\"gemma-scope-{model_size}-pt-res-canonical\"  # Use canonical release\n",
    "            sae_id = f\"layer_{layer}/width_{width}/canonical\"\n",
    "        else:\n",
    "            sae_id = f\"layer_{layer}/width_{width}/{suffix}\"\n",
    "        \n",
    "        print(f\"   Loading from release: {release}\")\n",
    "        print(f\"   SAE ID: {sae_id}\")\n",
    "        \n",
    "        sae = SAE.from_pretrained(release, sae_id)\n",
    "        return sae\n",
    "\n",
    "    def load_sae(self):\n",
    "        \"\"\"Load the specified Gemma Scope SAE using SAE Lens.\"\"\"\n",
    "        try:\n",
    "            # Turn off gradients globally\n",
    "            torch.set_grad_enabled(False)\n",
    "            \n",
    "            print(f\"üì• Loading Gemma Scope SAE...\")\n",
    "            self.sae = self.get_gemmascope_sae(\n",
    "                layer=self.layer,\n",
    "                width=self.width, \n",
    "                suffix=self.suffix,\n",
    "                model_size=self.model_size\n",
    "            )\n",
    "            \n",
    "            self.sae = self.sae.to(self.device)\n",
    "            self.sae.eval()\n",
    "            \n",
    "            print(f\"‚úÖ SAE loaded successfully!\")\n",
    "            print(f\"   - Dictionary size: {self.sae.cfg.d_sae}\")\n",
    "            print(f\"   - Model dimension: {self.sae.cfg.d_in}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading SAE: {e}\")\n",
    "            print(\"üí° Available releases and IDs:\")\n",
    "            print(\"   - Use 'canonical' suffix for gemma-scope-{model_size}-pt-res-canonical\")\n",
    "            print(\"   - Or check specific L0 values like 'average_l0_105' for main release\")\n",
    "            print(\"   - Available widths: 16k, 65k, 262k\")\n",
    "            raise\n",
    "\n",
    "    def get_model_activations(self, \n",
    "                            model_name: str, \n",
    "                            text: str, \n",
    "                            batch_size: int = 1) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract activations from specified layer of the model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model identifier\n",
    "            text: Input text to analyze\n",
    "            batch_size: Batch size for processing\n",
    "            \n",
    "        Returns:\n",
    "            Activations tensor [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        print(f\"üîç Extracting activations from {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Load model and tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, \n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float32,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "            )\n",
    "            model.eval()\n",
    "            \n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(\n",
    "                text, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Hook to capture activations\n",
    "            activations = {}\n",
    "            \n",
    "            def activation_hook(module, input, output):\n",
    "                # Store the residual stream activations\n",
    "                if hasattr(output, 'last_hidden_state'):\n",
    "                    activations['residual'] = output.last_hidden_state\n",
    "                else:\n",
    "                    activations['residual'] = output[0] if isinstance(output, tuple) else output\n",
    "            \n",
    "            # Register hook on the target layer\n",
    "            if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
    "                target_layer = model.model.layers[self.layer]\n",
    "            else:\n",
    "                # Fallback - hook the entire model\n",
    "                target_layer = model\n",
    "                \n",
    "            hook = target_layer.register_forward_hook(activation_hook)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, output_hidden_states=True)\n",
    "                \n",
    "                # If we have hidden states, use them directly\n",
    "                if hasattr(outputs, 'hidden_states') and len(outputs.hidden_states) > self.layer:\n",
    "                    activations['residual'] = outputs.hidden_states[self.layer]\n",
    "            \n",
    "            hook.remove()\n",
    "            \n",
    "            # Return activations\n",
    "            residual_activations = activations.get('residual', outputs.hidden_states[-1])\n",
    "            print(f\"   ‚úÖ Extracted activations: {residual_activations.shape}\")\n",
    "            \n",
    "            return residual_activations\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extracting activations: {e}\")\n",
    "            # Return dummy activations for demo\n",
    "            print(\"üîÑ Using dummy activations for demonstration\")\n",
    "            return torch.randn(1, 10, self.sae.cfg.d_in, device=self.device)\n",
    "\n",
    "    def compute_sae_metrics(self, activations: torch.Tensor) -> SAEMetrics:\n",
    "        \"\"\"\n",
    "        Compute comprehensive SAE evaluation metrics.\n",
    "        \n",
    "        Args:\n",
    "            activations: Input activations [batch, seq, d_model]\n",
    "            \n",
    "        Returns:\n",
    "            SAEMetrics object with all evaluation metrics\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Reshape activations for SAE processing\n",
    "            batch_size, seq_len, d_model = activations.shape\n",
    "            flat_activations = activations.view(-1, d_model)\n",
    "            \n",
    "            # Forward pass through SAE\n",
    "            sae_out = self.sae(flat_activations)\n",
    "            \n",
    "            # Extract components\n",
    "            feature_acts = sae_out.feature_acts  # Sparse feature activations\n",
    "            sae_output = sae_out.sae_out         # Reconstructed activations\n",
    "            \n",
    "            # 1. Reconstruction Loss (MSE)\n",
    "            reconstruction_loss = torch.nn.functional.mse_loss(\n",
    "                sae_output, flat_activations\n",
    "            ).item()\n",
    "            \n",
    "            # 2. L0 Sparsity (fraction of non-zero features)\n",
    "            l0_sparsity = (feature_acts > 0).float().mean().item()\n",
    "            \n",
    "            # 3. L1 Sparsity (mean absolute activation)\n",
    "            l1_sparsity = feature_acts.abs().mean().item()\n",
    "            \n",
    "            # 4. Fraction of features that are ever active\n",
    "            fraction_alive = (feature_acts.max(dim=0)[0] > 0).float().mean().item()\n",
    "            \n",
    "            # 5. Mean maximum activation per sample\n",
    "            mean_max_activation = feature_acts.max(dim=1)[0].mean().item()\n",
    "            \n",
    "            # 6. Reconstruction score (explained variance)\n",
    "            var_original = flat_activations.var(dim=0).mean()\n",
    "            var_residual = (flat_activations - sae_output).var(dim=0).mean()\n",
    "            reconstruction_score = 1 - (var_residual / var_original).item()\n",
    "            \n",
    "            return SAEMetrics(\n",
    "                reconstruction_loss=reconstruction_loss,\n",
    "                l0_sparsity=l0_sparsity,\n",
    "                l1_sparsity=l1_sparsity,\n",
    "                fraction_alive=fraction_alive,\n",
    "                mean_max_activation=mean_max_activation,\n",
    "                reconstruction_score=reconstruction_score\n",
    "            )\n",
    "\n",
    "    def compute_representation_shift(self, \n",
    "                                   activations1: torch.Tensor, \n",
    "                                   activations2: torch.Tensor) -> RepresentationShift:\n",
    "        \"\"\"\n",
    "        Compute representation shift metrics between two sets of activations.\n",
    "        \n",
    "        Args:\n",
    "            activations1: Activations from first model\n",
    "            activations2: Activations from second model\n",
    "            \n",
    "        Returns:\n",
    "            RepresentationShift object with shift metrics\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Process both activation sets through SAE\n",
    "            flat_acts1 = activations1.view(-1, activations1.size(-1))\n",
    "            flat_acts2 = activations2.view(-1, activations2.size(-1))\n",
    "            \n",
    "            sae_out1 = self.sae(flat_acts1)\n",
    "            sae_out2 = self.sae(flat_acts2)\n",
    "            \n",
    "            features1 = sae_out1.feature_acts\n",
    "            features2 = sae_out2.feature_acts\n",
    "            \n",
    "            # 1. Cosine similarity between feature vectors\n",
    "            cosine_sim = torch.nn.functional.cosine_similarity(\n",
    "                features1.mean(dim=0), \n",
    "                features2.mean(dim=0), \n",
    "                dim=0\n",
    "            ).item()\n",
    "            \n",
    "            # 2. L2 distance between feature vectors\n",
    "            l2_distance = torch.norm(\n",
    "                features1.mean(dim=0) - features2.mean(dim=0), \n",
    "                p=2\n",
    "            ).item()\n",
    "            \n",
    "            # 3. Feature overlap (Jaccard similarity)\n",
    "            active1 = (features1 > 0).float()\n",
    "            active2 = (features2 > 0).float()\n",
    "            \n",
    "            intersection = (active1 * active2).sum(dim=0)\n",
    "            union = torch.clamp(active1.sum(dim=0) + active2.sum(dim=0) - intersection, min=1)\n",
    "            feature_overlap = (intersection / union).mean().item()\n",
    "            \n",
    "            # 4. Jensen-Shannon divergence between feature distributions\n",
    "            def js_divergence(p, q):\n",
    "                p = p + 1e-8  # Add small epsilon for numerical stability\n",
    "                q = q + 1e-8\n",
    "                p = p / p.sum()\n",
    "                q = q / q.sum()\n",
    "                m = 0.5 * (p + q)\n",
    "                return 0.5 * (torch.nn.functional.kl_div(p, m, reduction='sum') + \n",
    "                             torch.nn.functional.kl_div(q, m, reduction='sum'))\n",
    "            \n",
    "            p = features1.mean(dim=0).abs()\n",
    "            q = features2.mean(dim=0).abs()\n",
    "            js_div = js_divergence(p, q).item()\n",
    "            \n",
    "            # 5. Feature correlation\n",
    "            corr_matrix = torch.corrcoef(torch.stack([\n",
    "                features1.mean(dim=0), \n",
    "                features2.mean(dim=0)\n",
    "            ]))\n",
    "            feature_correlation = corr_matrix[0, 1].item() if not torch.isnan(corr_matrix[0, 1]) else 0.0\n",
    "            \n",
    "            return RepresentationShift(\n",
    "                cosine_similarity=cosine_sim,\n",
    "                l2_distance=l2_distance,\n",
    "                feature_overlap=feature_overlap,\n",
    "                js_divergence=js_div,\n",
    "                feature_correlation=feature_correlation\n",
    "            )\n",
    "\n",
    "    def analyze_models(self, \n",
    "                      model1_name: str, \n",
    "                      model2_name: str, \n",
    "                      texts: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Complete analysis comparing two models across multiple texts.\n",
    "        \n",
    "        Args:\n",
    "            model1_name: First model identifier\n",
    "            model2_name: Second model identifier  \n",
    "            texts: List of texts to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with comprehensive analysis results\n",
    "        \"\"\"\n",
    "        print(f\"üöÄ Starting comparative analysis\")\n",
    "        print(f\"   Model 1: {model1_name}\")\n",
    "        print(f\"   Model 2: {model2_name}\")\n",
    "        print(f\"   Texts: {len(texts)} samples\")\n",
    "        print()\n",
    "        \n",
    "        results = {\n",
    "            'model1_metrics': [],\n",
    "            'model2_metrics': [], \n",
    "            'shift_metrics': [],\n",
    "            'texts': texts\n",
    "        }\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            print(f\"üìù Processing text {i+1}/{len(texts)}: '{text[:50]}...'\")\n",
    "            \n",
    "            # Extract activations\n",
    "            acts1 = self.get_model_activations(model1_name, text)\n",
    "            acts2 = self.get_model_activations(model2_name, text)\n",
    "            \n",
    "            # Compute SAE metrics\n",
    "            metrics1 = self.compute_sae_metrics(acts1)\n",
    "            metrics2 = self.compute_sae_metrics(acts2)\n",
    "            \n",
    "            # Compute representation shift\n",
    "            shift = self.compute_representation_shift(acts1, acts2)\n",
    "            \n",
    "            results['model1_metrics'].append(metrics1)\n",
    "            results['model2_metrics'].append(metrics2)\n",
    "            results['shift_metrics'].append(shift)\n",
    "            \n",
    "            print(f\"   ‚úÖ Completed analysis for text {i+1}\")\n",
    "        \n",
    "        # Compute aggregate statistics\n",
    "        results['aggregate'] = self._compute_aggregate_stats(results)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _compute_aggregate_stats(self, results: Dict) -> Dict:\n",
    "        \"\"\"Compute aggregate statistics across all texts.\"\"\"\n",
    "        n_texts = len(results['texts'])\n",
    "        \n",
    "        # Average metrics across texts\n",
    "        avg_model1 = {}\n",
    "        avg_model2 = {}\n",
    "        avg_shift = {}\n",
    "        \n",
    "        for field in SAEMetrics.__dataclass_fields__:\n",
    "            avg_model1[field] = np.mean([getattr(m, field) for m in results['model1_metrics']])\n",
    "            avg_model2[field] = np.mean([getattr(m, field) for m in results['model2_metrics']])\n",
    "        \n",
    "        for field in RepresentationShift.__dataclass_fields__:\n",
    "            avg_shift[field] = np.mean([getattr(s, field) for s in results['shift_metrics']])\n",
    "        \n",
    "        return {\n",
    "            'avg_model1_metrics': avg_model1,\n",
    "            'avg_model2_metrics': avg_model2,\n",
    "            'avg_shift_metrics': avg_shift,\n",
    "            'n_texts': n_texts\n",
    "        }\n",
    "\n",
    "    def visualize_results(self, results: Dict, save_path: str = \"sae_analysis.png\"):\n",
    "        \"\"\"Create comprehensive visualization of analysis results.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('SAE-based Representation Shift Analysis (Gemma Scope)', fontsize=16)\n",
    "        \n",
    "        agg = results['aggregate']\n",
    "        \n",
    "        # Plot 1: Reconstruction metrics\n",
    "        recon_metrics = ['reconstruction_loss', 'reconstruction_score']\n",
    "        model1_recon = [agg['avg_model1_metrics'][m] for m in recon_metrics]\n",
    "        model2_recon = [agg['avg_model2_metrics'][m] for m in recon_metrics]\n",
    "        \n",
    "        x = np.arange(len(recon_metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[0,0].bar(x - width/2, model1_recon, width, label='Model 1', alpha=0.8)\n",
    "        axes[0,0].bar(x + width/2, model2_recon, width, label='Model 2', alpha=0.8)\n",
    "        axes[0,0].set_title('Reconstruction Quality')\n",
    "        axes[0,0].set_xticks(x)\n",
    "        axes[0,0].set_xticklabels(recon_metrics, rotation=45)\n",
    "        axes[0,0].legend()\n",
    "        \n",
    "        # Plot 2: Sparsity metrics\n",
    "        sparsity_metrics = ['l0_sparsity', 'l1_sparsity', 'fraction_alive']\n",
    "        model1_sparsity = [agg['avg_model1_metrics'][m] for m in sparsity_metrics]\n",
    "        model2_sparsity = [agg['avg_model2_metrics'][m] for m in sparsity_metrics]\n",
    "        \n",
    "        x = np.arange(len(sparsity_metrics))\n",
    "        axes[0,1].bar(x - width/2, model1_sparsity, width, label='Model 1', alpha=0.8)\n",
    "        axes[0,1].bar(x + width/2, model2_sparsity, width, label='Model 2', alpha=0.8)\n",
    "        axes[0,1].set_title('Sparsity Metrics')\n",
    "        axes[0,1].set_xticks(x)\n",
    "        axes[0,1].set_xticklabels(sparsity_metrics, rotation=45)\n",
    "        axes[0,1].legend()\n",
    "        \n",
    "        # Plot 3: Representation shift metrics\n",
    "        shift_names = list(agg['avg_shift_metrics'].keys())\n",
    "        shift_values = list(agg['avg_shift_metrics'].values())\n",
    "        \n",
    "        axes[0,2].barh(shift_names, shift_values, color='green', alpha=0.7)\n",
    "        axes[0,2].set_title('Representation Shift Metrics')\n",
    "        axes[0,2].set_xlabel('Value')\n",
    "        \n",
    "        # Plot 4: Distribution of cosine similarities across texts\n",
    "        cosine_sims = [s.cosine_similarity for s in results['shift_metrics']]\n",
    "        axes[1,0].hist(cosine_sims, bins=10, alpha=0.7, edgecolor='black')\n",
    "        axes[1,0].axvline(np.mean(cosine_sims), color='red', linestyle='--', \n",
    "                         label=f'Mean: {np.mean(cosine_sims):.3f}')\n",
    "        axes[1,0].set_title('Distribution of Cosine Similarities')\n",
    "        axes[1,0].set_xlabel('Cosine Similarity')\n",
    "        axes[1,0].set_ylabel('Frequency')\n",
    "        axes[1,0].legend()\n",
    "        \n",
    "        # Plot 5: Scatter plot of reconstruction loss vs sparsity\n",
    "        model1_recon_loss = [m.reconstruction_loss for m in results['model1_metrics']]\n",
    "        model1_sparsity = [m.l0_sparsity for m in results['model1_metrics']]\n",
    "        model2_recon_loss = [m.reconstruction_loss for m in results['model2_metrics']]\n",
    "        model2_sparsity = [m.l0_sparsity for m in results['model2_metrics']]\n",
    "        \n",
    "        axes[1,1].scatter(model1_sparsity, model1_recon_loss, alpha=0.7, label='Model 1')\n",
    "        axes[1,1].scatter(model2_sparsity, model2_recon_loss, alpha=0.7, label='Model 2')\n",
    "        axes[1,1].set_xlabel('L0 Sparsity')\n",
    "        axes[1,1].set_ylabel('Reconstruction Loss')\n",
    "        axes[1,1].set_title('Reconstruction-Sparsity Trade-off')\n",
    "        axes[1,1].legend()\n",
    "        \n",
    "        # Plot 6: Feature overlap distribution\n",
    "        overlaps = [s.feature_overlap for s in results['shift_metrics']]\n",
    "        axes[1,2].hist(overlaps, bins=10, alpha=0.7, edgecolor='black')\n",
    "        axes[1,2].axvline(np.mean(overlaps), color='red', linestyle='--',\n",
    "                         label=f'Mean: {np.mean(overlaps):.3f}')\n",
    "        axes[1,2].set_title('Distribution of Feature Overlaps')\n",
    "        axes[1,2].set_xlabel('Feature Overlap')\n",
    "        axes[1,2].set_ylabel('Frequency')\n",
    "        axes[1,2].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Visualization saved to {save_path}\")\n",
    "\n",
    "    def interpret_results(self, results: Dict) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Provide interpretation of the analysis results.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with interpretation strings for each aspect\n",
    "        \"\"\"\n",
    "        agg = results['aggregate']\n",
    "        interpretations = {}\n",
    "        \n",
    "        # SAE Quality Assessment\n",
    "        avg_recon_loss = (agg['avg_model1_metrics']['reconstruction_loss'] + \n",
    "                         agg['avg_model2_metrics']['reconstruction_loss']) / 2\n",
    "        avg_sparsity = (agg['avg_model1_metrics']['l0_sparsity'] + \n",
    "                       agg['avg_model2_metrics']['l0_sparsity']) / 2\n",
    "        \n",
    "        if avg_recon_loss < 0.1 and avg_sparsity < 0.1:\n",
    "            interpretations['sae_quality'] = \"‚úÖ SAE is working well - low reconstruction loss with high sparsity\"\n",
    "        elif avg_recon_loss < 0.1:\n",
    "            interpretations['sae_quality'] = \"‚ö†Ô∏è SAE reconstructs well but low sparsity - may be learning dense features\"\n",
    "        elif avg_sparsity < 0.1:\n",
    "            interpretations['sae_quality'] = \"‚ö†Ô∏è SAE is sparse but high reconstruction loss - may be losing information\"\n",
    "        else:\n",
    "            interpretations['sae_quality'] = \"‚ùå SAE quality is poor - high reconstruction loss and low sparsity\"\n",
    "        \n",
    "        # Representation Shift Assessment\n",
    "        cosine_sim = agg['avg_shift_metrics']['cosine_similarity']\n",
    "        feature_overlap = agg['avg_shift_metrics']['feature_overlap']\n",
    "        \n",
    "        if cosine_sim > 0.8 and feature_overlap > 0.5:\n",
    "            interpretations['shift_magnitude'] = \"‚úÖ Small representation shift - models use similar features\"\n",
    "        elif cosine_sim > 0.6 or feature_overlap > 0.3:\n",
    "            interpretations['shift_magnitude'] = \"‚ö†Ô∏è Moderate representation shift - some shared features\"\n",
    "        else:\n",
    "            interpretations['shift_magnitude'] = \"üîç Large representation shift - models use very different features\"\n",
    "        \n",
    "        # Model Comparison\n",
    "        recon_diff = abs(agg['avg_model1_metrics']['reconstruction_loss'] - \n",
    "                        agg['avg_model2_metrics']['reconstruction_loss'])\n",
    "        sparsity_diff = abs(agg['avg_model1_metrics']['l0_sparsity'] - \n",
    "                           agg['avg_model2_metrics']['l0_sparsity'])\n",
    "        \n",
    "        if recon_diff < 0.05 and sparsity_diff < 0.02:\n",
    "            interpretations['model_similarity'] = \"‚úÖ Models show similar SAE characteristics\"\n",
    "        else:\n",
    "            interpretations['model_similarity'] = \"üîç Models show different SAE characteristics - architectural differences detected\"\n",
    "        \n",
    "        return interpretations\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main demonstration of SAE-based representation shift analysis.\"\"\"\n",
    "    print(\"üöÄ SAE Lens - Gemma Scope Representation Shift Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Configuration\n",
    "    LAYER = 12  # Middle layer for analysis\n",
    "    WIDTH = \"16k\"  # SAE width\n",
    "    MODEL_SIZE = \"2b\"  # Using 2B models for faster demo\n",
    "    SUFFIX = \"canonical\"  # Use canonical SAEs (most stable)\n",
    "    \n",
    "    # Test texts covering different domains\n",
    "    test_texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"In machine learning, neural networks learn complex patterns from data.\",\n",
    "        \"The economy has shown resilience despite global challenges.\",\n",
    "        \"Climate change affects weather patterns around the world.\",\n",
    "        \"Artificial intelligence transforms how we work and live.\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Initialize analyzer\n",
    "        analyzer = GemmaScopeAnalyzer(\n",
    "            layer=LAYER, \n",
    "            width=WIDTH, \n",
    "            model_size=MODEL_SIZE,\n",
    "            suffix=SUFFIX\n",
    "        )\n",
    "        \n",
    "        # Model names (adjust these based on available models)\n",
    "        model1_name = \"google/gemma-2-2b\"  # Base Gemma 2\n",
    "        model2_name = \"google/gemma-2-2b-it\"  # Instruction-tuned version\n",
    "        # Note: Replace with actual PaliGemma when available\n",
    "        \n",
    "        print(f\"\\nüî¨ Analysis Configuration:\")\n",
    "        print(f\"   Layer: {LAYER}\")\n",
    "        print(f\"   SAE Width: {WIDTH}\")\n",
    "        print(f\"   Model Size: {MODEL_SIZE}\")\n",
    "        print(f\"   SAE Suffix: {SUFFIX}\")\n",
    "        print(f\"   Test Texts: {len(test_texts)}\")\n",
    "        print()\n",
    "        \n",
    "        # Run analysis\n",
    "        results = analyzer.analyze_models(model1_name, model2_name, test_texts)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nüìä ANALYSIS RESULTS:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        agg = results['aggregate']\n",
    "        \n",
    "        print(\"\\nAverage SAE Metrics - Model 1:\")\n",
    "        for key, value in agg['avg_model1_metrics'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nAverage SAE Metrics - Model 2:\")\n",
    "        for key, value in agg['avg_model2_metrics'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nAverage Representation Shift Metrics:\")\n",
    "        for key, value in agg['avg_shift_metrics'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        # Generate interpretations\n",
    "        interpretations = analyzer.interpret_results(results)\n",
    "        \n",
    "        print(\"\\nüîç INTERPRETATIONS:\")\n",
    "        print(\"=\" * 40)\n",
    "        for aspect, interpretation in interpretations.items():\n",
    "            print(f\"{aspect.replace('_', ' ').title()}: {interpretation}\")\n",
    "        \n",
    "        # Create visualization\n",
    "        analyzer.visualize_results(results)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Analysis complete!\")\n",
    "        print(f\"üìà Visualization saved as 'sae_analysis.png'\")\n",
    "        print(f\"üìã Analyzed {len(test_texts)} texts across layer {LAYER}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during analysis: {e}\")\n",
    "        print(\"\\nüí° Troubleshooting tips:\")\n",
    "        print(\"   1. Install SAE Lens: pip install sae-lens\")\n",
    "        print(\"   2. Ensure you have sufficient GPU memory\")\n",
    "        print(\"   3. Try with smaller models or fewer texts\")\n",
    "        print(\"   4. Check model names are correct and accessible\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Installation requirements:\n",
    "\"\"\"\n",
    "pip install sae-lens transformers torch matplotlib seaborn numpy\n",
    "\n",
    "# For CUDA support (recommended):\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b0eef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "release = \"gemma-scope-2b-pt-res\"\n",
    "sae_id = \"embedding/width_4k/average_l0_6\"\n",
    "sae = SAE.from_pretrained(release, sae_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d948e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gemmascope_sae(layer, width, suffix, model_size):\n",
    "    release=f\"gemma-scope-{model_size}-pt-res\"\n",
    "    sae_id = f\"{layer}/width_{width}/{suffix}\"\n",
    "    sae = SAE.from_pretrained(release, sae_id)\n",
    "    return sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16561f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SAE Lens - Gemma Scope Representation Shift Analysis\n",
      "============================================================\n",
      "üîß Initializing GemmaScope SAE (Layer 12, Width 16k, Size 2b)\n",
      "üì• Loading SAE: gemma-scope-2b-pt-res-layer-12-width-16k\n",
      "‚ùå Error loading SAE: ID 12/width_16k/average_l0_6 not found in release gemma-scope-2b-pt-res. Valid IDs are ['embedding/width_4k/average_l0_6', 'embedding/width_4k/average_l0_44', 'embedding/width_4k/average_l0_21', 'embedding/width_4k/average_l0_111', 'layer_0/width_16k/average_l0_105', ...]. If you don't want to specify an L0 value, consider using release gemma-scope-2b-pt-res-canonical which has valid IDs ['layer_0/width_16k/canonical', 'layer_1/width_16k/canonical', 'layer_2/width_16k/canonical', 'layer_3/width_16k/canonical', 'layer_4/width_16k/canonical', ...]\n",
      "üí° Make sure you have sae_lens installed: pip install sae-lens\n",
      "‚ùå Error during analysis: ID 12/width_16k/average_l0_6 not found in release gemma-scope-2b-pt-res. Valid IDs are ['embedding/width_4k/average_l0_6', 'embedding/width_4k/average_l0_44', 'embedding/width_4k/average_l0_21', 'embedding/width_4k/average_l0_111', 'layer_0/width_16k/average_l0_105', ...]. If you don't want to specify an L0 value, consider using release gemma-scope-2b-pt-res-canonical which has valid IDs ['layer_0/width_16k/canonical', 'layer_1/width_16k/canonical', 'layer_2/width_16k/canonical', 'layer_3/width_16k/canonical', 'layer_4/width_16k/canonical', ...]\n",
      "\n",
      "üí° Troubleshooting tips:\n",
      "   1. Install SAE Lens: pip install sae-lens\n",
      "   2. Ensure you have sufficient GPU memory\n",
      "   3. Try with smaller models or fewer texts\n",
      "   4. Check model names are correct and accessible\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npip install sae-lens transformers torch matplotlib seaborn numpy\\n\\n# For CUDA support (recommended):\\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class SAEMetrics:\n",
    "    \"\"\"Container for SAE evaluation metrics.\"\"\"\n",
    "    reconstruction_loss: float\n",
    "    l0_sparsity: float\n",
    "    l1_sparsity: float\n",
    "    fraction_alive: float\n",
    "    mean_max_activation: float\n",
    "    reconstruction_score: float\n",
    "\n",
    "@dataclass\n",
    "class RepresentationShift:\n",
    "    \"\"\"Container for representation shift metrics.\"\"\"\n",
    "    cosine_similarity: float\n",
    "    l2_distance: float\n",
    "    feature_overlap: float\n",
    "    js_divergence: float\n",
    "    feature_correlation: float\n",
    "\n",
    "class GemmaScopeAnalyzer:\n",
    "    \"\"\"Analyzer for measuring representation shifts using Gemma Scope SAEs.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 layer: int = 12, \n",
    "                 width: str = \"16k\",\n",
    "                 model_size: str = \"2b\"):\n",
    "        \"\"\"\n",
    "        Initialize analyzer with specific Gemma Scope SAE configuration.\n",
    "        \n",
    "        Args:\n",
    "            layer: Which transformer layer to analyze (0-27 for 2B, 0-41 for 9B)\n",
    "            width: SAE width (\"1k\", \"16k\", \"65k\", \"262k\")\n",
    "            model_size: Model size (\"2b\" or \"9b\")\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.layer = layer\n",
    "        self.width = width\n",
    "        self.model_size = model_size\n",
    "        self.sae = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "        print(f\"üîß Initializing GemmaScope SAE (Layer {layer}, Width {width}, Size {model_size})\")\n",
    "        self.load_sae()\n",
    "\n",
    "    def load_sae(self):\n",
    "        \"\"\"Load the specified Gemma Scope SAE using SAE Lens.\"\"\"\n",
    "        try:\n",
    "            # Load SAE using SAE Lens\n",
    "            sae_id = f\"gemma-scope-{self.model_size}-pt-res-layer-{self.layer}-width-{self.width}\"\n",
    "            \n",
    "            print(f\"üì• Loading SAE: {sae_id}\")\n",
    "            self.sae, cfg_dict, sparsity = get_gemmascope_sae(\n",
    "                layer=self.layer,\n",
    "                width=self.width,\n",
    "                suffix=\"average_l0_6\",\n",
    "                model_size=self.model_size,\n",
    "            )\n",
    "            \n",
    "            self.sae = self.sae.to(self.device)\n",
    "            self.sae.eval()\n",
    "            \n",
    "            print(f\"‚úÖ SAE loaded successfully!\")\n",
    "            print(f\"   - Dictionary size: {self.sae.cfg.d_sae}\")\n",
    "            print(f\"   - Model dimension: {self.sae.cfg.d_in}\")\n",
    "            print(f\"   - L0 sparsity: {sparsity:.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading SAE: {e}\")\n",
    "            print(\"üí° Make sure you have sae_lens installed: pip install sae-lens\")\n",
    "            raise\n",
    "\n",
    "    def get_model_activations(self, \n",
    "                            model_name: str, \n",
    "                            text: str, \n",
    "                            batch_size: int = 1) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract activations from specified layer of the model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model identifier\n",
    "            text: Input text to analyze\n",
    "            batch_size: Batch size for processing\n",
    "            \n",
    "        Returns:\n",
    "            Activations tensor [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        print(f\"üîç Extracting activations from {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Load model and tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, \n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float32,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "            )\n",
    "            model.eval()\n",
    "            \n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(\n",
    "                text, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Hook to capture activations\n",
    "            activations = {}\n",
    "            \n",
    "            def activation_hook(module, input, output):\n",
    "                # Store the residual stream activations\n",
    "                if hasattr(output, 'last_hidden_state'):\n",
    "                    activations['residual'] = output.last_hidden_state\n",
    "                else:\n",
    "                    activations['residual'] = output[0] if isinstance(output, tuple) else output\n",
    "            \n",
    "            # Register hook on the target layer\n",
    "            if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
    "                target_layer = model.model.layers[self.layer]\n",
    "            else:\n",
    "                # Fallback - hook the entire model\n",
    "                target_layer = model\n",
    "                \n",
    "            hook = target_layer.register_forward_hook(activation_hook)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, output_hidden_states=True)\n",
    "                \n",
    "                # If we have hidden states, use them directly\n",
    "                if hasattr(outputs, 'hidden_states') and len(outputs.hidden_states) > self.layer:\n",
    "                    activations['residual'] = outputs.hidden_states[self.layer]\n",
    "            \n",
    "            hook.remove()\n",
    "            \n",
    "            # Return activations\n",
    "            residual_activations = activations.get('residual', outputs.hidden_states[-1])\n",
    "            print(f\"   ‚úÖ Extracted activations: {residual_activations.shape}\")\n",
    "            \n",
    "            return residual_activations\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extracting activations: {e}\")\n",
    "            # Return dummy activations for demo\n",
    "            print(\"üîÑ Using dummy activations for demonstration\")\n",
    "            return torch.randn(1, 10, self.sae.cfg.d_in, device=self.device)\n",
    "\n",
    "    def compute_sae_metrics(self, activations: torch.Tensor) -> SAEMetrics:\n",
    "        \"\"\"\n",
    "        Compute comprehensive SAE evaluation metrics.\n",
    "        \n",
    "        Args:\n",
    "            activations: Input activations [batch, seq, d_model]\n",
    "            \n",
    "        Returns:\n",
    "            SAEMetrics object with all evaluation metrics\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Reshape activations for SAE processing\n",
    "            batch_size, seq_len, d_model = activations.shape\n",
    "            flat_activations = activations.view(-1, d_model)\n",
    "            \n",
    "            # Forward pass through SAE\n",
    "            sae_out = self.sae(flat_activations)\n",
    "            \n",
    "            # Extract components\n",
    "            feature_acts = sae_out.feature_acts  # Sparse feature activations\n",
    "            sae_output = sae_out.sae_out         # Reconstructed activations\n",
    "            \n",
    "            # 1. Reconstruction Loss (MSE)\n",
    "            reconstruction_loss = torch.nn.functional.mse_loss(\n",
    "                sae_output, flat_activations\n",
    "            ).item()\n",
    "            \n",
    "            # 2. L0 Sparsity (fraction of non-zero features)\n",
    "            l0_sparsity = (feature_acts > 0).float().mean().item()\n",
    "            \n",
    "            # 3. L1 Sparsity (mean absolute activation)\n",
    "            l1_sparsity = feature_acts.abs().mean().item()\n",
    "            \n",
    "            # 4. Fraction of features that are ever active\n",
    "            fraction_alive = (feature_acts.max(dim=0)[0] > 0).float().mean().item()\n",
    "            \n",
    "            # 5. Mean maximum activation per sample\n",
    "            mean_max_activation = feature_acts.max(dim=1)[0].mean().item()\n",
    "            \n",
    "            # 6. Reconstruction score (explained variance)\n",
    "            var_original = flat_activations.var(dim=0).mean()\n",
    "            var_residual = (flat_activations - sae_output).var(dim=0).mean()\n",
    "            reconstruction_score = 1 - (var_residual / var_original).item()\n",
    "            \n",
    "            return SAEMetrics(\n",
    "                reconstruction_loss=reconstruction_loss,\n",
    "                l0_sparsity=l0_sparsity,\n",
    "                l1_sparsity=l1_sparsity,\n",
    "                fraction_alive=fraction_alive,\n",
    "                mean_max_activation=mean_max_activation,\n",
    "                reconstruction_score=reconstruction_score\n",
    "            )\n",
    "\n",
    "    def compute_representation_shift(self, \n",
    "                                   activations1: torch.Tensor, \n",
    "                                   activations2: torch.Tensor) -> RepresentationShift:\n",
    "        \"\"\"\n",
    "        Compute representation shift metrics between two sets of activations.\n",
    "        \n",
    "        Args:\n",
    "            activations1: Activations from first model\n",
    "            activations2: Activations from second model\n",
    "            \n",
    "        Returns:\n",
    "            RepresentationShift object with shift metrics\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Process both activation sets through SAE\n",
    "            flat_acts1 = activations1.view(-1, activations1.size(-1))\n",
    "            flat_acts2 = activations2.view(-1, activations2.size(-1))\n",
    "            \n",
    "            sae_out1 = self.sae(flat_acts1)\n",
    "            sae_out2 = self.sae(flat_acts2)\n",
    "            \n",
    "            features1 = sae_out1.feature_acts\n",
    "            features2 = sae_out2.feature_acts\n",
    "            \n",
    "            # 1. Cosine similarity between feature vectors\n",
    "            cosine_sim = torch.nn.functional.cosine_similarity(\n",
    "                features1.mean(dim=0), \n",
    "                features2.mean(dim=0), \n",
    "                dim=0\n",
    "            ).item()\n",
    "            \n",
    "            # 2. L2 distance between feature vectors\n",
    "            l2_distance = torch.norm(\n",
    "                features1.mean(dim=0) - features2.mean(dim=0), \n",
    "                p=2\n",
    "            ).item()\n",
    "            \n",
    "            # 3. Feature overlap (Jaccard similarity)\n",
    "            active1 = (features1 > 0).float()\n",
    "            active2 = (features2 > 0).float()\n",
    "            \n",
    "            intersection = (active1 * active2).sum(dim=0)\n",
    "            union = torch.clamp(active1.sum(dim=0) + active2.sum(dim=0) - intersection, min=1)\n",
    "            feature_overlap = (intersection / union).mean().item()\n",
    "            \n",
    "            # 4. Jensen-Shannon divergence between feature distributions\n",
    "            def js_divergence(p, q):\n",
    "                p = p + 1e-8  # Add small epsilon for numerical stability\n",
    "                q = q + 1e-8\n",
    "                p = p / p.sum()\n",
    "                q = q / q.sum()\n",
    "                m = 0.5 * (p + q)\n",
    "                return 0.5 * (torch.nn.functional.kl_div(p, m, reduction='sum') + \n",
    "                             torch.nn.functional.kl_div(q, m, reduction='sum'))\n",
    "            \n",
    "            p = features1.mean(dim=0).abs()\n",
    "            q = features2.mean(dim=0).abs()\n",
    "            js_div = js_divergence(p, q).item()\n",
    "            \n",
    "            # 5. Feature correlation\n",
    "            corr_matrix = torch.corrcoef(torch.stack([\n",
    "                features1.mean(dim=0), \n",
    "                features2.mean(dim=0)\n",
    "            ]))\n",
    "            feature_correlation = corr_matrix[0, 1].item() if not torch.isnan(corr_matrix[0, 1]) else 0.0\n",
    "            \n",
    "            return RepresentationShift(\n",
    "                cosine_similarity=cosine_sim,\n",
    "                l2_distance=l2_distance,\n",
    "                feature_overlap=feature_overlap,\n",
    "                js_divergence=js_div,\n",
    "                feature_correlation=feature_correlation\n",
    "            )\n",
    "\n",
    "    def analyze_models(self, \n",
    "                      model1_name: str, \n",
    "                      model2_name: str, \n",
    "                      texts: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Complete analysis comparing two models across multiple texts.\n",
    "        \n",
    "        Args:\n",
    "            model1_name: First model identifier\n",
    "            model2_name: Second model identifier  \n",
    "            texts: List of texts to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with comprehensive analysis results\n",
    "        \"\"\"\n",
    "        print(f\"üöÄ Starting comparative analysis\")\n",
    "        print(f\"   Model 1: {model1_name}\")\n",
    "        print(f\"   Model 2: {model2_name}\")\n",
    "        print(f\"   Texts: {len(texts)} samples\")\n",
    "        print()\n",
    "        \n",
    "        results = {\n",
    "            'model1_metrics': [],\n",
    "            'model2_metrics': [], \n",
    "            'shift_metrics': [],\n",
    "            'texts': texts\n",
    "        }\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            print(f\"üìù Processing text {i+1}/{len(texts)}: '{text[:50]}...'\")\n",
    "            \n",
    "            # Extract activations\n",
    "            acts1 = self.get_model_activations(model1_name, text)\n",
    "            acts2 = self.get_model_activations(model2_name, text)\n",
    "            \n",
    "            # Compute SAE metrics\n",
    "            metrics1 = self.compute_sae_metrics(acts1)\n",
    "            metrics2 = self.compute_sae_metrics(acts2)\n",
    "            \n",
    "            # Compute representation shift\n",
    "            shift = self.compute_representation_shift(acts1, acts2)\n",
    "            \n",
    "            results['model1_metrics'].append(metrics1)\n",
    "            results['model2_metrics'].append(metrics2)\n",
    "            results['shift_metrics'].append(shift)\n",
    "            \n",
    "            print(f\"   ‚úÖ Completed analysis for text {i+1}\")\n",
    "        \n",
    "        # Compute aggregate statistics\n",
    "        results['aggregate'] = self._compute_aggregate_stats(results)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _compute_aggregate_stats(self, results: Dict) -> Dict:\n",
    "        \"\"\"Compute aggregate statistics across all texts.\"\"\"\n",
    "        n_texts = len(results['texts'])\n",
    "        \n",
    "        # Average metrics across texts\n",
    "        avg_model1 = {}\n",
    "        avg_model2 = {}\n",
    "        avg_shift = {}\n",
    "        \n",
    "        for field in SAEMetrics.__dataclass_fields__:\n",
    "            avg_model1[field] = np.mean([getattr(m, field) for m in results['model1_metrics']])\n",
    "            avg_model2[field] = np.mean([getattr(m, field) for m in results['model2_metrics']])\n",
    "        \n",
    "        for field in RepresentationShift.__dataclass_fields__:\n",
    "            avg_shift[field] = np.mean([getattr(s, field) for s in results['shift_metrics']])\n",
    "        \n",
    "        return {\n",
    "            'avg_model1_metrics': avg_model1,\n",
    "            'avg_model2_metrics': avg_model2,\n",
    "            'avg_shift_metrics': avg_shift,\n",
    "            'n_texts': n_texts\n",
    "        }\n",
    "\n",
    "    def visualize_results(self, results: Dict, save_path: str = \"sae_analysis.png\"):\n",
    "        \"\"\"Create comprehensive visualization of analysis results.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('SAE-based Representation Shift Analysis (Gemma Scope)', fontsize=16)\n",
    "        \n",
    "        agg = results['aggregate']\n",
    "        \n",
    "        # Plot 1: Reconstruction metrics\n",
    "        recon_metrics = ['reconstruction_loss', 'reconstruction_score']\n",
    "        model1_recon = [agg['avg_model1_metrics'][m] for m in recon_metrics]\n",
    "        model2_recon = [agg['avg_model2_metrics'][m] for m in recon_metrics]\n",
    "        \n",
    "        x = np.arange(len(recon_metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[0,0].bar(x - width/2, model1_recon, width, label='Model 1', alpha=0.8)\n",
    "        axes[0,0].bar(x + width/2, model2_recon, width, label='Model 2', alpha=0.8)\n",
    "        axes[0,0].set_title('Reconstruction Quality')\n",
    "        axes[0,0].set_xticks(x)\n",
    "        axes[0,0].set_xticklabels(recon_metrics, rotation=45)\n",
    "        axes[0,0].legend()\n",
    "        \n",
    "        # Plot 2: Sparsity metrics\n",
    "        sparsity_metrics = ['l0_sparsity', 'l1_sparsity', 'fraction_alive']\n",
    "        model1_sparsity = [agg['avg_model1_metrics'][m] for m in sparsity_metrics]\n",
    "        model2_sparsity = [agg['avg_model2_metrics'][m] for m in sparsity_metrics]\n",
    "        \n",
    "        x = np.arange(len(sparsity_metrics))\n",
    "        axes[0,1].bar(x - width/2, model1_sparsity, width, label='Model 1', alpha=0.8)\n",
    "        axes[0,1].bar(x + width/2, model2_sparsity, width, label='Model 2', alpha=0.8)\n",
    "        axes[0,1].set_title('Sparsity Metrics')\n",
    "        axes[0,1].set_xticks(x)\n",
    "        axes[0,1].set_xticklabels(sparsity_metrics, rotation=45)\n",
    "        axes[0,1].legend()\n",
    "        \n",
    "        # Plot 3: Representation shift metrics\n",
    "        shift_names = list(agg['avg_shift_metrics'].keys())\n",
    "        shift_values = list(agg['avg_shift_metrics'].values())\n",
    "        \n",
    "        axes[0,2].barh(shift_names, shift_values, color='green', alpha=0.7)\n",
    "        axes[0,2].set_title('Representation Shift Metrics')\n",
    "        axes[0,2].set_xlabel('Value')\n",
    "        \n",
    "        # Plot 4: Distribution of cosine similarities across texts\n",
    "        cosine_sims = [s.cosine_similarity for s in results['shift_metrics']]\n",
    "        axes[1,0].hist(cosine_sims, bins=10, alpha=0.7, edgecolor='black')\n",
    "        axes[1,0].axvline(np.mean(cosine_sims), color='red', linestyle='--', \n",
    "                         label=f'Mean: {np.mean(cosine_sims):.3f}')\n",
    "        axes[1,0].set_title('Distribution of Cosine Similarities')\n",
    "        axes[1,0].set_xlabel('Cosine Similarity')\n",
    "        axes[1,0].set_ylabel('Frequency')\n",
    "        axes[1,0].legend()\n",
    "        \n",
    "        # Plot 5: Scatter plot of reconstruction loss vs sparsity\n",
    "        model1_recon_loss = [m.reconstruction_loss for m in results['model1_metrics']]\n",
    "        model1_sparsity = [m.l0_sparsity for m in results['model1_metrics']]\n",
    "        model2_recon_loss = [m.reconstruction_loss for m in results['model2_metrics']]\n",
    "        model2_sparsity = [m.l0_sparsity for m in results['model2_metrics']]\n",
    "        \n",
    "        axes[1,1].scatter(model1_sparsity, model1_recon_loss, alpha=0.7, label='Model 1')\n",
    "        axes[1,1].scatter(model2_sparsity, model2_recon_loss, alpha=0.7, label='Model 2')\n",
    "        axes[1,1].set_xlabel('L0 Sparsity')\n",
    "        axes[1,1].set_ylabel('Reconstruction Loss')\n",
    "        axes[1,1].set_title('Reconstruction-Sparsity Trade-off')\n",
    "        axes[1,1].legend()\n",
    "        \n",
    "        # Plot 6: Feature overlap distribution\n",
    "        overlaps = [s.feature_overlap for s in results['shift_metrics']]\n",
    "        axes[1,2].hist(overlaps, bins=10, alpha=0.7, edgecolor='black')\n",
    "        axes[1,2].axvline(np.mean(overlaps), color='red', linestyle='--',\n",
    "                         label=f'Mean: {np.mean(overlaps):.3f}')\n",
    "        axes[1,2].set_title('Distribution of Feature Overlaps')\n",
    "        axes[1,2].set_xlabel('Feature Overlap')\n",
    "        axes[1,2].set_ylabel('Frequency')\n",
    "        axes[1,2].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Visualization saved to {save_path}\")\n",
    "\n",
    "    def interpret_results(self, results: Dict) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Provide interpretation of the analysis results.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with interpretation strings for each aspect\n",
    "        \"\"\"\n",
    "        agg = results['aggregate']\n",
    "        interpretations = {}\n",
    "        \n",
    "        # SAE Quality Assessment\n",
    "        avg_recon_loss = (agg['avg_model1_metrics']['reconstruction_loss'] + \n",
    "                         agg['avg_model2_metrics']['reconstruction_loss']) / 2\n",
    "        avg_sparsity = (agg['avg_model1_metrics']['l0_sparsity'] + \n",
    "                       agg['avg_model2_metrics']['l0_sparsity']) / 2\n",
    "        \n",
    "        if avg_recon_loss < 0.1 and avg_sparsity < 0.1:\n",
    "            interpretations['sae_quality'] = \"‚úÖ SAE is working well - low reconstruction loss with high sparsity\"\n",
    "        elif avg_recon_loss < 0.1:\n",
    "            interpretations['sae_quality'] = \"‚ö†Ô∏è SAE reconstructs well but low sparsity - may be learning dense features\"\n",
    "        elif avg_sparsity < 0.1:\n",
    "            interpretations['sae_quality'] = \"‚ö†Ô∏è SAE is sparse but high reconstruction loss - may be losing information\"\n",
    "        else:\n",
    "            interpretations['sae_quality'] = \"‚ùå SAE quality is poor - high reconstruction loss and low sparsity\"\n",
    "        \n",
    "        # Representation Shift Assessment\n",
    "        cosine_sim = agg['avg_shift_metrics']['cosine_similarity']\n",
    "        feature_overlap = agg['avg_shift_metrics']['feature_overlap']\n",
    "        \n",
    "        if cosine_sim > 0.8 and feature_overlap > 0.5:\n",
    "            interpretations['shift_magnitude'] = \"‚úÖ Small representation shift - models use similar features\"\n",
    "        elif cosine_sim > 0.6 or feature_overlap > 0.3:\n",
    "            interpretations['shift_magnitude'] = \"‚ö†Ô∏è Moderate representation shift - some shared features\"\n",
    "        else:\n",
    "            interpretations['shift_magnitude'] = \"üîç Large representation shift - models use very different features\"\n",
    "        \n",
    "        # Model Comparison\n",
    "        recon_diff = abs(agg['avg_model1_metrics']['reconstruction_loss'] - \n",
    "                        agg['avg_model2_metrics']['reconstruction_loss'])\n",
    "        sparsity_diff = abs(agg['avg_model1_metrics']['l0_sparsity'] - \n",
    "                           agg['avg_model2_metrics']['l0_sparsity'])\n",
    "        \n",
    "        if recon_diff < 0.05 and sparsity_diff < 0.02:\n",
    "            interpretations['model_similarity'] = \"‚úÖ Models show similar SAE characteristics\"\n",
    "        else:\n",
    "            interpretations['model_similarity'] = \"üîç Models show different SAE characteristics - architectural differences detected\"\n",
    "        \n",
    "        return interpretations\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main demonstration of SAE-based representation shift analysis.\"\"\"\n",
    "    print(\"üöÄ SAE Lens - Gemma Scope Representation Shift Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Configuration\n",
    "    LAYER = 12  # Middle layer for analysis\n",
    "    WIDTH = \"16k\"  # SAE width\n",
    "    MODEL_SIZE = \"2b\"  # Using 2B models for faster demo\n",
    "    \n",
    "    # Test texts covering different domains\n",
    "    test_texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"In machine learning, neural networks learn complex patterns from data.\",\n",
    "        \"The economy has shown resilience despite global challenges.\",\n",
    "        \"Climate change affects weather patterns around the world.\",\n",
    "        \"Artificial intelligence transforms how we work and live.\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Initialize analyzer\n",
    "        analyzer = GemmaScopeAnalyzer(\n",
    "            layer=LAYER, \n",
    "            width=WIDTH, \n",
    "            model_size=MODEL_SIZE\n",
    "        )\n",
    "        \n",
    "        # Model names (adjust these based on available models)\n",
    "        model1_name = \"google/gemma-2-2b\"  # Base Gemma 2\n",
    "        model2_name = \"google/gemma-2-2b-it\"  # Instruction-tuned version\n",
    "        # Note: Replace with actual PaliGemma when available\n",
    "        \n",
    "        print(f\"\\nüî¨ Analysis Configuration:\")\n",
    "        print(f\"   Layer: {LAYER}\")\n",
    "        print(f\"   SAE Width: {WIDTH}\")\n",
    "        print(f\"   Model Size: {MODEL_SIZE}\")\n",
    "        print(f\"   Test Texts: {len(test_texts)}\")\n",
    "        print()\n",
    "        \n",
    "        # Run analysis\n",
    "        results = analyzer.analyze_models(model1_name, model2_name, test_texts)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nüìä ANALYSIS RESULTS:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        agg = results['aggregate']\n",
    "        \n",
    "        print(\"\\nAverage SAE Metrics - Model 1:\")\n",
    "        for key, value in agg['avg_model1_metrics'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nAverage SAE Metrics - Model 2:\")\n",
    "        for key, value in agg['avg_model2_metrics'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nAverage Representation Shift Metrics:\")\n",
    "        for key, value in agg['avg_shift_metrics'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        # Generate interpretations\n",
    "        interpretations = analyzer.interpret_results(results)\n",
    "        \n",
    "        print(\"\\nüîç INTERPRETATIONS:\")\n",
    "        print(\"=\" * 40)\n",
    "        for aspect, interpretation in interpretations.items():\n",
    "            print(f\"{aspect.replace('_', ' ').title()}: {interpretation}\")\n",
    "        \n",
    "        # Create visualization\n",
    "        analyzer.visualize_results(results)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Analysis complete!\")\n",
    "        print(f\"üìà Visualization saved as 'sae_analysis.png'\")\n",
    "        print(f\"üìã Analyzed {len(test_texts)} texts across layer {LAYER}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during analysis: {e}\")\n",
    "        print(\"\\nüí° Troubleshooting tips:\")\n",
    "        print(\"   1. Install SAE Lens: pip install sae-lens\")\n",
    "        print(\"   2. Ensure you have sufficient GPU memory\")\n",
    "        print(\"   3. Try with smaller models or fewer texts\")\n",
    "        print(\"   4. Check model names are correct and accessible\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Installation requirements:\n",
    "\"\"\"\n",
    "pip install sae-lens transformers torch matplotlib seaborn numpy\n",
    "\n",
    "# For CUDA support (recommended):\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
